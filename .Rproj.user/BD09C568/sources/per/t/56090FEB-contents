# Sampling

## Uniform random numbers
What we won't be doing in this module is generating true uniform random numbers. This is incredibly difficult and usually requires lots of expensive hardware. This is because computers aren't good at being random, they require algorithmic instructions. True random number generation often uses physical methods, such as the radioactive decay of atoms, or atmospheric noise.

Throughout this module, we will be using R's built in random number generation. This is a pseudo random number generator, that has excellent random properties, but will eventually repeat. A basic random number generation method is sampling from a uniform distribution on the unit interval
```{r}
runif(1, 0, 1)
```

## Inverse transform sampling
Suppose we want to sample from a non-uniform one-dimensional distribution. The inverse transform theorem allows us to do this using the distribution's inverse function. 

::: {.definition}
The **inverse function** of a distribution $F$, denoted $F^{-1}$, is defined for all $u \in (0, 1)$ by
$$
F^{-1}(u) = \inf\{x \in\mathbb{R} : F(x) > u\}.
$$
:::

::: {.theorem} 
Let $F :\mathbb{R} \rightarrow [0, 1]$, $U \sim U[0, 1]$ and $Y = F^{-1}(U)$. Then $Y$ has distribution $F$.
:::

::: {.proof}
We have
$$
p(Y \leq a) = p(F^{-1}(U) \leq a) = p(\inf\{x \in\mathbb{R} : F(x) > u\} \leq a). 
$$
Since $\inf\{x \in\mathbb{R} : F(x) > u\} \leq a$ can only hold if $F(a) \geq U$, we have
$$
p(Y \leq a)  = p(F(a)\geq U)
$$
As $U \sim U[0, 1]$, we have $p(F(a)\geq U) = F(a)$. 
:::

This theorem says that if we have a random variable $U \sim U[0, 1]$ and we want to get $Y \sim F$, then we can use $F^{-1}(U)$. Viewing this theorem graphically can provide a much more intuitive understanding. 

::: {.example}
We would like to sample from an exponential distribution with rate 0.5. The likelihood function is given by
$$
p(y \mid \lambda = 0.5) = 0.5e^{0.5y}.
$$
The distribution function can be derived by
\begin{align*}
F(y \mid \lambda = 0.5) &= \int_0^y 0.5e^{0.5t}\,dt \\
& =  1 - e^{0.5y}.
\end{align*}
Finally, the inverse function is given by
$$
F^{-1}(y \mid \lambda = 0.5) = -2\log(1-y).  
$$
Therefore, if $U \sim U[0, 1]$, $-2\log(1-U) \sim \hbox{Exp}(0.5)$. 

The R code below generates a plot to show this. We can plot the CDF for most one parameter distributions straightforwardly. We can think of this theorem as allowing us to sample a point on the y-axis and then computing the quantile this corresponds to. 

```{r}
set.seed(12345) # to reproduce
y <- seq(0, 10, 0.01) #Show on the interval [0, 5]
f <- 1 - exp(-0.5*y)    #Construct the cumulative density function (CDF)
plot(y, f, type ='l', xlab = "y", ylab= "CDF")

#Sample u
u <- runif(1)

#Get the corresponding y value
f.inv <- -2*log(1-u)

#plot 
segments(x0 = 0, y0 = u, x1 = f.inv, y1 = u, lty = 2)
segments(x0 = f.inv, y0 = 0, x1 = f.inv, y1 = u, lty = 2)
```
:::

## Rejection sampling
We now have a way of sampling methods where we can analytically derive the inverse distribution function. We can use this to sample from more complex densities, or simple densities more efficiently. Rejection sampling by sampling according to a density we can sample from and then rejecting or accepting that sample based on the density we're actually interested in. 

Suppose we want to sample from a density $p$, but can only generate samples from a density $q$. If there exists some constant $c > 0$, such that $\frac{p(y)}{q(y)} \leq c$ for all $y$, then we can generate samples from $p$ by

1. Sampling $Y \sim Q$

2. Sampling $U \sim U[0, 1]$

3. Computing $k = \frac{p(u)}{cq(y)}$

4. Accepting $y$ if $U < k$ and rejecting otherwise. 

This says draw sample a point $y$ according to the density $q$. Draw a vertical line at $y$ from the x-axis to $cq(y)$. Sample uniformly on this line. If the uniformly random sample is below $q$, then accept it. Otherwise, reject it. The theory behind this is as follows. Suppose we sample some point y according to this algorithm and we want to work out its density $f$, then 
$$
f(y) \propto q(y)p(U < k) = q(y)\frac{p(u)}{cq(y)} = \frac{p(u)}{c}.
$$
Therefore, $f = p$. 

::: {.example}
Suppose we want to sample from a distribution that has the density
$$
p(y) = \begin{cases}
\frac{3}{4}y(2-y), \qquad y \in [0, 2] \\
0, \qquad \textrm{otherwise}
\end{cases}.
$$
This has a maximum at $\frac{3}{4}$. We choose $p \sim U[0, 1]$ and $c = \frac{3}{4}$. The R code below shows a pictorial version of how one sample is generated. 
```{r}
set.seed(1234)   #to repoduce
M <- 3/4         #set M
y <- runif(1)    #sample Y ~ Q
p <- 3/4*y*(2-y) #compute p(y)
k <- p/(M*1)     #compute k
u <- runif(1)    #sample U ~ U[0, 1]
ifelse(u < k, 'accept', 'reject') #Accept if  u < k


#Create nice plot
a <- seq(0, 2, 0.01)
b <- 3/4*a*(2-a)
c <- M*rep(1, length(a))
plot(a, b, ylim = c(0, M), type = 'l')
lines(a, c)
segments(x0 = y, y0 = 0, x1 = y,  y1 =3/4*y*(2-y) , lty = 2, lwd = 2)
segments(x0 = y,  y0 =3/4*y*(2-y), x1 = y, y1 = M, lty = 2, col = 2, lwd = 2)
points(x = y, y = u, pch = 19)

```
The plot also shows how the choices of $M$ and $q$ can make the sampling more or less efficient. In our example, the rejection space is large, meaning many of our proposed samples will be rejected. Here, we could have chosen a better $q$ to minimise this space. 
:::

## Markov Chain Monte Carlo
Markov Chain Monte Carlo (MCMC) is a set of algorithms that sample from a posterior distribution. The combine the idea of rejection sampling, with the theory of Markov chains. Before we set out the theory of Markov chains, we'll go through an example to show how MCMC works.


::: {.example #King} 
([Adapted from Statistical Rethinking \S 9](https://xcelab.net/rm/statistical-rethinking/)) Consider an eccentric King whose kingdom consists of a ring of 10 islands. Directly north is island one, the smallest island. Going clockwise around the archipelago, next is island two, which is twice the size of island one, then island three, which is three times as large as island one. Finally, island 10 is next to island one and ten times as large. 

The King wanted to visit all of his islands, but spending time on each one according to its size. That is he should spend the most time on island ten and the least on island one. Being climate conscious, he also decided that flying from one side of the archipelago to the other was not allowed. Instead, he would only sail from one island to either of its neighbors. So from island one, he could reach islands two and ten. 

He decided to travel according to these rules: 

1. At the end of each week, he decides to stay on the same island or move to a neighboring island according to a coin toss. If it's heads he proposes moving clockwise, and tails anti-clockwise. The island he is considering moving to is called the proposal island. 

2. To decided if he is going to move to the proposal island, the King counts out a number of shells equal to the number of size of the island. So if island five is the proposal island, he counts out five shells. He then counts out a number of stones equal to the size of the current island. 

3. If the number of seashells is greater than the number of stones, he moves to the proposed island. If the number of seashells is less than the number of stones, he takes a different strategy. He discards the number of stones equal to the number of seashells. So if there are six stone and five seashells, he ends up with 6-5=1 stone. He then places the stones and seashells into a bag a chooses one at random. If he picks a seashell, he moves to the proposed island, if he picks a shell, he stays put. 

This is a complex way of moving around, but it produces the required result; the time he spends on each island is proportionate to the size of the island. The code below shows an example of this over 10,000 weeks. 

```{r}
weeks <- 10000
island <- numeric(weeks)
current <- 10
for(i in 1:weeks){
  
  ## record current position
  island[i] <- current
  
  #Flip a coin to move to a propose a new island
  proposed <- current + sample(c(1, -1), size = 1)
  
  #Ensure he loops round the island
  if(proposed < 1) 
    proposed <- 10
  if(proposed > 10)
    proposed <- 1
  
  #Decide to move
  p <- proposed/current
  u <- runif(1)
  if(u < p)
    current <- proposed
  
  
}

#Plot results
par(mfrow = c(1, 2))
plot(island, type = 'l', xlab = "week", ylab = "island")
barplot(table(island)/weeks, xlab = "island", ylab = "Proportion of time")
```
:::

We can recognise several different statistical principles in this example. The King decides to move islands dependent on where he is currently, not based on where he has been previously (Markov principle). He proposes an island to move to and accepts or rejects this decision based on some distribution (rejection principle). We are now going to describe some of the properties of Markov chains, including the Markov principle. 

## Properties of Markov Chains

::: {.definition #Markov}
A sequence of random variables $\{Y_1, Y_2, \ldots\}$ is a **Markov chain** if $p(Y_{n+1} \mid Y_{n}, \ldots, Y_1) = p(Y_{n+1} \mid Y_{n})$. That is that distribution of the next state $Y_{n+1}$ only depends on the current state $Y_n$ and not any previous states. 
:::

::: {.definition}
The probability of transitioning from state $i$ to state $j$ in a Markov chain is given by $p_{ij}$. The **transition matrix** for a Markov chain with $N$ states is the $N \times N$ matrix $P = p_{ij}$, where the $\{i, j\}^{th}$ entry is probability is moving from state $i$ to state $j$. 
:::

These two properties make Markov chains nice to work with, especially the Markov property (Definition \@ref(def:Markov)). Two other important definitions are

::: {.definition}
The **period **of a state $i$ is given by $d_i = \textrm{gcd}\{n  > 0; p_{ii} > 0 \}$. A state is **aperiodic** if $d_i = 1$. An **aperiodic chain** is a chain where all states are a periodic. 
:::


::: {.definition}
A Markov chain is **irreducible** if there exists an $n \in \mathbb{N}$ such that $p(Y_n = i \mid Y_0 = j)$ for all pairs $i$ and $j$. In other words, it is possible to move from any state to any other state in a finite number of steps. 
:::

We can use these definitions to start working with distributions. Suppose, the state we start at is drawn from some distribution $Y_1 \sim \boldsymbol{q}$. Then the distributions of the  second state $Y_2$ depends on the distribution of $Y_1$ and the transition probabilities
$$
p(Y_2 = j) = \sum_i q_ip_{ij}.
$$
If we denote the distribution of $Y_2 \sim \boldsymbol{q}^{(2)}$, then we can write it in terms of the transition matrix $\boldsymbol{q}^\prime = \boldsymbol{q}P$. Now suppose we would like the distribution of $Y_3 \sim \boldsymbol{q}^{(3)}$, thanks to the Markov property, this is the distribution for $Y_2$ multiplied by the transition matrix, so $Y_3 \sim qP^2$. Inductively, $P_k \sim qP^{k-1}$. To use Markov chains to sample from distributions, we need to identify the Eigenvalues of the transition matrix. 

::: {.proposition}
A transition matrix $P$ always has at least one eigenvalue equal to one. 
:::

::: {.proof}
The columns of $P$ sum to 1 as they are probability distributions. Therefore, $1$ is an eigenvector. 
:::

::: {.definition} If a transition matrix $P$ has a unique Eigenvalue that takes the value 1, there is a unique distribution $\pi$ such that 
$$
\pi P = \pi. 
$$
This distribution $\pi$, is known as the **stationary distribution**. 
:::

This important concept underpins MCMC methods. It says that no matter where we start our chain, we'll eventually end up sampling states according to the distribution $\pi$. It make take a long time to reach the stationary distribution, but it will eventually get there. 

In order to check whether our Markov chain will converge to a stationary distributions, we need to check:

1. the Markov chain is aperiodic,

2. the Markov chain is irreducible, and 

3. that there exists a unique distribution $\pi$ such that $\pi P = \pi$.

::: {.example}
In Example \@ref(exm:King), the King wanted to visit the islands according to how large they are. We can think of the islands as the states and the stationary distribution as $p(Y = i) \propto i$. The eccentric method the King used allowed him to construct a transition matrix for an aperiodic Markov chain. He also never visited islands regularly using this method. 
:::

When designing a Markov chain, it is usually straightforward to design one that meets conditions one and two. Condition three is more difficult to prove, but for some chains it is possible to show they satisfy detailed balance. 

::: {.definition}
The Markov chain $P$ satisfies **detailed balance** with respect to the distribution $\pi$ if
$$
\pi_i p_{ij} = \pi_j p_{ji}. 
$$
:::

::: {.theorem name="Detailed Balance"}
Let $P$ be a transition matrix that satisfies detailed balance with respect to the distribution $\pi$. Then $\pi P = \pi$. 
:::

::: {.proof}
The $j^{th}$ row of $\pi P$ is 
\begin{align*}
\sum_{i} \pi_i p_{ij} & = \sum_{i} \pi_j p_{ji} \quad \textrm{(detailed balance)} \\
 & = \pi_j \sum_{i} p_{ji} \\
 & = \pi_j.\qquad \textrm{(probaility sums to 1)}
\end{align*}
Hence $\pi P = \pi$.
:::

The section has shown us that we can use a Markov chain to simulate from a distribution $\pi$. All we need is for the Markov chain to be irreducible, aperiodic, and for the transition matrix to satisfy $\pi P = \pi$. This provides the foundation theory for MCMC and allows us to sample from a posterior distribution $\pi$. What it doesn't tell us is how to design the Markov chain, and that is what the next sections deal with. 

## Gibbs Sampler
So far in this module, we have derived posterior distributions where the posterior distribution has a closed form, and the forms of the prior and posterior distribution have the same family. This has very helpful analytical and computational properties. One common way by which this arises is through prior conjugacy. 

::: {.defintion}
If the prior distribution $\pi(\theta)$ has the same distirbutional family as the posterior distribution $\pi(\theta \mid \boldsymbol{y})$, then the prior distribution is a **conjugate prior distribution**. 
:::

::: {.example}
In Example \@ref(exm:binom), we observed data that was distibruted according to a Binomial disrtibution with unknown probability of success $\theta$. We place a uniform prior distribution on $\theta \sim U[1, 1]$. This is equivalent to a Beta distribution $\theta \sim \textrm{Beta}(1, 1)$. The resulting posterior was also a Beta distribution $\theta \mid \boldsymbol{y} \sim \textrm{Beta}(4, 198)$. The prior and posterior distibrutions both are Beta distributions, so the prior distibruton is conjugate.
:::

## Metropolis-Hastings


## MCMC Diagnostics



