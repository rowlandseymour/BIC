# Sampling

## Uniform Random Numbers
What we won't be doing in this module is generating true uniform random numbers. This is incredibly difficult and usually requires lots of expensive hardware. This is because computers aren't good at being random, they require algorithmic instructions. True random number generation often uses physical methods, such as the radioactive decay of atoms, or atmospheric noise.

Throughout this module, we will be using R's built in random number generation. This is a pseudo random number generator that has excellent random properties, but will eventually repeat. A basic random number generation tool that we will repeatedly use in the module involves sampling from a uniform distribution on the unit interval, which can be done in R using
```{r}
runif(1, 0, 1)
```


## Inverse Transform Sampling
Suppose we want to sample from a non-uniform one-dimensional distribution. The inverse transform theorem allows us to do this using the distribution's inverse function. 

::: {.definition}
Let $X$ be a real-valued random variable with a distribution function $F$. Then the **inverse function** of a distribution  function $F$, denoted $F^{-1}$, is defined for all $u \in (0, 1)$ by
$$
F^{-1}(u) = \inf\{x \in\mathbb{R} : F(x) > u\}.
$$
:::

::: {.theorem} 
Let $F :\mathbb{R} \rightarrow [0, 1]$ be a continuous distribution function, $U \sim U[0, 1]$ and $Y = F^{-1}(U)$. Then $Y$ has distribution function $F$.
:::

::: {.proof}
We have
$$
\mathbb{P}(Y \leq a) = \mathbb{P}(F^{-1}(U) \leq a) = \mathbb{P}(\inf\{x \in\mathbb{R} : F(x) > u\} \leq a). 
$$
Since $\inf\{x \in\mathbb{R} : F(x) > u\} \leq a$ can only hold if $F(a) \geq U$, we have
$$
\mathbb{P}(Y \leq a)  = \mathbb{P}(F(a)\geq U)
$$
As $U \sim U[0, 1]$, we have $\mathbb{P}(F(a)\geq U) = F(a)$. 
:::

This theorem says that if we have a random variable $U \sim U[0, 1]$ and we want to get $Y \sim F$, then we can use $F^{-1}(U)$. Viewing this theorem graphically can provide a much more intuitive understanding. 

::: {.example}
We would like to sample from an exponential distribution with rate $\lambda$, i.e. $Y ~ \sim \hbox{Exp}(\lambda)$. The density function is given by

$$
\pi(y \mid \lambda) = \begin{cases} 
      \lambda e^{\lambda y} & y \geq 0 \\
    0  & \text{otherwise.}
   \end{cases}
$$

The distribution function can be derived by
\begin{align*}
F(y \mid \lambda) &= \int_0^y \lambda e^{\lambda t}\,dt \\
& =  1 - e^{\lambda y}.
\end{align*}
Finally, the inverse function is given by
$$
F^{-1}(y \mid \lambda) = -\frac{1}{\lambda}\log(1-y).  
$$
Therefore, if $U \sim U[0, 1]$, then it follows that $-\frac{1}{\lambda}log(1-U) \sim \hbox{Exp}(\lambda)$. 

The R code below generates a plot to show this (with $\lambda = 0.5$). We can plot the CDF for most one parameter distributions straightforwardly. We can think of this theorem as allowing us to sample a point on the y-axis and then computing the quantile this corresponds to. 

```{r}
set.seed(12345) # to reproduce
y <- seq(0, 10, 0.01) #Show on the interval [0, 5]
f <- 1 - exp(-0.5*y)    #Construct the cumulative density 
                        #function (CDF)
plot(y, f, type ='l', xlab = "y", ylab= "CDF")

#Sample u
u <- runif(1)

#Get the corresponding y value
f.inv <- -2*log(1-u)

#plot 
segments(x0 = 0, y0 = u, x1 = f.inv, y1 = u, lty = 2)
segments(x0 = f.inv, y0 = 0, x1 = f.inv, y1 = u, lty = 2)
text(x = f.inv, y = -0.01, expression(F[-1](U)), col = 4)
text(x = -.1, y = u, "U", col = 4)
```
:::

::: {.example}
Suppose we want to generate samples from the Cauchy distribution with location 0 and scale 1. This has density function 
$$
\pi(x) = \frac{1}{\pi(1+x^2)}, \quad x \in \mathbb{R}.
$$
A plot of this function is shown below. 
```{r}
x <- seq(-5, 5, 0.01)
y <- 1/(pi*(1 + x^2))
plot(x, y, type = 'l')
```
To use the inverse transform method, we first need to find the CDF:
$$
F(x) = \int_{-\infty}^x \frac{1}{\pi(1+t^2)}dt
$$
Letting $t = \tan \theta$, we can write $dt = \sec^2(\theta)$. The integral becomes
$$
F(x) = \int_{-\frac{\pi}{2}}^{\arctan(x)} \frac{\sec^2(\theta)}{\pi(1+\tan^2(\theta))} d\theta
$$
As $1 + \tan^2(\theta) = \sec^2(\theta)$, we can write the integral as
$$
F(x) = \int_{-\frac{\pi}{2}}^{\arctan(x)} \frac{1}{\pi}du\\
= \left[\frac{\theta}{\pi}\right]_{-\frac{\pi}{2}}^{\arctan(x)}\\
= \arctan(x) + \frac{1}{2}.
$$
The inverse of the distribution function is 
$$
F^{-1}(x) = \tan\left(\pi\left(x - \frac{1}{2}\right)\right).
$$
Hence, if $U \sim U[0, 1]$, then $\tan\left(\pi\left(x - \frac{1}{2}\right)\right) \sim \hbox{Cauchy}(0, 1)$.
:::

## Rejection Sampling
We now have a way of sampling realisations from distributions where we can analytically derive the inverse distribution function. We can use this to sample from more complex densities, or simple densities more efficiently. Rejection sampling works by sampling according to a density we can sample from and then rejecting or accepting that sample based on the density we're actually interested in. The plot below shows an example from this. We would like to generate a sample from the distribution with the curved density function, which is challenging. Instead, we find a distribution whose density function bounds the one we are interested in a sample from that. In this case we can use the uniform distribution. Once we have generated our sample from he uniform distribution, we choose to accept or reject it based on the distribution we are interested in. In this case we reject it. 

```{r echo = FALSE}
set.seed(1234)   #to reproduce
M <- 3/4         #set M
y <- runif(1)    #sample Y ~ Q
p <- 3/4*y*(2-y) #compute pi(y)
k <- p/(M*1)     #compute k
u <- runif(1)    #sample U ~ U[0, 1]


#Create nice plot
a <- seq(0, 2, 0.01)
b <- 3/4*a*(2-a)
c <- M*rep(1, length(a))
plot(a, b, ylim = c(0, M), type = 'l')
lines(a, c)
segments(x0 = y, y0 = 0, x1 = y,  y1 =3/4*y*(2-y) , lty = 2, lwd = 2)
segments(x0 = y,  y0 =3/4*y*(2-y), x1 = y, y1 = M, lty = 2, col = 2, lwd = 2)
points(x = y, y = u, pch = 19)
```


Suppose we want to sample from a density $\pi$, but can only generate samples from a density $q$. If there exists some constant $c > 0$, such that $\frac{\pi(y)}{q(y)} \leq c$ for all $y$, then we can generate samples from $\pi$ by

1. Sampling $Y \sim Q$

2. Sampling $U \sim U[0, 1]$

3. Computing $k = \frac{\pi(u)}{cq(y)}$

4. Accepting $y$ if $U < k$ and rejecting otherwise. 

This says draw sample a point $y$ according to the density $q$. Draw a vertical line at $y$ from the $x$-axis to $cq(y)$. Sample uniformly on this line. If the uniformly random sample is below $q$, then accept it. Otherwise, reject it. The theory behind this is as follows. Suppose we sample some point y according to this algorithm and we want to work out its density $f$, then 
$$
f(y) \propto q(y)\pi(U < k) = q(y)\frac{\pi(u)}{cq(y)} = \frac{\pi(u)}{c}.
$$
Therefore, $f = p$. 

::: {.example}
Suppose we want to sample from a distribution that has the density
$$
\pi(y) = \begin{cases}
\frac{3}{4}y(2-y), \qquad y \in [0, 2] \\
0, \qquad \textrm{otherwise}
\end{cases}.
$$
This has a maximum at $\frac{3}{4}$. We choose $p \sim U[0, 1]$ and $c = \frac{3}{2}$. The R code below shows a pictorial version of how one sample is generated. 
```{r}
set.seed(1234)   #to reproduce
scaling.c <- 3/2         #set c
y <- runif(1, 0, 2)    #sample Y ~ Q
p <- 3/4*y*(2-y) #compute pi(y)
k <- p/(scaling.c*1/2)     #compute k
u <- runif(1)    #sample U ~ U[0, 1]
ifelse(u < k, 'accept', 'reject') #Accept if  u < k


#Create nice plot
a <- seq(0, 2, 0.01)
b <- 3/4*a*(2-a)
scaling.c  <- scaling.c*rep(1, length(a))
plot(a, b, ylim = c(0, 3/2), type = 'l')
lines(a, scaling.c)
segments(x0 = y, y0 = 0, x1 = y,  y1 =3/4*y*(2-y) , 
          lty = 2, lwd = 2)
segments(x0 = y,  y0 =3/4*y*(2-y), x1 = y, y1 = 3/2, lty = 2, 
          col = 2, lwd = 2)
points(x = y, y = u, pch = 19)

```
The plot also shows how the choices of $c$ and $q$ can make the sampling more or less efficient. In our example, the rejection space is large, meaning many of our proposed samples will be rejected. Here, we could have chosen a better $q$ to minimise this space. 
:::

::: {.example}
Suppose we want to sample from a Beta(4, 8) distribution. This distribution looks like
```{r}
x <- seq(0, 1, 0.001)
y <- dbeta(x, 4, 8)
```
A uniform distribution on [0, 1] will cover the Beta distribution and we can then use a rejection sampling algorithm. First, we need to find $c$, the maximum of the Beta distribution. We can find this by differentiating the pdf and setting it equal to 0:
$$
\pi(x) = \frac{1}{B(4, 8)}x^3(1-x)^7 \\
\implies \frac{d \pi(x)}{d x} = 3x^2(1-x)^7 - 7x^3(1-x)^6 \\
\implies \frac{d \pi(x)}{d x} = x^2(1-x)^7(3-10x).
$$
Setting this equal to 0 gives us the maximum at $x = \frac{3}{10}$. This means we can set $c = \frac{3}{10}$. The value of $k$ can be simplified to $k = \frac{10}{3B(4, 8)}u^3(1-u)^7$. Our rejection sampling algorithm is therefore

1. Sample $u \sim U[0, 1]$
2. Compute $k = \frac{10}{3B(4, 8)}u^3(1-u)^7$
3. Accept $u$ with probability $k$. 
:::


::: {.example}
In this example, we want to sample from a Gamma(3, 2) distribution. The density function is shown below.
```{r}
x <- seq(0, 6, 0.01)
y <- dgamma(x, 3, 2)
```

We will use an Exp(1) distribution as our proposal distribution. To find the value of $c$, consider $R(x) = \frac{\pi(x)}{q(x)}$
$$
R(x) = \frac{\frac{2^3}{\Gamma(3)}x^2\exp(-2x)}{\exp(-x)} \\
 =  \frac{2^3}{\Gamma(3)}x^2 \exp(-x)
$$
To find the maximum of this ratio, we differentiate and set the result equal to 0. 
$$
\frac{dR}{dx} = 2x\exp(-x) - x^2\exp(-x)\\
= x\exp(-x)(2 - x)
$$
The maximum is therefore at 2. The value of the ratio at $x = 2$ is $R(2) = \frac{2^3}{\Gamma(3)}4\exp(-2)$. This is therefore our value of c. We can see how $\pi(x)$ and $cq(x)$ look of the graph below. 
```{r}
x <- seq(0, 6, 0.01)
y <- dgamma(x, 3, 2)
scaling.c <- 2^3/gamma(3)*4*exp(-2)
q <- dexp(x, 1)

plot(x, y, type = 'l')
lines(x, q*scaling.c, col = 2, lty = 2)
```

The exponential distribution completely encloses the gamma distribution using this value of $c$, wiht the two densities just touching at $x=2$. Our rejection sampling algorithm is therefore

1. Sample $u \sim \hbox{exp}(1)$
2. Compute $k = \frac{\pi(u)}{cq(u)}$
3. Accept $u$ with probability $k$. 
:::

### Rejection Sampling Efficiency
Suppose we are using a rejection sampling algorithm to sample from $f(y)$ using the proposal distribution $g(y)$. How good is our rejection sampling algorithm? What does it mean to be a 'good' rejection sampling algorithm. One measure of the efficiency of a sampler is, on average, how many samples to we need to generate until one is accepted. 

::: {.proposition}
The number of samples proposed in a rejection sampling algorithm before one is accepted is distributed geometrically with mean $\frac{1}{M}$. 
:::

::: {.proof}
Given a proposed value $y$, and let $A$ be the event of sample is accepted. The probability a sample is accepted, given it takes the value $y$ is 
$$
A \mid y \sim \hbox{Bernoulli}\left(\frac{f(y)}{Mg(y)}\right).
$$

By the tower property
\begin{align*}
E(A) & = E(E(A\mid y)) \\
& = E\left(\frac{f(y)}{Mg(y)}\right) \\
& = \int \frac{f(y)}{Mg(y)} g(y) dy \\
& = \frac{1}{M}\int f(y) dy \\
& = \frac{1}{M}.
\end{align*}

Therefore, the number of samples proposed before one is accepted is distributed geometrically with mean $\frac{1}{M}$. 
:::

## Ziggurat Sampling
The final method we are going to look at in this chapter is Ziggurat sampling. It is used to sample from a $N(0, 1)$ distribution using samples from a $U(0, 1)$ distribution. A Ziggurat is a kind of stepped pyramid. To start the sampling algorithm, we approximate the normal distribution by a series of horizontal rectangles, each with the same area. An example of this is shown below. 

```{r echo = FALSE}
x <- seq(0, 5, 0.01)
y <- dnorm(x, 0, 1)
plot(x, y, type = 'l', col = "blue", lwd = 3)

table <- list()
table$x <- c(2.093446, 2.093446, 1.676493, 1.393368, 1.100918, 0.765337)
table$y <- c(0.044592, 0.097856, 0.151121, 0.217632, 0.297658, 0.398943)

segments(x0 = 0, x1 = table$x, y0= table$y, y1 = table$y)
segments(x0 = table$x[2:6], x1 = table$x[2:6] , y0= table$y[1:5], y1 = table$y[2:6])
segments(x0 = table$x[1], x1 = 2.5, y0 = table$y[1], y1 = table$y[1])
segments(x0 = table$x[1], x1 = 2.5, y0 = table$y[1], y1 = table$y[1])
segments(x0 = 2.5, x1 = 2.5, y0 = 0, y1 = table$y[1])
```
To generate a sample, we choose a rectangle at random -- we might do this by sampling on the y-axis uniformly at random. We then sample uniformly at random within the rectangle. We accept the sample if it is 'clearly' in the box. By clearly, we mean that is is also contained in the rectangle above. If it is, not contained in the rectangle above, we generate a new uniform random number to accept or reject the sample. 

Suppose the bottom right corner of the $i^{th}$ rectangle has coordinates $(x_i, y_i)$. The Ziggurat algorithm is 

1. Sample a rectangle $i$ uniformly at random
2. Sample $u_0 \sim U[0, 1]$ and set $x = u_0x_i$
3. If $x < x_{i+1}$ accept the sample
4. Sample $u_1 \sim U[0, 1]$ and set $y = y_i + u_1(y_{i+1} − y_i)$
5. If $y < f(y)$, where $f$ is the density function of a standard normal distribution, then accept the sample.


This only generates samples for the positive side of the normal distribution. To generate samples from the full distribution, we multiply each sample by -1 with probability a half. 


## Lab

The aim of this lab is to code up some sampling methods. You have already done some similar work in lab 1 (e.g. estimating $\pi$). 

::: {.exercise}
Visit random.org to generate some truly random numbers. How does this site generate numbers that are truly random?
:::

::: {.exercise}
A random variable $X$ has density $\pi(x) = ax^2$ for $x\in[0,1]$ and 0 otherwise.  Find the value $a$. Use the inverse transform method to generate 10,000 samples from this distribution. Plot a histogram against the true density function. 
:::

::: {.exercise}
Let $X$ have the density $\pi(x) = \frac{1}{\theta}x^{\frac{1-\theta}{\theta}}$ for $x \in [0, 1]$. Use the inverse transform method to generate 10,000 samples from this distribution with $\theta = \{1, 5, 10\}$. 
:::

::: {.exercise}
Let $X$ have the density $\pi(x) = 3x^2$ for $x \in [0, 1]$ and 0 otherwise. Use a rejection sampling method to generate 10,000 from this distribution. Plot the results against the true density to check you have the same distribution. 
:::

::: {.exercise}
The half normal distribution with mean 0 and variance 1 has density function 
$$
\pi(x) = \frac{2}{\sqrt{2\pi}}\exp{(-x^2/2)}
$$
for $x \geq 0$ and 0 otherwise.

1. Denote the exponential density function by $q(x) = \lambda \exp(-\lambda x)$. Find the smallest $c$ such that  $\pi(x)/q(x) < c$. 
2. Use a rejection sampling algorithm with an exponential proposal distribution to generate samples from the half normal distibrution with mean 0 and variance 1. 
:::





