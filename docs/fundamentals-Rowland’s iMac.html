<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Fundamentals of Bayesian Inference | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Fundamentals of Bayesian Inference | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Fundamentals of Bayesian Inference | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Rowland Seymour" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="programming-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#getting-help"><i class="fa fa-check"></i><b>0.4</b> Getting Help</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.5</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.6</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-probability"><i class="fa fa-check"></i><b>1.3</b> Bayesian Probability</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#conditional-probability-and-exchangability"><i class="fa fa-check"></i><b>1.4</b> Conditional Probability and Exchangability</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.5</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="1.6" data-path="fundamentals.html"><a href="fundamentals.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-binomial-distribution"><i class="fa fa-check"></i><b>3.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting Conclsuions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-exponential-distribution"><i class="fa fa-check"></i><b>3.3</b> The Exponential Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-normal-distribtuion"><i class="fa fa-check"></i><b>3.4</b> The Normal Distribtuion</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.5</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.6</b> Prediction</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.7</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bernstein-von-mises-theorem"><i class="fa fa-check"></i><b>3.8</b> Bernstein-von-Mises Theorem</a></li>
<li class="chapter" data-level="3.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exercises-1"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.4</b> Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>4.5</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#metropolis-hastings"><i class="fa fa-check"></i><b>4.6</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.7" data-path="sampling.html"><a href="sampling.html#gibbs-sampler"><i class="fa fa-check"></i><b>4.7</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="4.8" data-path="sampling.html"><a href="sampling.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>4.8</b> MCMC Diagnostics</a></li>
<li class="chapter" data-level="4.9" data-path="sampling.html"><a href="sampling.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>5</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>5.1</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>5.1.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="5.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>5.1.2</b> Imputing Latent Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>5.2</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>5.2.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="5.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>5.2.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-data-augmenatation"><i class="fa fa-check"></i><b>5.3</b> Lab: Data Augmenatation</a></li>
<li class="chapter" data-level="5.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-gaussian-processes"><i class="fa fa-check"></i><b>5.4</b> Lab: Gaussian Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fundamentals" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Fundamentals of Bayesian Inference<a href="fundamentals.html#fundamentals" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Bayesian inference is built on a different way of thinking about parameters of probability distributions than methods you have learnt so far. In the past 30 years or so, Bayesian inference has become much more popular. This is partly due to increased computational power becoming available. In this first chapter, we are going to set out to answer:</p>
<ol style="list-style-type: decimal">
<li><p>What are the fundamental principles of Bayesian inference?</p></li>
<li><p>What makes Bayesian inference different from other methods?</p></li>
</ol>
<div id="statistical-inference" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Statistical Inference<a href="fundamentals.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The purpose of statistical inference is to “draw conclusions, from numerical data, about quantities that are not observed” (Bayesian Data Analysis, chapter 1). Generally speaking, there are two kinds of inference:</p>
<ol style="list-style-type: decimal">
<li>Inference for quantities that are unobserved or haven’t happened yet. Examples of this might be the size of a payout an insurance company has to make, or a patients outcome in a clinical trial had they been received a certain treatment.</li>
<li>Inference for quantities that are not possible to observe. This is usual because they are part of modelling process, like parameters in a linear model.</li>
</ol>
<p>In this module, we are going to look at a different way of carrying out statistical inference, one that doesn’t depend on long run events. Instead, we’re going to introduce the definition of probability that allows us to interpret the subjective chance that an event occurs.</p>
</div>
<div id="frequentist-theory" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Frequentist Theory<a href="fundamentals.html#frequentist-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Frequentist probability is built upon the theory on long run events. Probabilities must be interpretable as frequencies over multiple repetitions of the experiment that is being analysed, and are calculated from the sampling distributions of measured quantities.</p>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  </strong></span>The long run relative frequency of an event is the <strong>probability</strong> of that event.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 1.1  </strong></span>If a frequentist wanted to assign a probability to rolling a 6 on a particular dice, then they would roll the dice a large number of times and compute the relative frequency.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.2  </strong></span>The <strong>sampling distribution</strong> of a statistic is the distribution based on a long run of samples of a fixed size from the population.</p>
</div>
<p>The sampling distribution is an important concept in frequentist theory as it describes the randomness in the process. From a frequentist standpoint, we have a model containing some parameter <span class="math inline">\(\theta\)</span> and some data <span class="math inline">\(\boldsymbol{y}\)</span>. All the evidence in the data <span class="math inline">\(\boldsymbol{y}\)</span> about <span class="math inline">\(\theta\)</span> is contained in the likelihood function <span class="math inline">\(\pi(\boldsymbol{y} \mid \theta)\)</span>. The parameter <span class="math inline">\(\theta\)</span> is fixed and the likelihood function describes the probability of observing the data <span class="math inline">\(\boldsymbol{y}\)</span> given the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>The most common way to estimate the value of <span class="math inline">\(\theta\)</span> is using maximum likelihood estimation. Although other methods do exist (e.g. method of moments, or generalised maximum likelihood estimation).</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 1.3  </strong></span>The maximum likelihood estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat{\theta}\)</span>, is the value such that <span class="math inline">\(\hat{\theta} = \max_{\theta} \pi(\boldsymbol{y} \mid \boldsymbol{y})\)</span>.</p>
</div>
<p>Uncertainty around the maximum likelihood estimate is based on the theory of long running events that underpin frequentist theory.</p>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.4  </strong></span>Let <span class="math inline">\(X\)</span> be a random sample from a probability distribution <span class="math inline">\(\theta\)</span>. A <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>confidence interval</strong> for <span class="math inline">\(\theta\)</span> is an interval <span class="math inline">\((u(Y), v(Y))\)</span> such that
<span class="math display">\[
\mathbb{P}(u(Y) &lt; \theta &lt; v(Y)) = 1-\alpha
\]</span></p>
</div>
<p>This means that if you had an infinite number of samples for <span class="math inline">\(Y\)</span> and the corresponding infinite number of confidence intervals, then <span class="math inline">\(100(1-\alpha)\)</span>% of them would contain the true value of <span class="math inline">\(\theta\)</span>. It does <em>not</em> mean that there is a <span class="math inline">\(100(1-\alpha)\)</span> probability a particular interval contains the true value of <span class="math inline">\(\theta\)</span>.</p>
<p>Given that we want to understand the properties of <span class="math inline">\(\theta\)</span> given the data we have observed <span class="math inline">\(\boldsymbol{y}\)</span>, then you might think it makes sense to investigate the distribution <span class="math inline">\(\pi(\theta \mid \boldymbold{y})\)</span>. This distribution says what are the likely values of <span class="math inline">\(\theta\)</span> given the information we have observed from the data <span class="math inline">\(\boldsmybol{y}\)</span>. We will talk about Bayes’ theorem in more detail later on in this chapter, but, for now, we will use it to write down this distribution
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) = \frac{\pi(\boldsymbol{y} \mid \theta)\pi(\theta)}{\pi(\boldsymbol{y})}.
\]</span>
This is where frequentist theory cannot help us, particularly the term <span class="math inline">\(\pi(\theta)\)</span>. Randomness can only come from the data, so how can we assign a probability distribution to a constant <span class="math inline">\(\theta\)</span>? The term <span class="math inline">\(\pi(\theta)\)</span> is meaningless under this philosophy. Instead, we turn to a different philosophy where we can assign a probability distribution to <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="bayesian-probability" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Bayesian Probability<a href="fundamentals.html#bayesian-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Bayesian paradigm is built around a different definition of probability. This allows us to generate probability distirbutuions for parameters values.</p>
<div class="definition">
<p><span id="def:unlabeled-div-6" class="definition"><strong>Definition 1.5  </strong></span>The subjective belief of an event is the <strong>probability</strong> of that event.</p>
</div>
<p>This definition means we can assign probabilities to events that frequentists do not recognise as valid.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 1.2  </strong></span>Consider the following:</p>
<ol style="list-style-type: decimal">
<li><p>The probability that I vote for the labour party at the next election</p></li>
<li><p>A photo taken from the James Watt telescope contains a new planet.</p></li>
<li><p>The real identify of Banksy is Robin Gunningham.</p></li>
</ol>
<p>These are not events that can be repeated in the long run.</p>
</div>
</div>
<div id="conditional-probability-and-exchangability" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Conditional Probability and Exchangability<a href="fundamentals.html#conditional-probability-and-exchangability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we derive Bayes’ theorem, we recap some important definitions in probability.</p>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>Definition 1.6  </strong></span>Given two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the <strong>conditional probability</strong> that event <span class="math inline">\(A\)</span> occurs given the event <span class="math inline">\(B\)</span> has already occurred is
<span class="math display">\[
\pi(A \mid B) = \frac{\pi(A \cap B)}{\pi(B)},
\]</span>
when <span class="math inline">\(\pi(B) &gt; 0\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>Definition 1.7  </strong></span>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>independent</strong> given event <span class="math inline">\(C\)</span> if and only if
<span class="math display">\[ \pi(A \cap B \mid C) = \pi(A \mid C)\pi(B \mid C).\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 1.8  </strong></span>Let <span class="math inline">\(\pi(y_1, \ldots, y_N)\)</span> be the joint density of <span class="math inline">\(Y_1, \ldots, Y_N\)</span>. If <span class="math inline">\(\pi(y_1, \ldots, y_N) = \pi(y_{\pi_1}, \ldots, y_{\pi_N})\)</span> for a permutations <span class="math inline">\(\pi\)</span> of <span class="math inline">\(\{1, \ldots, N\}\)</span>, then <span class="math inline">\(Y_1, \ldots, Y_N\)</span> are <strong>exchangeable</strong>.</p>
</div>
<p>Exchangability means that the labels of the random variables don’t contain any information about the outcomes. This is an important idea in many areas of probability and statistics, and we often model exchangeable events as iid.</p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 1.3  </strong></span>If <span class="math inline">\(Y_i \sim Bin(n, p)\)</span> are independent and identically distributed for <span class="math inline">\(i = 1, 2, 3\)</span>, then <span class="math inline">\(\pi(Y_1, Y_2, Y_3) = \pi(Y_3, Y_1, Y_2)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 1.4  </strong></span>Let <span class="math inline">\((X, Y)\)</span> follow a bivariate normal distribution with mean <strong>0</strong>, variances <span class="math inline">\(\sigma_x = \sigma_y = 1\)</span> and a correlation parameter <span class="math inline">\(\rho \in [-1, 1]\)</span>. <span class="math inline">\((X, Y)\)</span> are exchangable, but only independent if <span class="math inline">\(\rho = 0\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-13" class="proposition"><strong>Proposition 1.1  </strong></span>If <span class="math inline">\(\theta \sim \pi(\theta)\)</span> and <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> from a sample space <span class="math inline">\(\mathcal{Y}\)</span> are conditionally iid given some parameter <span class="math inline">\(\theta\)</span>, then marginally <span class="math inline">\(Y_1, \ldots, Y_N\)</span> are exchangable.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>Suppose <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> are conditionally iid given some parameter <span class="math inline">\(\theta\)</span>. Then for any permutation <span class="math inline">\(\pi\)</span> of <span class="math inline">\(\{1, \ldots, N\}\)</span> and observations <span class="math inline">\(\{y_1, \ldots, y_N\}\)</span>
<span class="math display">\[\begin{equation}
\begin{split}
\pi(y_1, \ldots, y_N) &amp;= \int \pi(y_1, \ldots, y_N \mid \theta) \pi(\theta)\, d\theta \qquad \textrm{(definition of marginal distribution)}\\
&amp; = \int \left\{\prod_{i=1}^N\pi(y_i \mid \theta)\right\} \pi(\theta)\, d\theta \qquad \textrm{(definition of conditionally iid)}\\
&amp; = \int \left\{\prod_{i=1}^N\pi(y_{\pi_i} \mid \theta)\right\} \pi(\theta)\, d\theta \qquad \textrm{(product is commutative)} \\
&amp; = \pi(y_{\pi_1}, \ldots, y_{\pi_N}) \qquad \textrm{(definition of marginal distribution)}
\end{split}
\end{equation}\]</span></p>
</div>
<p>This tells us that if we have some conditionally iid random variables and a subjective prior belief about some parameter <span class="math inline">\(\theta\)</span>, then we have exchangeability. This is nice to have, but the implication in the other direction is much more interesting and powerful.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 1.1  (de Finetti) </strong></span>If a sequence of random variables <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> from a sample space <span class="math inline">\(\mathcal{Y}\)</span> is exchangeable, then its joint distribution can be written as
<span class="math display">\[
\pi(y_1, \ldots, y_N) = \int \left\{\prod_{i=1}^N\pi(y_i \mid \theta)\right\} \pi(\theta)\, d\theta
\]</span>
for some parameter <span class="math inline">\(\theta\)</span>, some distribution on <span class="math inline">\(\theta\)</span>, and some sampling model <span class="math inline">\(\pi(y_i \mid \theta)\)</span>.</p>
</div>
<p>This is a kind of existence theorem for Bayesian inference. It says that if we have exchangeable random varibales, then a parameter <span class="math inline">\(\theta\)</span> must exist and a subjective probability distribution <span class="math inline">\(\pi(\theta)\)</span> must also exist. The argument against Bayesian inference is that it doesn’t guarantee a <em>good</em> subjective probability distribution <span class="math inline">\(\pi(\theta)\)</span> exists.</p>
</div>
<div id="bayes-theorem" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Bayes’ Theorem<a href="fundamentals.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we have an understanding of conditional probability and exchangeability, we can put these two together to understand Bayes’ Theorem. Bayes’ theorem is concerned with the distribution of the parameter <span class="math inline">\(\theta\)</span> given some observed data <span class="math inline">\(y\)</span>. It tries to answer the question: what does the data tell us about the model parameters?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-16" class="theorem"><strong>Theorem 1.2  (Bayes) </strong></span>The distribution of the model parameter <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(y\)</span> is
<span class="math display">\[
\pi(\theta \mid y) = \frac{\pi(y \mid \theta)\pi(\theta)}{\pi(y)}
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align}
\pi(\theta \mid y) &amp;= \frac{\pi(\theta, y)}{\pi(y)}\\
\implies \pi(\theta, y) &amp;= \pi(\theta \mid y)\pi(y)
\end{align}\]</span>
Analogously, using <span class="math inline">\(\pi(y \mid \theta)\)</span> we can derive
<span class="math display">\[
\pi(\theta, y) = \pi(y \mid \theta)\pi(\theta)
\]</span>
Putting these two terms equal to each other and dividing by <span class="math inline">\(\pi(y)\)</span> gives
<span class="math display">\[
\pi(\theta \mid y) = \frac{\pi(y \mid \theta)\pi(\theta)}{\pi(y)}
\]</span></p>
</div>
<p>There are four terms in Bayes’ theorem:</p>
<ol style="list-style-type: decimal">
<li>The <strong>posterior distribution</strong> <span class="math inline">\(\pi(\theta \mid y)\)</span>. This tells us our belief about the model parameter <span class="math inline">\(\theta\)</span> given the data we have observed <span class="math inline">\(y\)</span>.</li>
<li>The <strong>likelihood function</strong> <span class="math inline">\(\pi(y \mid \theta)\)</span>. The likelihood function is common to both frequentist and Bayesian methods. By the likelihood principle, the likelihood function contains all the information the data can tell us about the model parameter <span class="math inline">\(\theta\)</span>.</li>
<li>The <strong>prior distribution</strong> <span class="math inline">\(\pi(\theta)\)</span>. This is the distribution that describes our prior beliefs about the value of <span class="math inline">\(\theta\)</span>. The form of <span class="math inline">\(\theta\)</span> should be decided before we see the data. It may be a vague distribution (e.g. <span class="math inline">\(\theta \sim N(0, 10^2)\)</span>) or a specific distribution based on prior information from experts (e.g. <span class="math inline">\(\theta \sim N(5.5, 1.3^2)\)</span>).<br />
</li>
<li>The <strong>evidence of the data</strong> <span class="math inline">\(\pi(y)\)</span>. This is sometimes called the average probability of the data or the marginal likelihood. In practice, we do not need to derive this term as it can be back computed to ensure the posterior distribution sums/integrates to one.</li>
</ol>
<p>A consequence of point four is that posterior distributions are usually derived proportionally, and (up to proportionality) Bayes’ theorem
<span class="math display">\[
\pi(\theta \mid y) \propto \pi(y\mid\theta)\pi(\theta).
\]</span></p>
<blockquote>
<p><strong>Some history of Thomas Bayes</strong>. Thomas Bayes was an English theologean born in 1702. His “Essay towards solving a problem in the doctrine of chances” was published posthumously. It introduces theroems on conditional probability and the idea of prior probability. He discusses an experiment where the data can be modelled using the Binomial distribution and he guesses (places a prior distribution) on the probability of success.</p>
</blockquote>
<blockquote>
<p>Richard Price sent Bayes’ work to the Royal Society two years after Bayes had died. In his commentary on Bayes’ work, he suggested that the Bayesian way of thinking proves the existance of God, stating: The purpose I mean is, to show what reason we have for believing that there are in the constitution of things fixt laws according to which things happen, and that, therefore, the frame of the world must be the effect of the wisdom and power of an intelligent cause; and thus to confirm the argument taken from final causes for the existence of the Deity.</p>
</blockquote>
<blockquote>
<p>It’s not clear how Bayesian Thomas Bayes actually was, as his work was mainly about specific forms of probability theory and not his intepretation of it. The Bayesian way of thinking was really popularised by Laplace, who wrote about deductive probability in the early 19th century.</p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="Thomas_Bayes.gif" alt="Probably not Thomas Bayes, but the best we have got. Courtesy of Wikipedia https://en.wikipedia.org/wiki/Thomas_Bayes."  />
<p class="caption">
Figure 1.1: Probably not Thomas Bayes, but the best we have got. Courtesy of Wikipedia <a href="https://en.wikipedia.org/wiki/Thomas_Bayes" class="uri">https://en.wikipedia.org/wiki/Thomas_Bayes</a>.
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 1.5  </strong></span>We finish this chapter with a very simple example. The advantage of the example being so simple is that we can obtain plots in R that show what’s going on.</p>
<p>Suppose we have a model <span class="math inline">\(y \sim N(\theta, 1)\)</span> and we want to estimate <span class="math inline">\(\theta\)</span>. To do this we need to derive the posterior distribution. By Bayes’ theorem,
<span class="math display">\[
\pi(\theta \mid y) \propto \pi(y \mid \theta) \pi(\theta).
\]</span>
We know the form of <span class="math inline">\(\pi(y \mid \theta) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(y - \theta)^2}\)</span>, but how should we describe our prior beliefs about <span class="math inline">\(\theta\)</span>? Here are three options:</p>
<ol style="list-style-type: decimal">
<li><p>We can be very vague about <span class="math inline">\(\theta\)</span> – we genuinely don’t know about its value. We assign a uniform prior distribution to <span class="math inline">\(\theta\)</span> that takes values between -1,000 and +1,000, i.e. <span class="math inline">\(\theta \sim u[-1000, 1000]\)</span>. Up to proportionality <span class="math inline">\(\pi(\theta) \propto 1\)</span> for <span class="math inline">\(\theta \in [-1000, 1000]\)</span>.</p></li>
<li><p>After thinking hard about the problem, or talking to an expert, we decide that the only thing we know about <span class="math inline">\(\theta\)</span> is that it can’t be negative. We adjust our prior distribution from 1. to be <span class="math inline">\(\theta \sim u[0, 1000]\)</span>. Up to proportionality <span class="math inline">\(\pi(\theta) \propto 1\)</span> for <span class="math inline">\(\theta \in [0, 1000]\)</span>.</p></li>
<li><p>We decide to talk to a series of experts about <span class="math inline">\(\theta\)</span> asking for their views on likely values of <span class="math inline">\(\theta\)</span>. Averaging the experts opinions gives <span class="math inline">\(\theta \sim N(3, 0.7^2)\)</span>. This is a method known as prior elicitation.</p></li>
</ol>
<p>We now go and observe some data. After a lot of time and effort, we collect one data point – <span class="math inline">\(y = 0\)</span>.</p>
<p>Now we have all the ingredients to construct the posterior distribution. We multiply the likelihood function evaluated at <span class="math inline">\(y = 0\)</span> by each of the three prior distributions. This gives us the posterior distributions (up to proportionality).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="fundamentals.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#The likelihood function is the normal PDF</span></span>
<span id="cb1-2"><a href="fundamentals.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#To illustrate this, we evaluate this from [-5, 5].</span></span>
<span id="cb1-3"><a href="fundamentals.html#cb1-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-4"><a href="fundamentals.html#cb1-4" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-5"><a href="fundamentals.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="fundamentals.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#The first prior distribution we try is a </span></span>
<span id="cb1-7"><a href="fundamentals.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#uniform [-1000, 1000] distribution. This is a </span></span>
<span id="cb1-8"><a href="fundamentals.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#vague prior distribution. </span></span>
<span id="cb1-9"><a href="fundamentals.html#cb1-9" aria-hidden="true" tabindex="-1"></a>uniform.prior <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(x))</span>
<span id="cb1-10"><a href="fundamentals.html#cb1-10" aria-hidden="true" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>uniform.prior</span>
<span id="cb1-11"><a href="fundamentals.html#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="fundamentals.html#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="fundamentals.html#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#The second prior distribution we try is a uniform </span></span>
<span id="cb1-14"><a href="fundamentals.html#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#[0, 1000] distribution, i.e. theta is non-negative. </span></span>
<span id="cb1-15"><a href="fundamentals.html#cb1-15" aria-hidden="true" tabindex="-1"></a>step.prior <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb1-16"><a href="fundamentals.html#cb1-16" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>step.prior</span>
<span id="cb1-17"><a href="fundamentals.html#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="fundamentals.html#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="fundamentals.html#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">#The third prior distribution we try is a</span></span>
<span id="cb1-20"><a href="fundamentals.html#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">#specific normal prior distribution. It</span></span>
<span id="cb1-21"><a href="fundamentals.html#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">#has mean 3 and variance 0.7.</span></span>
<span id="cb1-22"><a href="fundamentals.html#cb1-22" aria-hidden="true" tabindex="-1"></a>normal.prior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="fl">0.7</span>)</span>
<span id="cb1-23"><a href="fundamentals.html#cb1-23" aria-hidden="true" tabindex="-1"></a>posterior3 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>normal.prior</span>
<span id="cb1-24"><a href="fundamentals.html#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="fundamentals.html#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">#Now we plot the likelihoods, prior and posterior distributions. </span></span>
<span id="cb1-26"><a href="fundamentals.html#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">#Each row corresponds to a different prior distribution. Each</span></span>
<span id="cb1-27"><a href="fundamentals.html#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">#column corresponds to a part in Bayes&#39; theorem. </span></span>
<span id="cb1-28"><a href="fundamentals.html#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb1-29"><a href="fundamentals.html#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Likelihood&quot;</span>)</span>
<span id="cb1-30"><a href="fundamentals.html#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, uniform.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Prior&quot;</span>)</span>
<span id="cb1-31"><a href="fundamentals.html#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, posterior1, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Posterior&quot;</span>)</span>
<span id="cb1-32"><a href="fundamentals.html#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb1-33"><a href="fundamentals.html#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, step.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-34"><a href="fundamentals.html#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, posterior2, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-35"><a href="fundamentals.html#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb1-36"><a href="fundamentals.html#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, normal.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-37"><a href="fundamentals.html#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, posterior3, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<ol style="list-style-type: decimal">
<li><p>The posterior distribution is proportional to the likelihood function. The prior distribution closely matches frequentist inference. Both the MLE and posterior mean are 0.</p></li>
<li><p>We get a lopsided posterior distribution, that is proportional to the likelihood function for positive values of <span class="math inline">\(\theta\)</span>, but is 0 for negative values of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>We get some sort of average of the likelihood function and the prior distribution. Had we collected more data, the posterior distribution would have been weighted toward the information from the likelihood function more.</p></li>
</ol>
</div>
</div>
<div id="exercises" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Exercises<a href="fundamentals.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-19" class="exercise"><strong>Exercise 1.1  </strong></span>Consider a standard pack of 52 playing cards. You pick a card at random, what is the probability you pick:</p>
<ol style="list-style-type: decimal">
<li>A Queen, given you have picked a picture card (King, Queen, Jack)?</li>
<li>The five of clubs, given you have picked a black card?</li>
<li>A black card, given you have not picked the five of clubs?</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-20" class="solution"><em>Solution</em>. </span>A pack of playing cards is equally divided into four suits: Hearts (red), Diamonds (red), Clubs (black), and Spades (black). Each suit has 13 cards numbered 2 - 10, Jack, Queen, King (all three picture cards), and Ace.</p>
<ol style="list-style-type: decimal">
<li><span class="math display">\[\begin{align*}
P(\hbox{Queen} \mid \hbox{Picture}) &amp;= \frac{P(\hbox{Queen and Picture})}{P(\hbox{Picture)}} \\
&amp; = \frac{4/52}{12/52} \\
&amp; = \frac{1}{3}.
\end{align*}\]</span></li>
<li><span class="math display">\[\begin{align*}
P(\hbox{5 Clubs} \mid \hbox{Black}) &amp;= \frac{P(\hbox{5 Clubs and Black})}{P(\hbox{Black)}} \\
&amp; = \frac{1/52}{1/2} \\
&amp; = \frac{1}{26}.
\end{align*}\]</span></li>
<li><span class="math display">\[\begin{align*}
P(\hbox{Black} \mid \hbox{Not 5 Clubs}) &amp; = \frac{P(\hbox{Black} \mid \hbox{Not 5 Clubs})}{P(\hbox{Not 5 Clubs})} \\
&amp; = \frac{25/52}{51/52}\\
&amp;= \frac{25}{51}.
\end{align*}\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-21" class="exercise"><strong>Exercise 1.2  </strong></span>Decide if each of the following events can be assigned probabilities by frequentists:</p>
<ol style="list-style-type: decimal">
<li>The Bermuda triangle exists.</li>
<li>Getting a 6 when rolling a dice.</li>
<li>Someone will test positive for Covid-19 after contracting the disease.</li>
<li>The sun will rise tomorrow.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-22" class="solution"><em>Solution</em>. </span></p>
<ol style="list-style-type: decimal">
<li>No, this can’t be assigned a probability.</li>
<li>Yes, you can repeatedly roll of dice.</li>
<li>Yes, this can be assigned a probability. You can repeatedly test someone for the disease, hence there is a long-run frequency of the test returning a positive result.</li>
<li>It depends what you mean by tomorrow. Suppose today is 1st January 2023, if tomorrow means 2nd January 2023, then no. 2nd January 2023 will on occur once and there is no long-run frequency. If, however, you define tomorrow by the day after today, then yes. There have been many (The Earth has been going round the Sun for ~4.5 billion years, so approximately 4.5*365 tomorrows), so it can be assigned a probability.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-23" class="exercise"><strong>Exercise 1.3  </strong></span>An urn contains three coins. Two of the coins are fair, but one of the coins has heads on both sides.</p>
<ol style="list-style-type: decimal">
<li>You pick a coin out of the urn without looking and flip it. What’s the probability you get heads?</li>
<li>You pick a coin out of the urn without looking and flip it and get heads. What’s the probability it’s the two-headed coin?</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-24" class="solution"><em>Solution</em>. </span>Label the coins 1, 2, 3, where <span class="math inline">\(P(\hbox{Heads} \mid \hbox{Coin } 1) = P(\hbox{Heads} \mid \hbox{Coin } 2) = \frac{1}{2}\)</span> and <span class="math inline">\(P(\hbox{Heads} \mid \hbox{Coin } 3) = 1\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Using the law of total probability
<span class="math display">\[\begin{align*}
P(\hbox{Heads}) &amp; = P(\hbox{Heads} \mid \hbox{Coin } 1) P(\hbox{Coin } 1) + \\
&amp; \qquad  P(\hbox{Heads} \mid \hbox{Coin } 2) P(\hbox{Coin } 2) + P(\hbox{Heads} \mid \hbox{Coin } 3) P(\hbox{Coin } 3) \\
&amp; = \frac{1}{2}\cdot\frac{1}{3} + \frac{1}{2}\cdot\frac{1}{3} + 1\cdot\frac{1}{3} \\
&amp; = \frac{2}{3}.
\end{align*}\]</span></p></li>
<li><p>Using Bayes’ theorem
<span class="math display">\[\begin{align*}
P(\hbox{Coin 1}\mid \hbox{Heads}) &amp;= \frac{P(\hbox{Heads} \mid \hbox{Coin 1})P(\hbox{Coin 1})}{P(\hbox{Heads})} \\
&amp; = \frac{1\cdot\frac{1}{3}}{\frac{2}{3}} \\
&amp; = \frac{1}{2}.
\end{align*}\]</span></p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-25" class="exercise"><strong>Exercise 1.4  </strong></span>You see a sponsored post online with the word <em>bitcoin</em> in. You want to work out the probability the post is spam.</p>
<ol style="list-style-type: decimal">
<li>Using the law of total probability, show the probability the post is spam, given it contains the word probability is
<span class="math display">\[
  \pi(\textrm{spam} \mid \textrm{bitcoin}) = \frac{\pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam})}{\pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam}) + \pi(\textrm{bitcoin} \mid \textrm{not spam})\pi(\textrm{not spam})}
  \]</span></li>
<li>Most spam filters take a naive approach and set
<span class="math display">\[
  \pi(\textrm{spam}) =\pi(\textrm{not spam}) = \frac{1}{2}.
  \]</span>
If an post is known to be spam, there’s an 80% chance it contains the word bitcoin. If an post is not spam, then there’s a 1% chance it contains the word bitcoin. Calculate the probability the post is spam given it contains bitcoin.</li>
<li>Suppose you take a much more pessimistic view, and assume that 80% of all sponsered posts are spam. Recalculate the probability the post is spam given it contains bitcoin.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-26" class="solution"><em>Solution</em>. </span>By Bayes’ theorem, we have
<span class="math display">\[
\pi(\textrm{spam} \mid \textrm{bitcoin}) = \frac{\pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam})}{\pi(\textrm{bitcoin})}.
\]</span>
1. By the law of total probability,
<span class="math display">\[
\pi(\textrm{bitcoin}) = \pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam}) + \pi(\textrm{bitcoin} \mid \textrm{not spam})\pi(\textrm{not spam}).
\]</span>
Thus,
<span class="math display">\[
  \pi(\textrm{spam} \mid \textrm{bitcoin}) = \frac{\pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam})}{\pi(\textrm{bitcoin} \mid \textrm{spam})\pi(\textrm{spam}) + \pi(\textrm{bitcoin} \mid \textrm{not spam})\pi(\textrm{not spam})}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>From the question, we have <span class="math inline">\(\pi(\textrm{spam}) =\pi(\textrm{not spam}) = \frac{1}{2}\)</span>, <span class="math inline">\(\pi(\textrm{bitcoin} \mid \textrm{spam}) = 0.8\)</span> and <span class="math inline">\(\pi(\textrm{bitcoin} \mid \textrm{not spam}) = 0.01\)</span>. Plugging these into the probability gives
<span class="math display">\[
  \pi(\textrm{spam} \mid \textrm{bitcoin}) = \frac{80}{81} \approx 98.7\%.
\]</span></li>
<li>This time <span class="math inline">\(\pi(\textrm{spam}) = 0.8\)</span> and <span class="math inline">\(\pi(\textrm{not spam}) = 0.2\)</span>, which yields
<span class="math display">\[
  \pi(\textrm{spam} \mid \textrm{bitcoin}) = \frac{320}{321} \approx 99.7\%.
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-27" class="exercise"><strong>Exercise 1.5  </strong></span>You are working on a project investigating pollution related illnesses in the West Midlands. I have sampled the proportion of people with pollution related illnesses in five areas of the West Midlands, <span class="math inline">\(y_1, \ldots, y_5\)</span>, with nothing to distinguish the data. This exercise is about the last data point <span class="math inline">\(y_5\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Should you model these data points exchangeably?</li>
<li>I now tell you the first four of these rates (0.72, 1.00, 0.85, 0.78 per 100,000). Should you continue to model these data points exchangeably?</li>
<li>Now, suppose instead of telling you these four rates, I had told you the five areas I have information about are Birmingham City Centre, Smethwick, Edgbaston, Dorridge, and Sutton Coldfield. Should you continue to model these data points exchangeably?</li>
<li>Now suppose I give you the data in part 2 and say that the missing data point <span class="math inline">\(y_5\)</span> is Birmingham City Centre. Should you continue to model these data points exchangeably?</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-28" class="solution"><em>Solution</em>. </span>This is based on Gelman et. al (2013, p. 105).</p>
<ol style="list-style-type: decimal">
<li>Yes, you have no information to distinguish the data points so exchangability seems a reasonable assumption.</li>
<li>Yes, you still have no information to distinguish between any of the data points.</li>
<li>Yes, the joint distribution of these variables doesn’t depend on the labels. However, you may start to formulate prior beliefs. Birmingham City Centre and Smethwick are likely to have higher rates of air pollution than leafy Dorridge.</li>
<li>No, these can no longer be modelled exchangably. You have reason to believe that Birmingham City Centre is likely to have a substantially higher value than the rest. That is you have information about <span class="math inline">\(\pi(y_5 &gt; \max(y_1, y_2, y_3, y_4) \mid y_1, y_2, y_3, y_4)\)</span>, so exchangability is no longer a suitable assumption.</li>
</ol>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="programming-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
