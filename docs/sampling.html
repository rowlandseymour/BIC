<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Sampling | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Sampling | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Sampling | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Rowland Seymour" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-inference.html"/>
<link rel="next" href="advanced-computation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#getting-help"><i class="fa fa-check"></i><b>0.4</b> Getting Help</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.5</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.6</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-probability"><i class="fa fa-check"></i><b>1.3</b> Bayesian Probability</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#conditional-probability-and-exchangability"><i class="fa fa-check"></i><b>1.4</b> Conditional Probability and Exchangability</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.5</b> Bayesâ€™ Theorem</a></li>
<li class="chapter" data-level="1.6" data-path="fundamentals.html"><a href="fundamentals.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-binomial-distribution"><i class="fa fa-check"></i><b>3.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting Conclsuions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-exponential-distribution"><i class="fa fa-check"></i><b>3.3</b> The Exponential Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-normal-distribtuion"><i class="fa fa-check"></i><b>3.4</b> The Normal Distribtuion</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.5</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.6</b> Prediction</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.7</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bernstein-von-mises-theorem"><i class="fa fa-check"></i><b>3.8</b> Bernstein-von-Mises Theorem</a></li>
<li class="chapter" data-level="3.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exercises-1"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.4</b> Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>4.5</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#metropolis-hastings"><i class="fa fa-check"></i><b>4.6</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.7" data-path="sampling.html"><a href="sampling.html#gibbs-sampler"><i class="fa fa-check"></i><b>4.7</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="4.8" data-path="sampling.html"><a href="sampling.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>4.8</b> MCMC Diagnostics</a></li>
<li class="chapter" data-level="4.9" data-path="sampling.html"><a href="sampling.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>5</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>5.1</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>5.1.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="5.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>5.1.2</b> Imputing Latent Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>5.2</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>5.2.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="5.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>5.2.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-data-augmenatation"><i class="fa fa-check"></i><b>5.3</b> Lab: Data Augmenatation</a></li>
<li class="chapter" data-level="5.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-gaussian-processes"><i class="fa fa-check"></i><b>5.4</b> Lab: Gaussian Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sampling" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Sampling<a href="sampling.html#sampling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="uniform-random-numbers" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Uniform Random Numbers<a href="sampling.html#uniform-random-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What we wonâ€™t be doing in this module is generating true uniform random numbers. This is incredibly difficult and usually requires lots of expensive hardware. This is because computers arenâ€™t good at being random, they require algorithmic instructions. True random number generation often uses physical methods, such as the radioactive decay of atoms, or atmospheric noise.</p>
<p>Throughout this module, we will be using Râ€™s built in random number generation. This is a pseudo random number generator that has excellent random properties, but will eventually repeat. A basic random number generation tool that we will repeatedly use in the module involves sampling from a uniform distribution on the unit interval, which can be done in R using</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="sampling.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.7979742</code></pre>
</div>
<div id="inverse-transform-sampling" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Inverse Transform Sampling<a href="sampling.html#inverse-transform-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we want to sample from a non-uniform one-dimensional distribution. The inverse transform theorem allows us to do this using the distributionâ€™s inverse function.</p>
<div class="definition">
<p><span id="def:unlabeled-div-63" class="definition"><strong>Definition 4.1  </strong></span>Let <span class="math inline">\(X\)</span> be a real-valued random variable with a distribution function <span class="math inline">\(F\)</span>. Then the <strong>inverse function</strong> of a distribution function <span class="math inline">\(F\)</span>, denoted <span class="math inline">\(F^{-1}\)</span>, is defined for all <span class="math inline">\(u \in (0, 1)\)</span> by
<span class="math display">\[
F^{-1}(u) = \inf\{x \in\mathbb{R} : F(x) &gt; u\}.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-64" class="theorem"><strong>Theorem 4.1  </strong></span>Let <span class="math inline">\(F :\mathbb{R} \rightarrow [0, 1]\)</span> be a continuous distribution function, <span class="math inline">\(U \sim U[0, 1]\)</span> and <span class="math inline">\(Y = F^{-1}(U)\)</span>. Then <span class="math inline">\(Y\)</span> has distribution function <span class="math inline">\(F\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-65" class="proof"><em>Proof</em>. </span>We have
<span class="math display">\[
\mathbb{P}(Y \leq a) = \mathbb{P}(F^{-1}(U) \leq a) = \mathbb{P}(\inf\{x \in\mathbb{R} : F(x) &gt; u\} \leq a).
\]</span>
Since <span class="math inline">\(\inf\{x \in\mathbb{R} : F(x) &gt; u\} \leq a\)</span> can only hold if <span class="math inline">\(F(a) \geq U\)</span>, we have
<span class="math display">\[
\mathbb{P}(Y \leq a)  = \mathbb{P}(F(a)\geq U)
\]</span>
As <span class="math inline">\(U \sim U[0, 1]\)</span>, we have <span class="math inline">\(\mathbb{P}(F(a)\geq U) = F(a)\)</span>.</p>
</div>
<p>This theorem says that if we have a random variable <span class="math inline">\(U \sim U[0, 1]\)</span> and we want to get <span class="math inline">\(Y \sim F\)</span>, then we can use <span class="math inline">\(F^{-1}(U)\)</span>. Viewing this theorem graphically can provide a much more intuitive understanding.</p>
<div class="example">
<p><span id="exm:unlabeled-div-66" class="example"><strong>Example 4.1  </strong></span>We would like to sample from an exponential distribution with rate <span class="math inline">\(\lambda\)</span>, i.e.Â <span class="math inline">\(Y ~ \sim \hbox{Exp}(\lambda)\)</span>. The density function is given by</p>
<p><span class="math display">\[
\pi(y \mid \lambda) = \begin{cases}
      \lambdae^{\lambday} &amp; y \geq 0 \\
    0  &amp; \text{otherwise.}
   \end{cases}
\]</span></p>
<p>The distribution function can be derived by
<span class="math display">\[\begin{align*}
F(y \mid \lambda) &amp;= \int_0^y \lambda e^{\lambda t}\,dt \\
&amp; =  1 - e^{\lambda y}.
\end{align*}\]</span>
Finally, the inverse function is given by
<span class="math display">\[
F^{-1}(y \mid \lambda) = -\frac{1}{\lambda}\log(1-y).  
\]</span>
Therefore, if <span class="math inline">\(U \sim U[0, 1]\)</span>, then it follows that <span class="math inline">\(-\frac{1}{\lambda}log(1-U) \sim \hbox{Exp}(\lambda)\)</span>.</p>
<p>The R code below generates a plot to show this (with <span class="math inline">\(\lambda = 0.5\)</span>). We can plot the CDF for most one parameter distributions straightforwardly. We can think of this theorem as allowing us to sample a point on the y-axis and then computing the quantile this corresponds to.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="sampling.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>) <span class="co"># to reproduce</span></span>
<span id="cb45-2"><a href="sampling.html#cb45-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.01</span>) <span class="co">#Show on the interval [0, 5]</span></span>
<span id="cb45-3"><a href="sampling.html#cb45-3" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>y)    <span class="co">#Construct the cumulative density function (CDF)</span></span>
<span id="cb45-4"><a href="sampling.html#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, f, <span class="at">type =</span><span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">ylab=</span> <span class="st">&quot;CDF&quot;</span>)</span>
<span id="cb45-5"><a href="sampling.html#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="sampling.html#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Sample u</span></span>
<span id="cb45-7"><a href="sampling.html#cb45-7" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb45-8"><a href="sampling.html#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="sampling.html#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Get the corresponding y value</span></span>
<span id="cb45-10"><a href="sampling.html#cb45-10" aria-hidden="true" tabindex="-1"></a>f.inv <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>u)</span>
<span id="cb45-11"><a href="sampling.html#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="sampling.html#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plot </span></span>
<span id="cb45-13"><a href="sampling.html#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="dv">0</span>, <span class="at">y0 =</span> u, <span class="at">x1 =</span> f.inv, <span class="at">y1 =</span> u, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb45-14"><a href="sampling.html#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> f.inv, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">x1 =</span> f.inv, <span class="at">y1 =</span> u, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb45-15"><a href="sampling.html#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> f.inv, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.01</span>, <span class="fu">expression</span>(F[<span class="sc">-</span><span class="dv">1</span>](U)), <span class="at">col =</span> <span class="dv">4</span>)</span>
<span id="cb45-16"><a href="sampling.html#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="sc">-</span>.<span class="dv">1</span>, <span class="at">y =</span> u, <span class="st">&quot;U&quot;</span>, <span class="at">col =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
</div>
<div id="rejection-sampling" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Rejection Sampling<a href="sampling.html#rejection-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now have a way of sampling realisations from distributions where we can analytically derive the inverse distribution function. We can use this to sample from more complex densities, or simple densities more efficiently. Rejection sampling works by sampling according to a density we can sample from and then rejecting or accepting that sample based on the density weâ€™re actually interested in. The plot below shows an example from this. We would like to generate a sample from the distribution with the curved density function, which is challenging. Instead, we find a distribution whose density function bounds the one we are interested in a sample from that. In this case we can use the uniform distribution. Once we have generated our sample from he uniform distribution, we choose to accept or reject it based on the distribution we are interested in. In this case we reject it.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Suppose we want to sample from a density <span class="math inline">\(\pi\)</span>, but can only generate samples from a density <span class="math inline">\(q\)</span>. If there exists some constant <span class="math inline">\(c &gt; 0\)</span>, such that <span class="math inline">\(\frac{\pi(y)}{q(y)} \leq c\)</span> for all <span class="math inline">\(y\)</span>, then we can generate samples from <span class="math inline">\(p\)</span> by</p>
<ol style="list-style-type: decimal">
<li><p>Sampling <span class="math inline">\(Y \sim Q\)</span></p></li>
<li><p>Sampling <span class="math inline">\(U \sim U[0, 1]\)</span></p></li>
<li><p>Computing <span class="math inline">\(k = \frac{\pi(u)}{cq(y)}\)</span></p></li>
<li><p>Accepting <span class="math inline">\(y\)</span> if <span class="math inline">\(U &lt; k\)</span> and rejecting otherwise.</p></li>
</ol>
<p>This says draw sample a point <span class="math inline">\(y\)</span> according to the density <span class="math inline">\(q\)</span>. Draw a vertical line at <span class="math inline">\(y\)</span> from the x-axis to <span class="math inline">\(cq(y)\)</span>. Sample uniformly on this line. If the uniformly random sample is below <span class="math inline">\(q\)</span>, then accept it. Otherwise, reject it. The theory behind this is as follows. Suppose we sample some point y according to this algorithm and we want to work out its density <span class="math inline">\(f\)</span>, then
<span class="math display">\[
f(y) \propto q(y)\pi(U &lt; k) = q(y)\frac{\pi(u)}{cq(y)} = \frac{\pi(u)}{c}.
\]</span>
Therefore, <span class="math inline">\(f = p\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-67" class="example"><strong>Example 4.2  </strong></span>Suppose we want to sample from a distribution that has the density
<span class="math display">\[
\pi(y) = \begin{cases}
\frac{3}{4}y(2-y), \qquad y \in [0, 2] \\
0, \qquad \textrm{otherwise}
\end{cases}.
\]</span>
This has a maximum at <span class="math inline">\(\frac{3}{4}\)</span>. We choose <span class="math inline">\(p \sim U[0, 1]\)</span> and <span class="math inline">\(c = \frac{3}{4}\)</span>. The R code below shows a pictorial version of how one sample is generated.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="sampling.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)   <span class="co">#to reproduce</span></span>
<span id="cb46-2"><a href="sampling.html#cb46-2" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>         <span class="co">#set M</span></span>
<span id="cb46-3"><a href="sampling.html#cb46-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)    <span class="co">#sample Y ~ Q</span></span>
<span id="cb46-4"><a href="sampling.html#cb46-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span><span class="sc">*</span>y<span class="sc">*</span>(<span class="dv">2</span><span class="sc">-</span>y) <span class="co">#compute pi(y)</span></span>
<span id="cb46-5"><a href="sampling.html#cb46-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> p<span class="sc">/</span>(M<span class="sc">*</span><span class="dv">1</span>)     <span class="co">#compute k</span></span>
<span id="cb46-6"><a href="sampling.html#cb46-6" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)    <span class="co">#sample U ~ U[0, 1]</span></span>
<span id="cb46-7"><a href="sampling.html#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ifelse</span>(u <span class="sc">&lt;</span> k, <span class="st">&#39;accept&#39;</span>, <span class="st">&#39;reject&#39;</span>) <span class="co">#Accept if  u &lt; k</span></span></code></pre></div>
<pre><code>## [1] &quot;reject&quot;</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="sampling.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create nice plot</span></span>
<span id="cb48-2"><a href="sampling.html#cb48-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fl">0.01</span>)</span>
<span id="cb48-3"><a href="sampling.html#cb48-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span><span class="sc">*</span>a<span class="sc">*</span>(<span class="dv">2</span><span class="sc">-</span>a)</span>
<span id="cb48-4"><a href="sampling.html#cb48-4" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> M<span class="sc">*</span><span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(a))</span>
<span id="cb48-5"><a href="sampling.html#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(a, b, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, M), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span>
<span id="cb48-6"><a href="sampling.html#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(a, c)</span>
<span id="cb48-7"><a href="sampling.html#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> y, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">x1 =</span> y,  <span class="at">y1 =</span><span class="dv">3</span><span class="sc">/</span><span class="dv">4</span><span class="sc">*</span>y<span class="sc">*</span>(<span class="dv">2</span><span class="sc">-</span>y) , <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb48-8"><a href="sampling.html#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> y,  <span class="at">y0 =</span><span class="dv">3</span><span class="sc">/</span><span class="dv">4</span><span class="sc">*</span>y<span class="sc">*</span>(<span class="dv">2</span><span class="sc">-</span>y), <span class="at">x1 =</span> y, <span class="at">y1 =</span> M, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb48-9"><a href="sampling.html#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> y, <span class="at">y =</span> u, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-21-1.png" width="672" />
The plot also shows how the choices of <span class="math inline">\(M\)</span> and <span class="math inline">\(q\)</span> can make the sampling more or less efficient. In our example, the rejection space is large, meaning many of our proposed samples will be rejected. Here, we could have chosen a better <span class="math inline">\(q\)</span> to minimise this space.</p>
</div>
</div>
<div id="markov-chain-monte-carlo" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Markov Chain Monte Carlo<a href="sampling.html#markov-chain-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Markov Chain Monte Carlo (MCMC) is a class of algorithms that produce samples from a probability distribution. These methods combine the idea of rejection sampling with the theory of Markov chains. Before we set out the theory of Markov chains, weâ€™ll go through an example to show how MCMC works.</p>
<div class="example">
<p><span id="exm:King" class="example"><strong>Example 4.3  </strong></span>(<a href="https://xcelab.net/rm/statistical-rethinking/">Adapted from Statistical Rethinking 9</a>) Consider an eccentric King whose kingdom consists of a ring of 10 islands. Directly north is island one, the smallest island. Going clockwise around the archipelago, next is island two, which is twice the size of island one, then island three, which is three times as large as island one. Finally, island 10 is next to island one and ten times as large.</p>
<p>The King wanted to visit all of his islands, but spending time on each one according to its size. That is he should spend the most time on island ten and the least on island one. Being climate conscious, he also decided that flying from one side of the archipelago to the other was not allowed. Instead, he would only sail from one island to either of its neighbors. So from island one, he could reach islands two and ten.</p>
<p>He decided to travel according to these rules:</p>
<ol style="list-style-type: decimal">
<li><p>At the end of each week, he decides to stay on the same island or move to a neighboring island according to a coin toss. If itâ€™s heads he proposes moving clockwise, and tails anti-clockwise. The island he is considering moving to is called the proposal island.</p></li>
<li><p>To decided if he is going to move to the proposal island, the King counts out a number of shells equal to the number of size of the island. So if island five is the proposal island, he counts out five shells. He then counts out a number of stones equal to the size of the current island.</p></li>
<li><p>If the number of seashells is greater than the number of stones, he moves to the proposed island. If the number of seashells is less than the number of stones, he takes a different strategy. He discards the number of stones equal to the number of seashells. So if there are six stone and five seashells, he ends up with 6-5=1 stones. He then places the stones and seashells into a bag a chooses one at random. If he picks a seashell, he moves to the proposed island, otherwise if he picks a shell, he stays put.</p></li>
</ol>
<p>This is a complex way of moving around, but it produces the required result; the time he spends on each island is proportionate to the size of the island. The code below shows an example of this over 10,000 weeks.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="sampling.html#cb49-1" aria-hidden="true" tabindex="-1"></a>weeks <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb49-2"><a href="sampling.html#cb49-2" aria-hidden="true" tabindex="-1"></a>island <span class="ot">&lt;-</span> <span class="fu">numeric</span>(weeks)</span>
<span id="cb49-3"><a href="sampling.html#cb49-3" aria-hidden="true" tabindex="-1"></a>current <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb49-4"><a href="sampling.html#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>weeks){</span>
<span id="cb49-5"><a href="sampling.html#cb49-5" aria-hidden="true" tabindex="-1"></a>  <span class="do">## record current position</span></span>
<span id="cb49-6"><a href="sampling.html#cb49-6" aria-hidden="true" tabindex="-1"></a>  island[i] <span class="ot">&lt;-</span> current</span>
<span id="cb49-7"><a href="sampling.html#cb49-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb49-8"><a href="sampling.html#cb49-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Flip a coin to move to a propose a new island</span></span>
<span id="cb49-9"><a href="sampling.html#cb49-9" aria-hidden="true" tabindex="-1"></a>  proposed <span class="ot">&lt;-</span> current <span class="sc">+</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb49-10"><a href="sampling.html#cb49-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb49-11"><a href="sampling.html#cb49-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Ensure he loops round the island</span></span>
<span id="cb49-12"><a href="sampling.html#cb49-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(proposed <span class="sc">&lt;</span> <span class="dv">1</span>) </span>
<span id="cb49-13"><a href="sampling.html#cb49-13" aria-hidden="true" tabindex="-1"></a>    proposed <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb49-14"><a href="sampling.html#cb49-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(proposed <span class="sc">&gt;</span> <span class="dv">10</span>)</span>
<span id="cb49-15"><a href="sampling.html#cb49-15" aria-hidden="true" tabindex="-1"></a>    proposed <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb49-16"><a href="sampling.html#cb49-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb49-17"><a href="sampling.html#cb49-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Decide to move</span></span>
<span id="cb49-18"><a href="sampling.html#cb49-18" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> proposed<span class="sc">/</span>current</span>
<span id="cb49-19"><a href="sampling.html#cb49-19" aria-hidden="true" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb49-20"><a href="sampling.html#cb49-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(u <span class="sc">&lt;</span> p)</span>
<span id="cb49-21"><a href="sampling.html#cb49-21" aria-hidden="true" tabindex="-1"></a>    current <span class="ot">&lt;-</span> proposed</span>
<span id="cb49-22"><a href="sampling.html#cb49-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-23"><a href="sampling.html#cb49-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-24"><a href="sampling.html#cb49-24" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot results</span></span>
<span id="cb49-25"><a href="sampling.html#cb49-25" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb49-26"><a href="sampling.html#cb49-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(island, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;Week&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Island&quot;</span>)</span>
<span id="cb49-27"><a href="sampling.html#cb49-27" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(island)<span class="sc">/</span>weeks, <span class="at">xlab =</span> <span class="st">&quot;Island&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Proportion of time&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<p>We can recognise several different statistical principles in this example. The King decides to move islands dependent on where he is currently, not based on where he has been previously (Markov property). He proposes an island to move to and accepts or rejects this decision based on some distribution (rejection principle). We are now going to describe some of the properties of Markov chains, including the Markov property.</p>
</div>
<div id="properties-of-markov-chains" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Properties of Markov Chains<a href="sampling.html#properties-of-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:Markov" class="definition"><strong>Definition 4.2  </strong></span>A sequence of random variables <span class="math inline">\(\{Y_1, Y_2, \ldots\}\)</span> is a <strong>Markov chain</strong> if <span class="math inline">\(\mathbb{P}(Y_{n+1} \mid Y_{n}, \ldots, Y_1) = \mathbb{P}(Y_{n+1} \mid Y_{n})\)</span>. That is that distribution of the next state <span class="math inline">\(Y_{n+1}\)</span> only depends on the current state <span class="math inline">\(Y_n\)</span> and not any previous states.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-68" class="definition"><strong>Definition 4.3  </strong></span>The probability of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in a Markov chain is given by <span class="math inline">\(p_{ij}\)</span>. The <strong>transition matrix</strong> for a Markov chain with <span class="math inline">\(N\)</span> states is the <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(P\)</span>, where the <span class="math inline">\(\{i, j\}^{th}\)</span> entry denoted by <span class="math inline">\(p_{ij}\)</span> is probability is moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>.</p>
</div>
<p>These two properties make Markov chains nice to work with, especially the Markov property (Definition <a href="sampling.html#def:Markov">4.2</a>). Two other important definitions are</p>
<div class="definition">
<p><span id="def:unlabeled-div-69" class="definition"><strong>Definition 4.4  </strong></span>The <strong>period </strong>of a state <span class="math inline">\(i\)</span> is given by <span class="math inline">\(d_i = \textrm{gcd}\{n &gt; 0; p_{ii} &gt; 0 \}\)</span>. A state is <strong>aperiodic</strong> if <span class="math inline">\(d_i = 1\)</span>. An <strong>aperiodic chain</strong> is a chain where all states are a periodic.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-70" class="definition"><strong>Definition 4.5  </strong></span>A Markov chain is <strong>irreducible</strong> if there exists an <span class="math inline">\(n \in \mathbb{N}\)</span> such that <span class="math inline">\(\mathbb{P} (Y_n = i \mid Y_0 = j)\)</span> for all pairs <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. In other words, it is possible to move from any state to any other state in a finite number of steps.</p>
</div>
<p>We can use these definitions to start working with distributions. Suppose, the state we start at is drawn from some distribution <span class="math inline">\(Y_1 \sim \boldsymbol{q}\)</span>. Then the distributions of the second state <span class="math inline">\(Y_2\)</span> depends on the distribution of <span class="math inline">\(Y_1\)</span> and the transition probabilities
<span class="math display">\[
\mathbb{P}(Y_2 = j) = \sum_i q_ip_{ij}.
\]</span>
If we denote the distribution of <span class="math inline">\(Y_2 \sim \boldsymbol{q}^{(2)}\)</span>, then we can write it in terms of the transition matrix <span class="math inline">\(\boldsymbol{q}^\prime = \boldsymbol{q}P\)</span>. Now suppose we would like the distribution of <span class="math inline">\(Y_3 \sim \boldsymbol{q}^{(3)}\)</span>, thanks to the Markov property, this is the distribution for <span class="math inline">\(Y_2\)</span> multiplied by the transition matrix, so <span class="math inline">\(Y_3 \sim qP^2\)</span>. Inductively, <span class="math inline">\(P_k \sim qP^{k-1}\)</span>. To use Markov chains to sample from distributions, we need to identify the Eigenvalues of the transition matrix.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-71" class="proposition"><strong>Proposition 4.1  </strong></span>A transition matrix <span class="math inline">\(P\)</span> always has at least one eigenvalue equal to one.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-72" class="proof"><em>Proof</em>. </span>The columns of <span class="math inline">\(P\)</span> sum to 1 as they are probability distributions. Therefore, <span class="math inline">\(1\)</span> is an eigenvalue.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-73" class="definition"><strong>Definition 4.6  </strong></span>If a transition matrix <span class="math inline">\(P\)</span> has a unique Eigenvalue that takes the value 1, there is a unique distribution <span class="math inline">\(\pi\)</span> such that
<span class="math display">\[
\pi P = \pi.
\]</span>
This distribution <span class="math inline">\(\pi\)</span>, is known as the <strong>stationary distribution</strong>.</p>
</div>
<p>This important concept underpins MCMC methods. It says that no matter where we start our chain, weâ€™ll eventually end up sampling states according to the distribution <span class="math inline">\(\pi\)</span>. It make take a long time to reach the stationary distribution, but it will eventually get there.</p>
<p>In order to check whether our Markov chain will converge to a stationary distribution, we need to check:</p>
<ol style="list-style-type: decimal">
<li><p>the Markov chain is aperiodic,</p></li>
<li><p>the Markov chain is irreducible, and</p></li>
<li><p>that there exists a unique distribution <span class="math inline">\(\pi\)</span> such that <span class="math inline">\(\pi P = \pi\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-74" class="example"><strong>Example 4.4  </strong></span>In Example <a href="sampling.html#exm:King">4.3</a>, the King wanted to visit the islands according to how large they are. We can think of the islands as the states and the stationary distribution as <span class="math inline">\(p(Y = i) \propto i\)</span>. The eccentric method the King used allowed him to construct a transition matrix for an aperiodic Markov chain. He also never visited islands regularly using this method.</p>
</div>
<p>When designing a Markov chain, it is usually straightforward to design one that meets conditions one and two. Condition three is more difficult to prove, but for some chains it is possible to show they satisfy detailed balance.</p>
<div class="definition">
<p><span id="def:unlabeled-div-75" class="definition"><strong>Definition 4.7  </strong></span>The Markov chain with transition matrix <span class="math inline">\(P\)</span> satisfies the <strong>detailed balance</strong> equation with respect to the distribution <span class="math inline">\(\pi\)</span> if
<span class="math display">\[
\pi_i p_{ij} = \pi_j p_{ji}.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-76" class="theorem"><strong>Theorem 4.2  (Detailed Balance) </strong></span>Let <span class="math inline">\(P\)</span> be a transition matrix that satisfies detailed balance with respect to the distribution <span class="math inline">\(\pi\)</span>. Then <span class="math inline">\(\pi P = \pi\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-77" class="proof"><em>Proof</em>. </span>The <span class="math inline">\(j^{th}\)</span> row of <span class="math inline">\(\pi P\)</span> is
<span class="math display">\[\begin{align*}
\sum_{i} \pi_i p_{ij} &amp; = \sum_{i} \pi_j p_{ji} \quad \textrm{(detailed balance)} \\
&amp; = \pi_j \sum_{i} p_{ji} \\
&amp; = \pi_j.\qquad \textrm{(probaility sums to 1)}
\end{align*}\]</span>
Hence <span class="math inline">\(\pi P = \pi\)</span>.</p>
</div>
<p>The section has shown us that we can use a Markov chain theory to simulate from a probability distribution <span class="math inline">\(\pi\)</span>. All we need is for the Markov chain to be irreducible, aperiodic, and for the transition matrix to satisfy <span class="math inline">\(\pi P = \pi\)</span>. This provides the foundation theory for MCMC and allows us to sample from a posterior distribution <span class="math inline">\(\pi\)</span>. What it doesnâ€™t tell us is how to design the Markov chain, and that is what the next sections deal with.</p>
</div>
<div id="metropolis-hastings" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Metropolis-Hastings<a href="sampling.html#metropolis-hastings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Weâ€™re now going to look at MCMC algorithms. The first algorithm we are going to look at is the Metropolis-Hasting algorithm. This is a useful algorithm if we cannot sample directly from the posterior distribution and if the conditional distributions do not have a closed form. The Metropolis-Hastings algorithm is like the island example we saw earlier. At each iteration, we propose a new sample and then accept or reject it based on the likelihood function, the prior and how likely we are to propose this new sample given the current one.</p>
<p>Suppose we want to sample from the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. The Metropolis-Hastings works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set the initial value <span class="math inline">\(\theta^{(0)}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span>.</p></li>
<li><p>Propose a new value of <span class="math inline">\(\theta&#39;\)</span> from some distribution <span class="math inline">\(q\)</span></p></li>
<li><p>Accept <span class="math inline">\(\theta&#39;\)</span> with probability
<span class="math display">\[
p_{\textrm{acc}} = \min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})}\frac{q(\theta \mid \theta&#39;)}{q(\theta&#39; \mid \theta)}, 1\right\}.
\]</span></p></li>
<li><p>Repeat steps 3 to 4 for <span class="math inline">\(i = 2, \ldots, M\)</span>.</p></li>
</ol>
<p>There are two parts to the acceptance probability in step 4. The first is the posterior ratio, similar to saying the likelihood of <span class="math inline">\(\theta&#39;\)</span> given the observed data over the likelihood of <span class="math inline">\(\theta\)</span> given the data. The second is the proposal ratio. It is similar to saying the likelihood of proposing <span class="math inline">\(\theta\)</span> given the current value <span class="math inline">\(\theta&#39;\)</span>, over the likelihood of proposing <span class="math inline">\(\theta&#39;\)</span> given the current value <span class="math inline">\(\theta\)</span>.</p>
<p>In practice, we donâ€™t need to evaluate the full posterior distribution. Recall
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) = \frac{\pi(\boldsymbol{y} \mid \theta) \pi(\theta)}{\pi(y)}
\]</span>
As the the denominator doesnâ€™t depend on <span class="math inline">\(\theta\)</span>, it cancels in the ration. The ratio becomes
<span class="math display">\[
\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})} = \frac{\pi(\boldsymbol{y} \mid \theta&#39;) \pi(\theta&#39;)}{\pi(\boldsymbol{y} \mid \theta) \pi(\theta)}.
\]</span>
This is the likelihood ratio multiplied by the prior ratio.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-78" class="proposition"><strong>Proposition 4.2  </strong></span>The Markov chain generated by the Metropolis-Hastings algorithm satisfies detailed balance with respect to the posterior distribution.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-79" class="proof"><em>Proof</em>. </span>Denote the current state <span class="math inline">\(\theta\)</span> and the proposed state <span class="math inline">\(\theta&#39;\)</span>. We would like to show
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) \pi(\theta&#39;\mid\theta) = \pi(\theta&#39; \mid \boldsymbol{y}) \pi(\theta\mid\theta&#39;).
\]</span>
The density of <span class="math inline">\(\theta&#39;\)</span> given the proposed state <span class="math inline">\(\theta\)</span> is the proposal density multiplied by the acceptance probability. It is given by
<span class="math display">\[\begin{align*}
\pi(\theta&#39; \mid \theta) &amp;= q(\theta&#39; \mid \theta)p_{acc}\\
&amp;=  q(\theta&#39; \mid \theta)\min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta&#39; \mid \boldsymbol{y})}\frac{q(\theta \mid \theta&#39;)}{q(\theta&#39; \mid \theta)}, \, 1\right\} \\
&amp; = \min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta&#39; \mid \boldsymbol{y})}q(\theta \mid \theta&#39;),\, q(\theta&#39; \mid \theta)\right\}.
\end{align*}\]</span></p>
<p>The left hand side of the detailed balance equation becomes
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y})\pi(\theta&#39; \mid \theta) = \min\{\pi(\theta&#39; \mid \boldsymbol{y})q(\theta \mid \theta&#39;),\, \pi(\theta \mid \boldsymbol{y})q(\theta&#39; \mid \theta)\}.
\]</span>
Analogously, we can show the right hand side is</p>
<p><span class="math display">\[
\pi(\theta&#39; \mid \boldsymbol{y})\pi(\theta \mid \theta&#39;) = \min\{\pi(\theta&#39; \mid \boldsymbol{y})q(\theta \mid \theta&#39;),\, \pi(\theta \mid \boldsymbol{y})q(\theta&#39; \mid \theta)\}.
\]</span>
Hence, <span class="math inline">\(\pi(\theta \mid \boldsymbol{y}) \pi(\theta&#39;\mid\theta) = \pi(\theta&#39; \mid \boldsymbol{y}) \pi(\theta\mid\theta&#39;)\)</span> and the Markov chain satisfies detailed balance with respect to the posterior disquisition.</p>
</div>
<div class="example">
<p><span id="exm:norm" class="example"><strong>Example 4.5  </strong></span>Lets think again about the reaction time example in the previous chapter.The time until each lorry driver reacts (in milliseconds) is</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="sampling.html#cb50-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.34</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.27</span>, <span class="fl">0.74</span>, <span class="fl">0.44</span>, <span class="fl">0.46</span>, <span class="fl">0.65</span>, <span class="fl">0.36</span>, <span class="fl">0.55</span>, <span class="fl">0.58</span>, <span class="fl">0.55</span>, <span class="fl">0.53</span>, <span class="fl">0.56</span>, <span class="fl">0.54</span>, <span class="fl">0.61</span>, <span class="fl">0.43</span>, <span class="fl">0.52</span>, <span class="fl">0.45</span>, <span class="fl">0.49</span>, <span class="fl">0.32</span>, <span class="fl">0.33</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.34</span>, <span class="fl">0.60</span>, <span class="fl">0.59</span>, <span class="fl">0.43</span>, <span class="fl">0.57</span>, <span class="fl">0.34</span>)</span>
<span id="cb50-2"><a href="sampling.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Reaction time (ms)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Assuming <span class="math inline">\(Y_i \sim N(\mu, \sigma^2)\)</span> are independent and identically distributed for <span class="math inline">\(i=1,...,n\)</span>, by Bayesâ€™ theorem, the posterior distribution is</p>
<p><span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \pi(\boldsymbol{y} \mid \mu, \sigma^2) \pi(\mu).
\]</span>
One of the issues here is that we have assigned a normal prior distribution to the population mean parameter <span class="math inline">\(\mu\)</span>. The advantage previously was that we could derive a posterior distribution with closed form. The disadvantage however is that the choice of prior distribution assigns some positive probability to impossible values of <span class="math inline">\(\mu\)</span>, i.e.Â reaction times less than zero.</p>
<p>Now we have a tool to sample from posterior distributions that donâ€™t have a closed form. We can instead assign an exponential prior distribution, a distribution which only has non-negative support. Letting <span class="math inline">\(\mu \sim \textrm{Exp}(0.01)\)</span> sets a vague prior distribution on <span class="math inline">\(\mu\)</span>. It can be shown that the posterior distribution (exercise) is therefore
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-0.01\mu -\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\}
\]</span></p>
<p>We can use the Metropolis-Hasting algorithm to sample from this posterior distribution. But how should we propose new value of <span class="math inline">\(\mu\)</span>? A common method is a Metropolis-Hastings Random Walk proposal distribution. The proposal distribution is symmetric and centered on <span class="math inline">\(\mu\)</span>. The two most common methods are <span class="math inline">\(\mu&#39; \mid \mu \sim U[\mu - \varepsilon, \mu + \varepsilon]\)</span> and <span class="math inline">\(\mu&#39; \mid \mu \sim N(\mu, \tau^2)\)</span>. We choose the uniform proposal distribution, with
<span class="math display">\[
q(\mu&#39; \mid \mu) = \frac{1}{2\varepsilon}.
\]</span></p>
<p>The acceptance probability is therefore
<span class="math display">\[
p_\textrm{acc} = \min\left\{\frac{\exp\left\{-0.01\mu&#39; -\sum_{i=1}^{30}\frac{(y_i - \mu&#39;)^2}{\sigma^2}\right\} }{\exp\left\{-0.01\mu -\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\} }, 1\right\}
\]</span></p>
<p>We can implement a sampler in R as follows:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="sampling.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Set up elements for MCMC</span></span>
<span id="cb51-2"><a href="sampling.html#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb51-3"><a href="sampling.html#cb51-3" aria-hidden="true" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb51-4"><a href="sampling.html#cb51-4" aria-hidden="true" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb51-5"><a href="sampling.html#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="sampling.html#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb51-7"><a href="sampling.html#cb51-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb51-8"><a href="sampling.html#cb51-8" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb51-9"><a href="sampling.html#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="sampling.html#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb51-11"><a href="sampling.html#cb51-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb51-12"><a href="sampling.html#cb51-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb51-13"><a href="sampling.html#cb51-13" aria-hidden="true" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.01</span>, mu <span class="sc">+</span> <span class="fl">0.01</span>)</span>
<span id="cb51-14"><a href="sampling.html#cb51-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb51-15"><a href="sampling.html#cb51-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb51-16"><a href="sampling.html#cb51-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-17"><a href="sampling.html#cb51-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb51-18"><a href="sampling.html#cb51-18" aria-hidden="true" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu.proposed <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb51-19"><a href="sampling.html#cb51-19" aria-hidden="true" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb51-20"><a href="sampling.html#cb51-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-21"><a href="sampling.html#cb51-21" aria-hidden="true" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb51-22"><a href="sampling.html#cb51-22" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb51-23"><a href="sampling.html#cb51-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-24"><a href="sampling.html#cb51-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb51-25"><a href="sampling.html#cb51-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb51-26"><a href="sampling.html#cb51-26" aria-hidden="true" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb51-27"><a href="sampling.html#cb51-27" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb51-28"><a href="sampling.html#cb51-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb51-29"><a href="sampling.html#cb51-29" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb51-30"><a href="sampling.html#cb51-30" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb51-31"><a href="sampling.html#cb51-31" aria-hidden="true" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb51-32"><a href="sampling.html#cb51-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-33"><a href="sampling.html#cb51-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>We can see that after about 300 iterations, the Markov chain has converged to its stationary distribution, the posterior distribution. We can see this more clearly by removing the first 300 iterations.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="sampling.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="sampling.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="at">xlab =</span> <span class="fu">expression</span>(mu), <span class="at">main =</span> <span class="st">&quot;Posterior distribution&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<p>The 95% credible interval for <span class="math inline">\(\mu\)</span> using this prior distribution is</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="sampling.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.4832146 0.4961484</code></pre>
<p>Recall that using the normal prior distribution, it was</p>
<pre><code>0.486 0.493</code></pre>
<p>It seems that the posterior distribution is very similar when using these two prior distributions. This is because the data are very informative.</p>
</div>
</div>
<div id="gibbs-sampler" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Gibbs Sampler<a href="sampling.html#gibbs-sampler" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we can sample directly from conditional posterior distributions, we can use a Gibbs sampler. Suppose we have a distribution with parameters <span class="math inline">\(\{\theta_1, \ldots, \theta_N\}\)</span>, a Gibbs sampler works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set initial values <span class="math inline">\(\{\theta_1^{(0)}, \ldots, \theta_N^{(0)}\}\)</span></p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span>.</p></li>
<li><p>Draw a value for <span class="math inline">\(\theta_1^{(i)}\)</span> from <span class="math inline">\(\pi(\theta_1 \mid \theta_2^{(i-1)}, \ldots, \theta_N^{(i-1)}))\)</span>.</p></li>
<li><p>Draw a value for <span class="math inline">\(\theta_2^{(i)}\)</span> from <span class="math inline">\(\pi(\theta_2 \mid \theta_1^{(i-1)}, \theta_3^{(i-1)}, \ldots, \theta_N^{(i-1)}))\)</span>.</p></li>
<li><p>Repeat steps 3 and 4 for parameters <span class="math inline">\(\{\theta_3^{(i)}, \ldots, \theta_N^{(i)}\}\)</span>.</p></li>
<li><p>Repeat steps 3, 4, and 5, for <span class="math inline">\(i = 2, \ldots M\)</span>.</p></li>
</ol>
<p>In code, this might look like</p>
<pre><code>M #number of iterations
N #number of parameters
theta.store   &lt;- matrix(NA, N, M)
theta         &lt;- numeric(N)

for(j in 1:M){
  for(j in 1:N){
    theta[i] &lt;- #sample from conditional with theta[-i]
  }
  theta.store[, j] &lt;- theta.current #store current values
}</code></pre>
<p>The sequence <span class="math inline">\(\left\{\theta_0^{(0)},\ldots, \theta_N^{(0)}\right\}, \left\{\theta_0^{(1)},\ldots, \theta_N^{(1)}\right\}, \ldots, \left\{\theta_0^{(M)},\ldots, \theta_N^{(M)}\right\}\)</span> form a Markov chain. They also form a series of samples from the posterior distribution. However it is important to note that by construction, these samples are not independent since each realisation depends on the previous sample and we must take this into account when doing estimation.</p>
<div class="example">
<p><span id="exm:unlabeled-div-80" class="example"><strong>Example 4.6  </strong></span>Recall the lab in the previous chapter where we derived the posterior distribution for the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> for normally distributed data. The marginal posterior distributions are given by
<span class="math display">\[
\mu \mid \boldsymbol{y}, \sigma^2 \sim N(\mu_1, \sigma^2_1),
\]</span></p>
<p><span class="math display">\[
\sigma^2 \mid \boldsymbol{y}, \mu \sim \textrm{inv-Gamma}\left(\alpha + \frac{N}{2}, \,\beta + \frac{\sum_{i=1}^N (y_i - \mu)^2}{2}\right),
\]</span></p>
<p>where <span class="math inline">\(\mu_1 =\left(\frac{\sum_{i=1}^{30}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)\)</span> and <span class="math inline">\(\sigma^2_1 = \left(\frac{30}{\sigma^2} + \frac{1}{\sigma_0^2}\right)^{-1}\)</span>.</p>
<p>We can set up a Metropolis-Hastings algorithm using Gibbs samplers to generate samples for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Set initial values <span class="math inline">\(\{\lambda^{(0)}, \gamma^{(0)}\}\)</span></p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span>.</p></li>
<li><p>Draw a value for <span class="math inline">\(\lambda^{(i)} \mid \boldsymbol{y}, \gamma^{(i-1)} &amp;\sim \textrm{Gamma}(10, 95 + \gamma^{(i-1)})\)</span></p></li>
<li><p>Draw a value for <span class="math inline">\(\gamma^{(i)} \mid \boldsymbol{y}, \,\lambda^{(i)} &amp;\sim \hbox{Exp}(\lambda^{(i)} + \nu)\)</span>.</p></li>
<li><p>Repeat steps 3 and 4 for <span class="math inline">\(i = 2, \ldots M\)</span>.</p></li>
</ol>
</div>
</div>
<div id="mcmc-diagnostics" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> MCMC Diagnostics<a href="sampling.html#mcmc-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When running an MCMC algorithm, it is always important to check that the Markov chain has converged and is mixing well. For our purposes, mixing well means the chain is exploring the space of possible values of <span class="math inline">\(\theta\)</span> effectively and effectively and not getting stuck on the same value for a long time.</p>
<p>A key way of doing this is by looking at the trace plot, which is a time series of the posterior samples simulated by the algorithm. The trace plot should look like it has converged to the stationary distribution and exploring the stationary distribution efficiently. What it shouldnâ€™t look like is a long series of small steps, or being stuck in one spot for a long time. There are two definitions that help us isolate an efficient Markov chain.</p>
<div class="definition">
<p><span id="def:unlabeled-div-81" class="definition"><strong>Definition 4.8  </strong></span>The <strong>burn-in period</strong> is the number of iterations the Markov chain takes to reach the stationary distribution.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-82" class="definition"><strong>Definition 4.9  </strong></span>The <strong>thinning parameter</strong> is the period of iterations of the Markov chain that are stored.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-83" class="example"><strong>Example 4.7  </strong></span>In Example @{exm:norm}, we saw a Markov chain that mixes well. We took the burn-in period to be 3,000 iterations, which was how long it took to for the chain to converge.</p>
<p>In a Metropolis-Hasting random walk algorithm, the proposal distribution often has a large impact on how well the Markov chain mixes. The variance, or step size, of the proposal distribution can be tuned to ensure the chain mixes well.</p>
<p>The following two examples show poorly mixing Markov chains. The first is where the step size is too big and the chain frequently gets stuck for several hundred iterations.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="sampling.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb58-2"><a href="sampling.html#cb58-2" aria-hidden="true" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb58-3"><a href="sampling.html#cb58-3" aria-hidden="true" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb58-4"><a href="sampling.html#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="sampling.html#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb58-6"><a href="sampling.html#cb58-6" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb58-7"><a href="sampling.html#cb58-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb58-8"><a href="sampling.html#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="sampling.html#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb58-10"><a href="sampling.html#cb58-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb58-11"><a href="sampling.html#cb58-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb58-12"><a href="sampling.html#cb58-12" aria-hidden="true" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.1</span>, mu <span class="sc">+</span> <span class="fl">0.1</span>) <span class="co">#Step size too big</span></span>
<span id="cb58-13"><a href="sampling.html#cb58-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb58-14"><a href="sampling.html#cb58-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb58-15"><a href="sampling.html#cb58-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-16"><a href="sampling.html#cb58-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb58-17"><a href="sampling.html#cb58-17" aria-hidden="true" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu.proposed <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb58-18"><a href="sampling.html#cb58-18" aria-hidden="true" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb58-19"><a href="sampling.html#cb58-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-20"><a href="sampling.html#cb58-20" aria-hidden="true" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb58-21"><a href="sampling.html#cb58-21" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb58-22"><a href="sampling.html#cb58-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-23"><a href="sampling.html#cb58-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb58-24"><a href="sampling.html#cb58-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb58-25"><a href="sampling.html#cb58-25" aria-hidden="true" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb58-26"><a href="sampling.html#cb58-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb58-27"><a href="sampling.html#cb58-27" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb58-28"><a href="sampling.html#cb58-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb58-29"><a href="sampling.html#cb58-29" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb58-30"><a href="sampling.html#cb58-30" aria-hidden="true" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb58-31"><a href="sampling.html#cb58-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb58-32"><a href="sampling.html#cb58-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3000</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>The next is where the step size is too small. It takes a long time for the chain to converge (~50% of the run time). When the chain does converge, it is inefficient at exploring the space.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="sampling.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb59-2"><a href="sampling.html#cb59-2" aria-hidden="true" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb59-3"><a href="sampling.html#cb59-3" aria-hidden="true" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb59-4"><a href="sampling.html#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="sampling.html#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb59-6"><a href="sampling.html#cb59-6" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb59-7"><a href="sampling.html#cb59-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb59-8"><a href="sampling.html#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="sampling.html#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb59-10"><a href="sampling.html#cb59-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb59-11"><a href="sampling.html#cb59-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb59-12"><a href="sampling.html#cb59-12" aria-hidden="true" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.0005</span>, mu <span class="sc">+</span> <span class="fl">0.0005</span>) <span class="co">#Step size too small</span></span>
<span id="cb59-13"><a href="sampling.html#cb59-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb59-14"><a href="sampling.html#cb59-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb59-15"><a href="sampling.html#cb59-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-16"><a href="sampling.html#cb59-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb59-17"><a href="sampling.html#cb59-17" aria-hidden="true" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu.proposed <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb59-18"><a href="sampling.html#cb59-18" aria-hidden="true" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb59-19"><a href="sampling.html#cb59-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-20"><a href="sampling.html#cb59-20" aria-hidden="true" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb59-21"><a href="sampling.html#cb59-21" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb59-22"><a href="sampling.html#cb59-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-23"><a href="sampling.html#cb59-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb59-24"><a href="sampling.html#cb59-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb59-25"><a href="sampling.html#cb59-25" aria-hidden="true" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb59-26"><a href="sampling.html#cb59-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb59-27"><a href="sampling.html#cb59-27" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb59-28"><a href="sampling.html#cb59-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb59-29"><a href="sampling.html#cb59-29" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb59-30"><a href="sampling.html#cb59-30" aria-hidden="true" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb59-31"><a href="sampling.html#cb59-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb59-32"><a href="sampling.html#cb59-32" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb59-33"><a href="sampling.html#cb59-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span>
<span id="cb59-34"><a href="sampling.html#cb59-34" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Exercises<a href="sampling.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-84" class="exercise"><strong>Exercise 4.1  </strong></span>Let <span class="math inline">\(Y\)</span> be a random variable with a probability density function defined by
<span class="math display">\[
\pi(y) = \alpha y^3 \quad \hbox{for } 0 \leq y \leq 4,
\]</span>
where <span class="math inline">\(\alpha \in \mathbb{R}\)</span> is a constant.</p>
<ol style="list-style-type: decimal">
<li>Compute the value of <span class="math inline">\(\alpha\)</span>.</li>
<li>Derive the distribution function <span class="math inline">\(F(y)\)</span>.</li>
<li>Using the inverse transform theorem, derive a function <span class="math inline">\(g\)</span> such that if <span class="math inline">\(U \sim U[0, 1]\)</span>, then <span class="math inline">\(g(U)\)</span> has the same distribution as <span class="math inline">\(Y\)</span>.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-85" class="solution"><em>Solution</em>. </span>This exercise is about the inverse transform sampling method.</p>
<ol style="list-style-type: decimal">
<li>Using the fact that <span class="math inline">\(\int_0^4 \pi(y) dy = 1\)</span>, we have <span class="math inline">\(\alpha = \frac{1}{64}\)</span>.</li>
<li>Introducing a dummy variable <span class="math inline">\(x\)</span>, we have
<span class="math display">\[
\int_0^y \frac{1}{64}x^3dx = \frac{1}{256}y^4.
\]</span>
Hence the distribution function for <span class="math inline">\(Y\)</span> is given by
<span class="math display">\[
F(y) = \begin{cases}
0 &amp; \hbox{for } y \leq 0\\
\frac{1}{256}y^4 &amp; \hbox{for } 0  &lt; y \leq 4 \\
1 &amp; y &gt; 4
\end{cases}.
\]</span></li>
<li>We have
<span class="math display">\[
F(y) = \frac{1}{256}y^4 \iff F^{-1}(y) = 4y^{\frac{1}{4}}.
\]</span>
By the inverse transform theorem, if <span class="math inline">\(U \sim U[0, 1]\)</span> then <span class="math inline">\(4U^{\frac{1}{4}}\)</span> has the same distribution as <span class="math inline">\(Y\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-86" class="exercise"><strong>Exercise 4.2  </strong></span>Suppose <span class="math inline">\(Y\)</span> has the density function
<span class="math display">\[
f(y) = \frac{1}{2\sqrt{y}}e^{-\sqrt{y}} \quad \hbox{for } y \geq 0.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Using integration by substitution, derive the distribution function <span class="math inline">\(F(y)\)</span>.</li>
<li>Using the inverse transform method, construct a method for sampling from this distribution.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-87" class="solution"><em>Solution</em>. </span>Another exercise about the inverse transform method, but this time with a slightly more complex density function.</p>
<ol style="list-style-type: decimal">
<li>Introducing the dummy variable <span class="math inline">\(x\)</span>, we need to evaluate
<span class="math display">\[
\int_0^y \frac{1}{2\sqrt{x}}e^{-\sqrt{x}}\,dx.
\]</span></li>
</ol>
<p>The easiest substitution to use is <span class="math inline">\(u = \sqrt{x}\)</span>. This gives us <span class="math inline">\(dx = 2du\sqrt{x}\)</span>. Substituting these into the integral gives
<span class="math display">\[\begin{align*}
\int_0^y \frac{1}{2u}e^{-u}\,dx &amp; = \int_0^\sqrt{y} e^{-u}\,dx \\
&amp;= [-e^{-u}]_0^\sqrt{y} \\
&amp;= 1 - e^{-\sqrt{y}}.
\end{align*}\]</span>
A quicker, but less obvious way to compute the integral is to note that <span class="math inline">\(\frac{d}{dx}e^{f(x)} = f&#39;(x)e^{f(x)}\)</span>.
The distribution function is therefore
<span class="math display">\[
F(y) = \begin{cases}
1 - e^{-\sqrt{y}} &amp; \hbox{for } y \geq 0 \\
0  &amp; \hbox{otherwise}
\end{cases}.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Using the inverse transform method, if <span class="math inline">\(U\sim U[0, 1]\)</span> then <span class="math inline">\(F^{-1}(U)\)</span> will have the same distribution as <span class="math inline">\(Y\)</span>. The inverse of the distribution function is
<span class="math display">\[
F^{-1}(y) = (\log(1-y))^2.
\]</span>
The method would be</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Sample <span class="math inline">\(U\sim U[0, 1]\)</span></li>
<li>Compute <span class="math inline">\(Y=(\log(1-U))^2\)</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-88" class="exercise"><strong>Exercise 4.3  </strong></span>The density function for the half-normal distribution with variance 1 is
<span class="math display">\[
\pi(x) = \frac{2}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}x^2\right\} \qquad \hbox{for } x \geq 0.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Using an exponential distribution with rate <span class="math inline">\(\lambda\)</span> as a proposal distribution, show that
<span class="math display">\[
\frac{\pi(x)}{q(x)} = \frac{2}{\lambda\sqrt{2\pi}}\exp\left\{\lambda x - \frac{1}{2}x^2 \right\}.
\]</span></li>
<li>What is <span class="math inline">\(M\)</span>, the maximum value of this ratio? On the same plot, sketch <span class="math inline">\(\pi(x)\)</span> and <span class="math inline">\(Mq(x)\)</span>.</li>
<li>Construct a rejection sampling algorithm to sample from the half normal distribution.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-89" class="solution"><em>Solution</em>. </span>The normal distribution is difficult to sample from, especially as the inverse of its distribution is difficult to work with. One method to generate samples is using a rejection sampling method.</p>
<ol style="list-style-type: decimal">
<li>The density function of a exponential distribution with rate <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(q(x \mid \lambda) = \lambda e^{-\lambda x}\)</span>. The ratio is given by
<span class="math display">\[\begin{align*}
\frac{\pi(x)}{q(x)} &amp;= \frac{\frac{2}{\sqrt{2\pi}}\exp\{-\frac{1}{2}x^2\}}{\lambda e^{-\lambda x}} \\
&amp; = \frac{2}{\lambda\sqrt{2\pi}}\exp\left\{\lambda x - \frac{1}{2}x^2\right\}.
\end{align*}\]</span></li>
<li>The find the maximum value of this ratio, we first need to find the value of <span class="math inline">\(x\)</span> that maximises the ratio. Differentiating gives
<span class="math display">\[
\frac{d}{dx}\frac{2}{\lambda\sqrt{2\pi}}\exp\left\{\lambda x - \frac{1}{2}x^2 \right\} = \frac{2(\lambda - x)}{\lambda\sqrt{2\pi}}\exp\left\{\lambda x - \frac{1}{2}x^2 \right\}
\]</span>
Setting this equal to 0 shows that <span class="math inline">\(x = \lambda\)</span> maximises the ratio. The maxmimum value of the ratio is thus
<span class="math display">\[
M = \frac{\pi(\lambda)}{q(\lambda)} = \frac{2}{\lambda\sqrt{2\pi}}\exp\left\{-\frac{1}{2}\lambda^2\right\}
\]</span>
The plot below shows that not only is <span class="math inline">\(\pi(x) \leq Mq(x)\)</span>, but is that <span class="math inline">\(M\)</span> is the optimal value for a rejection sampling algorithm.</li>
</ol>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="sampling.html#cb60-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="fl">0.01</span>)</span>
<span id="cb60-2"><a href="sampling.html#cb60-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb60-3"><a href="sampling.html#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="sampling.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Denisties</span></span>
<span id="cb60-5"><a href="sampling.html#cb60-5" aria-hidden="true" tabindex="-1"></a>pi.x <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span>(<span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi))<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb60-6"><a href="sampling.html#cb60-6" aria-hidden="true" tabindex="-1"></a>q.x  <span class="ot">&lt;-</span> <span class="fu">dexp</span>(x, lambda) </span>
<span id="cb60-7"><a href="sampling.html#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="sampling.html#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co">#M</span></span>
<span id="cb60-9"><a href="sampling.html#cb60-9" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span>(lambda<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi))<span class="sc">*</span><span class="fu">exp</span>(<span class="fl">0.5</span><span class="sc">*</span>lambda<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb60-10"><a href="sampling.html#cb60-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-11"><a href="sampling.html#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot</span></span>
<span id="cb60-12"><a href="sampling.html#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, pi.x, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, M), <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;density&quot;</span>)</span>
<span id="cb60-13"><a href="sampling.html#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, M<span class="sc">*</span>q.x, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb60-14"><a href="sampling.html#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, <span class="fu">c</span>(<span class="fu">expression</span>(<span class="fu">pi</span>(x)), <span class="fu">expression</span>(<span class="fu">Mq</span>(x))), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li>The rejection sampling algorithm for the half normal distribution works as follows:</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Sample <span class="math inline">\(x\)</span> from the exponential distribution.</li>
<li>Sample <span class="math inline">\(u \sim U[0, 1]\)</span>.</li>
<li>If <span class="math inline">\(u &lt; \frac{\pi(x)}{Mq(x)}\)</span>, then accept <span class="math inline">\(x\)</span> as a sample from the half normal distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-90" class="exercise"><strong>Exercise 4.4  </strong></span>We are using a rejection sampling algorithm to sample from <span class="math inline">\(f(y)\)</span> using the proposal distribution <span class="math inline">\(g(y)\)</span>. Given a proposed value <span class="math inline">\(y\)</span>, and let <span class="math inline">\(A\)</span> be the event of sample is accepted. The probability a sample is accepted, given it takes the value <span class="math inline">\(y\)</span> is
<span class="math display">\[
A \mid y \sim \hbox{Bernoulli}\left(\frac{f(y)}{Mg(y)}\right).
\]</span></p>
<ol style="list-style-type: decimal">
<li>Using the tower property, show <span class="math inline">\(E(A) = \frac{1}{M}\)</span>.<br />
</li>
<li>Assuming each proposal is independent, what is the distribution of the number of samples generated until one is accepted?</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-91" class="solution"><em>Solution</em>. </span>We saw in the last question that we derived the optimal value of the constant <span class="math inline">\(M\)</span>. This exercise is about computing the efficiency of a rejection sampling algorithm, which depends on <span class="math inline">\(M\)</span>. The efficiency is the expected number of proposals generated until one is accepted.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(A\)</span> be the event a proposal is accepted, we would like We have <span class="math inline">\(A \mid y \sim \hbox{Bernoulli}\left(\frac{f(y)}{Mg(y)}\right)\)</span>. By the tower property
<span class="math display">\[\begin{align*}
E(A) &amp; = E(E(A\mid y)) \\
&amp; = E\left(\frac{f(y)}{Mg(y)}\right) \\
&amp; = \int \frac{f(y)}{Mg(y)} g(y) dy \\
&amp; = \frac{1}{M}\int f(y) dy \\
&amp; = \frac{1}{M}.
\end{align*}\]</span></li>
<li>The number of events until a success is modelled by the geometric distribution. The number of samples proposed before one is accepted is distributed geometrically with mean <span class="math inline">\(\frac{1}{M}\)</span>. The smaller the value of <span class="math inline">\(M\)</span>, the more efficient the sampler.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-92" class="exercise"><strong>Exercise 4.5  </strong></span>Alice wants to construct a Markov Chain to sample numbers from one to six. She chooses an initial number at random. She flips a coin and decides to stay with that number if itâ€™s a head, and move to a different number if itâ€™s a tails. If she chooses to move number she flips the coin again. If itâ€™s heads, she moves to a number one larger than she is now, if itâ€™s tails she moves to a number one smaller than she is now. Once she reaches one or six, she stops.</p>
<ol style="list-style-type: decimal">
<li>Draw a state space diagram for this chain.</li>
<li>Is Aliceâ€™s Markov chain aperiodic and irreducible?</li>
</ol>
<p>Bob comes up with a similar Markov chain. Like Alice, he chooses an initial number at random and flips a coin to decide if he should keep that number of move to a different one. However, his next step differs from Aliceâ€™s. If he chooses to move number he flips the coin again. If itâ€™s heads, he moves to a number two larger than he is now, if itâ€™s tails he moves to a number two smaller than he is now. He also treats the numbers like a circle, where if he is on six, he can move to four or two. Likewise, if he is on one, he can move to five or three.</p>
<ol style="list-style-type: decimal">
<li>Draw a state space diagram for this chain.</li>
<li>Is Bobâ€™s Markov chain aperiodic and irreducible?</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-93" class="solution"><em>Solution</em>. </span>The state space diagram for Alice and Bobâ€™s Markov chains are:
<embed src="State_Space_Diagram.pdf" style="display: block; margin: auto;" type="application/pdf" />
Neither Alice nor Bob have constructed Markov chains that are aperiodic and irreducible. In Aliceâ€™s chain, states one and six are absorbing states â€“ if Alice ends up with one of these numbers, she is never going to choose another number. This means states one and six have period one. As Aliceâ€™s chain has absorbing states, it isnâ€™t irreducible either. For example, it isnâ€™t possible to get from state six to two in a finite (or any) number of steps. As Bob has introduce a mechanism to loop round the numbers, his chain is aperiodic. However, it isnâ€™t irreducible. Moving up and down two numbers means that he is restricted to the even or odd numbers. If he starts on an even number, he is never going to reach an odd number.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-94" class="exercise"><strong>Exercise 4.6  </strong></span>You observe <span class="math inline">\(N\)</span> data points <span class="math inline">\(\{y_1, \ldots, y_N\} \sim N(\mu, \sigma^2)\)</span>. Place a normal prior distribution on the mean parameter <span class="math inline">\(\mu\)</span> and inverse-gamma prior distribution <span class="math inline">\(\sigma^2\)</span> to derive the posterior distributions <span class="math inline">\(\pi(\mu, \sigma^2 \mid \boldsymbol{y})\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-95" class="solution"><em>Solution</em>. </span>Using the prior distributions
<span class="math display">\[
\mu \sim N(\mu_0, \sigma_0^2), \qquad \sigma^2 \sim \hbox{inv}-\Gamma(\alpha, \beta)
\]</span>
gives the posterior distribution â€¦.</p>
<p>The full conditional distributions are
<span class="math display">\[
\mu \mid \sigma^2, \boldsymbol{y} \sim N(\mu_1, \sigma_1^2), \qquad \sigma^2 \mid \mu,\boldsymbol{y} \sim \hbox{inv}-\Gamma(\alpha_1, \beta_1).
\]</span>
A good MCMC algorithm could look like this:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\mu^{(1)} = 0\)</span>, <span class="math inline">\(\sigma^{2(0)} = 1\)</span> and <span class="math inline">\(i = 1\)</span>.</li>
<li>Sample <span class="math inline">\(\mu^{(i)} \mid \sigma^{2(i-1)}, \boldsymbol{y} \sim N(\mu_1, \sigma_1^2)\)</span>.</li>
<li>Sample <span class="math inline">\(\sigma^{2(i)} \mid \mu^{(i)}, \boldsymbol{y} \sim \hbox{inv}-\Gamma(\alpha_1, \beta_1)\)</span>.</li>
<li>Repeat steps 2 and 3 for a given number of iterations.</li>
<li>Check the trace plots to ensure the Markov chain has converged and mixed well.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-96" class="exercise"><strong>Exercise 4.7  </strong></span>In the lectures, we looked at Metropolis-Hastings Random Walk algorithms. Another algorithm is a Metropolis-Hastings Independence Sampler. In an independence sampler, the proposal distribution does not depend on the current values, that is
<span class="math display">\[
q(y&#39;\mid y) = q(y&#39;).
\]</span></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(y_1, \ldots, y_N \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is know. Show the posterior distribution has no closed form when the prior distribution <span class="math inline">\(\mu \sim \Gamma(\alpha, \beta)\)</span> is used.</li>
<li>Suppose you are constructing an MCMC algorithm to sample from this distribution. Denote the current value of the Markov chain <span class="math inline">\(\mu\)</span>. Given a proposal <span class="math inline">\(\mu&#39; \sim N(0, 5^2)\)</span> proposal distribution, derive the acceptance probability.</li>
</ol>
</div>
<div class="solution">
<p><span id="unlabeled-div-97" class="solution"><em>Solution</em>. </span>This question is about independence samplers. They are typically very good or very bad at sampling from the posterior distribution.</p>
<ol style="list-style-type: decimal">
<li><p>The likelihood function is given by
<span class="math display">\[
\pi(\boldsymbol{y} \mid \mu, \sigma^2) \propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \mu)^2 \right\}.
\]</span>
The prior distribution is <span class="math inline">\(\pi(\mu) \propto \mu^{\alpha-1}\exp\{-beta\mu\}\)</span>. The posterior distribution is the product of these and given by
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \mu^{\alpha-1} \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \mu)^2  -
\beta\mu\right\}.
\]</span></p></li>
<li><p>The acceptance probability is
<span class="math display">\[
p_{acc} = \frac{\pi(\mu&#39;\mid \boldsymbol{y}, \sigma^2)}{\pi(\mu&#39;\mid \boldsymbol{y}, \sigma^2)} \frac{\pi(\mu)}{\pi(\mu&#39;)}.
\]</span>
Substituting the proportional densities in yields
<span class="math display">\[
p_{acc} = \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \mu&#39;)^2 + \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \mu)^2 -\frac{1}{50}\mu^2 + \frac{1}{50}{\mu&#39;}^2 \right\}.
\]</span></p></li>
</ol>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-computation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
