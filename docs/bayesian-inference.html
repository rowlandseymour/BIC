<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Bayesian Inference | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.28.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Bayesian Inference | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Bayesian Inference | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Rowland Seymour" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="programming-in-r.html"/>
<link rel="next" href="sampling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#getting-help"><i class="fa fa-check"></i><b>0.4</b> Getting Help</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.5</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.6</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-probability"><i class="fa fa-check"></i><b>1.3</b> Bayesian Probability</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#conditional-probability-and-exchangability"><i class="fa fa-check"></i><b>1.4</b> Conditional Probability and Exchangability</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.5</b> Bayesâ€™ Theorem</a></li>
<li class="chapter" data-level="1.6" data-path="fundamentals.html"><a href="fundamentals.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-binomial-distribution"><i class="fa fa-check"></i><b>3.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting Conclsuions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-exponential-distribution"><i class="fa fa-check"></i><b>3.3</b> The Exponential Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-normal-distribtuion"><i class="fa fa-check"></i><b>3.4</b> The Normal Distribtuion</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.5</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.6</b> Prediction</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.7</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bernstein-von-mises-theorem"><i class="fa fa-check"></i><b>3.8</b> Bernstein-von-Mises Theorem</a></li>
<li class="chapter" data-level="3.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exercises-1"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.4</b> Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>4.5</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#metropolis-hastings"><i class="fa fa-check"></i><b>4.6</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.7" data-path="sampling.html"><a href="sampling.html#gibbs-sampler"><i class="fa fa-check"></i><b>4.7</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="4.8" data-path="sampling.html"><a href="sampling.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>4.8</b> MCMC Diagnostics</a></li>
<li class="chapter" data-level="4.9" data-path="sampling.html"><a href="sampling.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>5</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>5.1</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>5.1.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="5.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>5.1.2</b> Imputing Latent Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>5.2</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>5.2.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="5.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>5.2.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-data-augmenatation"><i class="fa fa-check"></i><b>5.3</b> Lab: Data Augmenatation</a></li>
<li class="chapter" data-level="5.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-gaussian-processes"><i class="fa fa-check"></i><b>5.4</b> Lab: Gaussian Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-inference" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Bayesian Inference<a href="bayesian-inference.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Whereas Chapter 1 dealt with the fundamentals of Bayesian inference and definitions, Chapter 3 is much more practical. We are going to be deriving posterior distributions and proving when it does and doesnâ€™t work.</p>
<div id="the-binomial-distribution" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> The Binomial Distribution<a href="bayesian-inference.html#the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The first example weâ€™re going to go through is with the Binomial distribution.</p>
<div class="example">
<p><span id="exm:binom" class="example"><strong>Example 3.1  </strong></span>A social media company wants to determine how many of its users are bots. A software engineer collects a random sample of 200 accounts and finds that eight are bots. She uses a Bayesian method to estimate the probability of an account being a bot. She labels the accounts with a 1 if they are a bot and 0 if there is are a real person. The set of account labels is given by <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_{200}\}\)</span> and the probability an account is a bot is <span class="math inline">\(\theta\)</span>. By Bayesâ€™ theorem, we obtain the following,
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y}\mid \theta) \pi(\theta).
\]</span></p>
<p><strong>Likelihood function</strong> <span class="math inline">\(\pi(\boldsymbol{y}\mid \theta)\)</span>. We observe 200 trials each with the same probability of success (denoted by <span class="math inline">\(\theta\)</span>) and probability of failure (given by <span class="math inline">\(1-\theta\)</span>). The Binomial distribution seems the most suitable way of modelling this. Therefore, the likelihood function is given by,
<span class="math display">\[
\pi(\boldsymbol{y}\mid \theta) = \begin{pmatrix} 200 \\ 3 \end{pmatrix} \theta^3(1-\theta)^{197},
\]</span>
assuming that any two accounts being a bot are independent of one another.</p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\theta)\)</span>. We now need to describe our prior beliefs about <span class="math inline">\(\theta\)</span>. We have no reason to suggest <span class="math inline">\(\theta\)</span> takes any specific value, so we use a uniform prior distribution <span class="math inline">\(\theta \sim U[0, 1]\)</span>, where <span class="math inline">\(\pi(\theta) = 1\)</span> for <span class="math inline">\(\theta \in [0, 1]\)</span>.</p>
<p><strong>Posterior distribution</strong> <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. We can now derive the posterior distribution up to proportionality
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) \propto \theta^3(1-\theta)^{197}.
\]</span>
This functional dependence on <span class="math inline">\(\theta\)</span> identifies the <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span> is a Beta distribution. The PDF for the beta distribution with shape parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
\pi(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1-x)^{\beta - 1}.
\]</span>
The posterior distribution is therefore <span class="math inline">\(\theta \mid \boldsymbol{y} \sim \textrm{Beta}(4, 198)\)</span>.</p>
</div>
</div>
<div id="reporting-conclsuions-from-bayesian-inference" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Reporting Conclsuions from Bayesian Inference<a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous example, we derived the posterior distribution <span class="math inline">\(\theta \mid \boldsymbol{y} \sim \textrm{Beta}(4, 198)\)</span>. But often, we want to share more descriptive information about our beliefs given the observed data. In this example, the posterior mean given the data is <span class="math inline">\(\frac{4}{198} = \frac{2}{99}\)</span>. That is to say given the data, we expect that for every 99 accounts, two to be bots. The posterior mode for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\frac{3}{200}\)</span> or 1.5%.</p>
<p>It is important to share the uncertainty about out beliefs. In a frequentist framework, this would be via a confidence interval. The Bayesian analogues is a credible interval.</p>
<div class="definition">
<p><span id="def:unlabeled-div-41" class="definition"><strong>Definition 3.1  </strong></span>A <strong>credible interval</strong> is a central interval of posterior probability which corresponds, in the case of a 100<span class="math inline">\((1-\alpha)\)</span>% interval, to the range of values that capture 100<span class="math inline">\((1-\alpha)\)</span>% of the posterior probability.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-42" class="example"><strong>Example 3.2  </strong></span>The 95% credible interval for the Binomial example is given by</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="bayesian-inference.html#cb21-1" aria-hidden="true" tabindex="-1"></a>cred.int<span class="fl">.95</span> <span class="ot">&lt;-</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">4</span>, <span class="dv">198</span>)</span>
<span id="cb21-2"><a href="bayesian-inference.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cred.int<span class="fl">.95</span>, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.005 0.043</code></pre>
<p>This says that we believe there is a 95% chance that the probability of an account being a bot lies between 0.005 and 0.043. This is a much more intuitive definition to the confidence interval, which says if we ran the experiment an infinite number of times and computed an infinite number of confidence intervals, 95% of them would contain the true value of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="the-exponential-distribution" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> The Exponential Distribution<a href="bayesian-inference.html#the-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:exponential" class="example"><strong>Example 3.3  </strong></span>An insurance company want to estimate the time until a claim is made on a specific policy. They describe the rate at which claims come in by <span class="math inline">\(\lambda\)</span>. The company provides a sample of 10 months at which a claim was made <span class="math inline">\(\boldsymbol{y} = \{14, 10, 6, 7, 13, 9, 12, 7, 9, 8\}\)</span>. By Bayesâ€™ theorem, the posterior distribution for <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y} \mid \lambda) \pi(\lambda).
\]</span></p>
<p><strong>Likelihood function</strong> <span class="math inline">\(\pi(\boldsymbol{y} \mid \lambda)\)</span>. The exponential distribution is a good way of modelling lifetimes or the length of time until an event happens. Assuming all the claims are independent of one another, the likelihood function is given by
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \lambda) &amp;= \prod_{i=1}^{10} \lambda e^{-\lambda y_i} \\
&amp; = \lambda^{10}e^{-\lambda \sum_{i=1}^{10} y_i} \\
&amp; = \lambda^{10} e^{-95\lambda}.
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\lambda)\)</span>. As we are modelling a rate parameter, we know it must be positive and continuous. We decide to use an exponential prior distribution for <span class="math inline">\(\lambda\)</span>, but leave the choice of the rate parameter up to the insurance professionals at the insurance company. The prior distribution is given by <span class="math inline">\(\lambda \sim \textrm{Exp}(\gamma).\)</span></p>
<p><strong>Posterior distribution</strong> <span class="math inline">\(\pi(\lambda \mid \boldsymbol{y})\)</span>. We now have all the ingredients to derive the posterior distribution. It is given by
<span class="math display">\[\begin{align*}
\pi(\lambda \mid \boldsymbol{y}) &amp;\propto \lambda^{10} e^{-95\lambda} \times \lambda e^{-\gamma\lambda} \\
&amp; \propto \lambda^{11}e^{-(95 + \gamma)\lambda}
\end{align*}\]</span>
The functional form tells us that the posterior distribution is a Gamma distribution. The PDF of a gamma random variable with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
\pi(x \mid \alpha, \beta) = \frac{\alpha^\beta}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}.
\]</span>
The distribution of the rate of the claims given the observed data is <span class="math inline">\(\lambda \mid \boldsymbol{y} \sim \textrm{Gamma}(10, 95 + \gamma)\)</span>.</p>
<p>The posterior mean months until a claim is <span class="math inline">\(\frac{10}{95 + \gamma}\)</span>. We can see the effect of the choice of rate parameter in this mean. Small values of <span class="math inline">\(\gamma\)</span> yield vague prior distribution, which plays a minimal role in the posterior distribution. Large values of <span class="math inline">\(\gamma\)</span> result in prior distributions that contribute a lot to the posterior distribution. The plots below show the prior and posterior distributions for <span class="math inline">\(\gamma = 0.01\)</span> and <span class="math inline">\(\gamma = 50\)</span>.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="bayesian-inference.html#cb23-1" aria-hidden="true" tabindex="-1"></a>plot.distributions <span class="ot">&lt;-</span> <span class="cf">function</span>(chi){</span>
<span id="cb23-2"><a href="bayesian-inference.html#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate at selected values of theta</span></span>
<span id="cb23-3"><a href="bayesian-inference.html#cb23-3" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.3</span>, <span class="fl">0.001</span>) </span>
<span id="cb23-4"><a href="bayesian-inference.html#cb23-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-5"><a href="bayesian-inference.html#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate prior density</span></span>
<span id="cb23-6"><a href="bayesian-inference.html#cb23-6" aria-hidden="true" tabindex="-1"></a>  prior <span class="ot">&lt;-</span> <span class="fu">dexp</span>(theta, <span class="at">rate =</span> chi)</span>
<span id="cb23-7"><a href="bayesian-inference.html#cb23-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-8"><a href="bayesian-inference.html#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate posterior density</span></span>
<span id="cb23-9"><a href="bayesian-inference.html#cb23-9" aria-hidden="true" tabindex="-1"></a>  posterior <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(theta, <span class="at">shape =</span> <span class="dv">10</span>, <span class="at">rate =</span> <span class="dv">95</span> <span class="sc">+</span> chi)</span>
<span id="cb23-10"><a href="bayesian-inference.html#cb23-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-11"><a href="bayesian-inference.html#cb23-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-12"><a href="bayesian-inference.html#cb23-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#plot</span></span>
<span id="cb23-13"><a href="bayesian-inference.html#cb23-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(theta, posterior, <span class="at">type=</span> <span class="st">&#39;l&#39;</span>, </span>
<span id="cb23-14"><a href="bayesian-inference.html#cb23-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">xlab =</span> <span class="fu">expression</span>(theta), <span class="at">ylab =</span> <span class="st">&quot;density&quot;</span>)</span>
<span id="cb23-15"><a href="bayesian-inference.html#cb23-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(theta, prior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb23-16"><a href="bayesian-inference.html#cb23-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Posterior&quot;</span>, <span class="st">&quot;Prior&quot;</span>),  </span>
<span id="cb23-17"><a href="bayesian-inference.html#cb23-17" aria-hidden="true" tabindex="-1"></a>         <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb23-18"><a href="bayesian-inference.html#cb23-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-19"><a href="bayesian-inference.html#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="bayesian-inference.html#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="fl">0.01</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="bayesian-inference.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="dv">50</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<p>The insurance managers recommend that because this is a new premium, a vague prior distribution be used and <span class="math inline">\(\gamma = 0.01\)</span>. The posterior mean is <span class="math inline">\(\frac{10}{95.01} \approx 0.105\)</span> and the 95% credible interval is</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="bayesian-inference.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">10</span>, <span class="fl">95.01</span>), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.05 0.18</code></pre>
</div>
</div>
<div id="the-normal-distribtuion" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> The Normal Distribtuion<a href="bayesian-inference.html#the-normal-distribtuion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Normal distribution is incredibly useful for modelling a wide range of natural phenomena and in its own right. Weâ€™re now going to derive posterior distributions for the normal distribution. As weâ€™re going to see, the concepts behind deriving posterior distributions are the same as in the previous two examples. However, the algebraic accounting is a lot more taxing.</p>
<div class="example">
<p><span id="exm:normal" class="example"><strong>Example 3.4  </strong></span>Reaction times can be modeled with a normal distribution. Suppose we have a data set of the reaction times of 30 lorry drivers when they see an obstacle. The reaction times were collected in a test environment on a rolling road. The time until each lorry driver reacts (in milliseconds) is</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="bayesian-inference.html#cb27-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.34</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.27</span>, <span class="fl">0.74</span>, <span class="fl">0.44</span>, <span class="fl">0.46</span>, <span class="fl">0.65</span>, <span class="fl">0.36</span>, <span class="fl">0.55</span>, <span class="fl">0.58</span>, <span class="fl">0.55</span>, <span class="fl">0.53</span>, <span class="fl">0.56</span>, <span class="fl">0.54</span>, <span class="fl">0.61</span>, <span class="fl">0.43</span>, <span class="fl">0.52</span>, <span class="fl">0.45</span>, <span class="fl">0.49</span>, <span class="fl">0.32</span>, <span class="fl">0.33</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.34</span>, <span class="fl">0.60</span>, <span class="fl">0.59</span>, <span class="fl">0.43</span>, <span class="fl">0.57</span>, <span class="fl">0.34</span>)</span>
<span id="cb27-2"><a href="bayesian-inference.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Reaction time (ms)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="bayesian-inference.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y)</span></code></pre></div>
<pre><code>## [1] 0.4896667</code></pre>
<p>Suppose that, somehow, we know the population standard deviation is 0.01<span class="math inline">\(ms\)</span> and we wish to estimate the population mean <span class="math inline">\(\mu\)</span>. By Bayesâ€™ theorem, the posterior distribution is
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \pi(\boldsymbol{y} \mid \mu, \sigma^2) \pi(\mu)
\]</span></p>
<p><strong>Likelihood function</strong>. We assume the each driverâ€™s reaction time is independently and identically distributed such that
<span class="math display">\[
y_i \sim N(\mu, \sigma^2)
\]</span>
The likelihood function is therefore given by the product of the 30 normal density functions as follows,
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \mu, \theta^2) &amp;= \prod_{i=1}^{30} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(y_i - \mu)^2}{2\sigma^2}\right\} \\
&amp;= (2\pi\sigma^2)^{-\frac{30}{2}}\exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{2\sigma^2}\right\}.
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong> We suppose we have no prior beliefs about the values that <span class="math inline">\(\mu\)</span> can take. We assign a normal prior distribution to <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span> despite it being a time. We will set <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma_0^2 = 1000\)</span> to signify our vague prior beliefs, but, for ease, we will use the symbolic values during the derivation of the posterior distribution. We have
<span class="math display">\[
\pi(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\}.
\]</span></p>
<p><strong>Posterior distribution</strong>. To derive the posterior distribution, up to proportionality, we multiply the prior distribution by the likelihood function. As the fractions out the front of both terms do not depend on <span class="math inline">\(\mu\)</span>, we can ignore these.
<span class="math display">\[\begin{align*}
\pi(\mu \mid \boldsymbol{y}, \sigma^2) &amp;\propto\exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{2\sigma^2}\right\}  \exp\left\{\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{2\sigma^2}-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\frac{\sum_{i=1}^{30}y_i^2}{2\sigma^2} + \frac{\mu\sum_{i=1}^{30}y_i}{\sigma^2} - \frac{30\mu^2}{2\sigma^2} - \frac{\mu^2}{2\sigma_0^2} + \frac{\mu\mu_0}{\sigma_0^2} - \frac{\mu_0^2}{2\sigma_0^2}\right\}.
\end{align*}\]</span></p>
<p>We can drop the first and last term as they do not depend on <span class="math inline">\(\mu\)</span>. With some arranging, the equation becomes
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\mu^2\left(\frac{30}{2\sigma^2}  + \frac{1}{2\sigma_0^2}\right) + \mu\left(\frac{\sum_{i=1}^{30}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)  \right\}
\]</span>
Defining <span class="math inline">\(\mu_1 =\left(\frac{\sum_{i=1}^{30}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)\)</span> and <span class="math inline">\(\sigma^2_1 = \left(\frac{30}{\sigma^2} + \frac{1}{\sigma_0^2}\right)^{-1}\)</span> tidies this up and gives
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\frac{\mu^2}{2\sigma_1^2} + \mu\mu_1 \right\}.
\]</span>
Our last step to turning this into a distribution is completing the square. Consider the exponent term, completing the square becomes
<span class="math display">\[
-2\sigma_1^2\mu^2 + \mu\mu_1 = -\frac{1}{2\sigma^2_1}\left(\mu - \frac{\mu_1}{\sigma_1^2} \right)^2.
\]</span>
Therefore, the posterior distribution, up to proportionality, is given by
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\frac{1}{2\sigma^2_1}\left(\mu - \frac{\mu_1}{\sigma_1^2} \right)^2\right\},
\]</span>
and so the posterior distribution of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\mu \mid \boldsymbol{y}, \sigma^2 \sim N(\mu_1, \sigma^2_1)\)</span>.</p>
<p>It may help to consider the meaning of <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\sigma^2_1\)</span>. The variance of the posterior distribution can be thought of as the weighted average of the population and sample precision, where the weight is the number of data points collected. The interpretation of the posterior mean can be seen more easily by writing is as
<span class="math display">\[
\mu  = \sigma_1^2\left(\frac{30\bar{y}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right).
\]</span>
The posterior mean is partially defined through the weighted average of the population and prior means, where the weighting depends on the number of data points collected and how precise the distributions are.</p>
<p>Now we have derived the posterior distribution, we can explore it using R.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="bayesian-inference.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">#data</span></span>
<span id="cb30-2"><a href="bayesian-inference.html#cb30-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb30-3"><a href="bayesian-inference.html#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="bayesian-inference.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">#prior</span></span>
<span id="cb30-5"><a href="bayesian-inference.html#cb30-5" aria-hidden="true" tabindex="-1"></a>sigma0 <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb30-6"><a href="bayesian-inference.html#cb30-6" aria-hidden="true" tabindex="-1"></a>mu0     <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb30-7"><a href="bayesian-inference.html#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="bayesian-inference.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">#posterior</span></span>
<span id="cb30-9"><a href="bayesian-inference.html#cb30-9" aria-hidden="true" tabindex="-1"></a>sigma1.sq <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>)  <span class="sc">+</span> N<span class="sc">/</span>(<span class="fl">0.01</span><span class="sc">^</span><span class="dv">2</span>))<span class="sc">^-</span><span class="dv">1</span></span>
<span id="cb30-10"><a href="bayesian-inference.html#cb30-10" aria-hidden="true" tabindex="-1"></a>mu1       <span class="ot">&lt;-</span> sigma1.sq<span class="sc">*</span>(<span class="fu">sum</span>(y)<span class="sc">/</span>(<span class="fl">0.01</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> mu0<span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb30-11"><a href="bayesian-inference.html#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="bayesian-inference.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(mu1, sigma1.sq) <span class="co">#output mean and variance</span></span></code></pre></div>
<pre><code>## [1] 4.896667e-01 3.333333e-06</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="bayesian-inference.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create plot</span></span>
<span id="cb32-2"><a href="bayesian-inference.html#cb32-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.48</span>, <span class="fl">0.5</span>, <span class="fl">0.0001</span>) </span>
<span id="cb32-3"><a href="bayesian-inference.html#cb32-3" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu, <span class="at">mean =</span> mu1, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma1.sq))</span>
<span id="cb32-4"><a href="bayesian-inference.html#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu, posterior, <span class="at">type =</span><span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The 95% credible interval for the populationâ€™s mean reaction time is</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="bayesian-inference.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), mu1, <span class="fu">sqrt</span>(sigma1.sq))</span></code></pre></div>
<pre><code>## [1] 0.4860883 0.4932451</code></pre>
</div>
<p>One issue in this example is the choice of the prior distribution for <span class="math inline">\(\mu\)</span>. Why are we putting a prior distribution that places weight on negative values, when we are modelling reaction times? We could argue that the resulting posterior distribution places negligible weight on invalid times. The real reason is analytical ease. The resulting posterior distribution has a nice closed form, the normal distribution. When the prior distribution induces the same function form in the posterior distribution, this is known as conjugacy.</p>
<div class="defintion">
<p>If the prior distribution <span class="math inline">\(\pi(\theta)\)</span> has the same distributional family as the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>, then the prior distribution is a <strong>conjugate prior distribution</strong>.</p>
</div>
</div>
<div id="hierarchical-models" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Hierarchical Models<a href="bayesian-inference.html#hierarchical-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many modelling problems, there will be multiple parameters each related to one another. These parameters may be directly related to the model, or they may be parameters we introduce through prior distributions. We can form a hierarchy of these parameters, from closest to further from the data, to construct our model.</p>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 3.5  </strong></span>Letâ€™s consider @ref{exm:exponential} again. We have some data <span class="math inline">\(\boldsymbol{y}\)</span> that are assumed to have been generated from an Exponential distribution with rate parameter <span class="math inline">\(\lambda\)</span>. We placed an Exponential prior distribution with rate <span class="math inline">\(\gamma\)</span> on <span class="math inline">\(\lambda\)</span> and the posterior distribution was <span class="math inline">\(\lambda \mid \boldsymbol{y} \sim \textrm{Gamma}(10, 95 + \gamma)\)</span>.</p>
<p>In that example, we discussed how the choice of <span class="math inline">\(\gamma\)</span> can affect the posterior distribution and conclusions presented to the company. One option is to place a prior distribution on <span class="math inline">\(\gamma\)</span> â€“ a hyperprior distribution. The hierachy formed is
<span class="math display">\[\begin{align*}
\boldsymbol{y} \mid \lambda &amp;\sim \hbox{Exp}(\lambda) &amp; \textrm{(likelihood)} \\
\lambda \mid \gamma &amp;\sim \hbox{Exp}(\gamma) &amp; \textrm{(prior distribution)} \\
\gamma \mid \nu &amp;\sim \hbox{Exp}(\nu) &amp; \textrm{(hyperprior distribution)}  \\
\end{align*}\]</span>.</p>
<p>By Bayesâ€™ theorem, we can write the posterior distribution as
<span class="math display">\[\begin{align*}
\pi(\lambda, \gamma \mid \boldsymbol{y}) &amp;\propto \pi(\boldsymbol{y} \mid \lambda)\pi(\lambda \mid \gamma)\pi(\gamma)
&amp;\propto \lambda^{11}e^{-\lambda(95 + \gamma)}\nu e^{-\nu\gamma}.
\end{align*}\]</span></p>
<p>To derive the full conditional distributions, we only consider the terms that depends on the parameters we are interested in. The full conditional distribution for <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}, \,\gamma) \propto \lambda^{11}e^{-\lambda(95 + \gamma)}.
\]</span>
This is unchanged and shows that <span class="math inline">\(\lambda \mid \boldsymbol{y}, \gamma \sim \textrm{Gamma}(10, 95 + \gamma)\)</span>. The full conditional distribution for <span class="math inline">\(\gamma\)</span> is
<span class="math display">\[
\pi(\gamma \mid \boldsymbol{y}, \,\lambda) \propto e^{-\nu\gamma}.
\]</span>
Therefore the full conditional distribution of <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(\gamma \mid \boldsymbol{y}, \,\lambda \sim \hbox{Exp}(\lambda + \nu)\)</span>.
In the next chapter, we will look at how to sample from these distributions.</p>
</div>
</div>
<div id="prediction" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Prediction<a href="bayesian-inference.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many cases, although we are interested in drawing inference for the model parameters, what we may also be interested in is predicting new values, whose distribution is determined by the model parameters and observed data.</p>
<div class="definition">
<p><span id="def:unlabeled-div-44" class="definition"><strong>Definition 3.2  </strong></span>Suppose we observe some data <span class="math inline">\(\boldsymbol{y}\)</span> given some model parameters <span class="math inline">\(\theta\)</span> and assign a prior distribution to <span class="math inline">\(\theta\)</span> and hence derive the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. The quantity we are interested in is some future observation <span class="math inline">\(z\)</span>, we would like to the distribution of <span class="math inline">\(z\)</span> given the observed data <span class="math inline">\(\boldsymbol{y}\)</span>, denoted by <span class="math inline">\(\pi(z \mid \boldsymbol{y})\)</span>. This distribution, known as the <strong>posterior predictive distribution</strong> of <span class="math inline">\(z\)</span> must be exhibited as a mixture distribution over the possible values of <span class="math inline">\(\theta\)</span> and is written as,
<span class="math display">\[
\pi(z \mid \boldsymbol{y}) = \int \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-45" class="example"><strong>Example 3.6  </strong></span>Students have to submit coursework for a particular statistical modules. However, each semester a number of students miss the deadline and hand in their coursework late. Last year, three out of 20 students handed their coursework in late. This year, the course has thirty students in. How many students can we expect to hand in their coursework late?</p>
<p>We can model the number of students handing their coursework in late, denoted by <span class="math inline">\(Y\)</span>, using a Binomial distribution, i.e.Â <span class="math inline">\(Y \sim \textrm{Bin}(n, \theta)\)</span> where <span class="math inline">\(n\)</span> is the number of students and <span class="math inline">\(\theta\)</span> is the probability of any particular student handing in their coursework late. As in Example <a href="bayesian-inference.html#exm:binom">3.1</a>, we assign a uniform prior distribution to <span class="math inline">\(\theta \sim U[0, 1]\)</span>. Given then observed data, we can derive <span class="math inline">\(\theta \mid \boldsymbol{y} \sim Beta(4, 28)\)</span> (See problem sheets for derivation).</p>
<p>Now we can derive the posterior predictive distribution of <span class="math inline">\(Z\)</span>, the number of students who hand in late. We model <span class="math inline">\(Z\)</span> using a Binomial distribution, <span class="math inline">\(Z \sim \textrm{Bin}(30, \theta)\)</span>. The distribution of <span class="math inline">\(Z\)</span> given the observed data is</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y}) &amp;= \int_0^1 \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta \\
&amp; = \int_0^1 \begin{pmatrix} 30 \\ z \end{pmatrix} \theta^z (1-\theta)^{30 - z} \frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\theta^{3}(1-\theta)^{27}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\int_0^1 \theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
\end{align*}\]</span>
This integral is difficult to evaluate immediately. But by multiplying (and dividing outside the integral) by a constant, we can turn it into the density function of a Beta<span class="math inline">\((5 + z, 58 - z)\)</span> random variable. This integrates to 1.</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y})  &amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\frac{\Gamma(z+4)\Gamma(58-z)}{\Gamma(62)}\int_0^1 \frac{\Gamma(62)}{\Gamma(z+4)\Gamma(58-z)}\theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)\Gamma(z+4)\Gamma(58-z)}{\Gamma(4)\Gamma(28)\Gamma(62)} \quad \textrm{for }  z \in \{0,1,...,30 \}.
\end{align*}\]</span></p>
<p>This code implements the distribution</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="bayesian-inference.html#cb35-1" aria-hidden="true" tabindex="-1"></a>beta.binom.posterior.predictive.distribution <span class="ot">&lt;-</span> <span class="cf">function</span>(z){</span>
<span id="cb35-2"><a href="bayesian-inference.html#cb35-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-3"><a href="bayesian-inference.html#cb35-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-4"><a href="bayesian-inference.html#cb35-4" aria-hidden="true" tabindex="-1"></a>  numerator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">32</span>)<span class="sc">*</span><span class="fu">gamma</span>(z <span class="sc">+</span> <span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">58</span><span class="sc">-</span>z)</span>
<span id="cb35-5"><a href="bayesian-inference.html#cb35-5" aria-hidden="true" tabindex="-1"></a>  denominator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">28</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">62</span>)</span>
<span id="cb35-6"><a href="bayesian-inference.html#cb35-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-7"><a href="bayesian-inference.html#cb35-7" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">choose</span>(<span class="dv">30</span>, z)<span class="sc">*</span>numerator<span class="sc">/</span>denominator</span>
<span id="cb35-8"><a href="bayesian-inference.html#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb35-9"><a href="bayesian-inference.html#cb35-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-10"><a href="bayesian-inference.html#cb35-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We can check that our posterior predictive distribution is a valid probability mass function by checking that the probabilities sum to one.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="bayesian-inference.html#cb36-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">30</span></span>
<span id="cb36-2"><a href="bayesian-inference.html#cb36-2" aria-hidden="true" tabindex="-1"></a>ppd <span class="ot">&lt;-</span> <span class="fu">beta.binom.posterior.predictive.distribution</span>(z)</span>
<span id="cb36-3"><a href="bayesian-inference.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(ppd)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="bayesian-inference.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(z, ppd, <span class="at">xlab =</span> <span class="st">&quot;z&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Posterior predictive mass&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The expected number of students who hand in late is 3.75 and thereâ€™s a 95% chance that up to 8 hand in late.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="bayesian-inference.html#cb39-1" aria-hidden="true" tabindex="-1"></a>z<span class="sc">%*%</span>ppd <span class="co">#expectation</span></span></code></pre></div>
<pre><code>##      [,1]
## [1,] 3.75</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="bayesian-inference.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(z, <span class="fu">cumsum</span>(ppd)) <span class="co">#CDF</span></span></code></pre></div>
<pre><code>##        z           
##  [1,]  0 0.06029453
##  [2,]  1 0.18723037
##  [3,]  2 0.35156696
##  [4,]  3 0.51889148
##  [5,]  4 0.66530044
##  [6,]  5 0.78021765
##  [7,]  6 0.86309065
##  [8,]  7 0.91880359
##  [9,]  8 0.95404202
## [10,]  9 0.97513714
## [11,] 10 0.98713498
## [12,] 11 0.99363285
## [13,] 12 0.99698773
## [14,] 13 0.99863936
## [15,] 14 0.99941423
## [16,] 15 0.99976022
## [17,] 16 0.99990696
## [18,] 17 0.99996591
## [19,] 18 0.99998826
## [20,] 19 0.99999622
## [21,] 20 0.99999887
## [22,] 21 0.99999969
## [23,] 22 0.99999992
## [24,] 23 0.99999998
## [25,] 24 1.00000000
## [26,] 25 1.00000000
## [27,] 26 1.00000000
## [28,] 27 1.00000000
## [29,] 28 1.00000000
## [30,] 29 1.00000000
## [31,] 30 1.00000000</code></pre>
</div>
</div>
<div id="non-informative-prior-distibrutions" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Non-informative Prior Distibrutions<a href="bayesian-inference.html#non-informative-prior-distibrutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen in a few examples how the choice of the prior distribution (and prior parameters) can impact posterior distributions and the resulting conclusions. As the choice of prior distribution is subjective, it is the main criticism of Bayesian inference. A possible way around this is to use a prior distribution that reflects a lack of information about <span class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 3.3  </strong></span>A <strong>non-informative prior distribution</strong> is a prior distribution that places equal weight on the every possible value of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-47" class="example"><strong>Example 3.7  </strong></span>In Example <a href="bayesian-inference.html#exm:binom">3.1</a>, we assigned a uniform prior distribution to the parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-48" class="theorem"><strong>Theorem 3.1  (Jeffrey) </strong></span>Given some observed data <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_N\}\)</span>, an invariant prior distribution is
<span class="math display">\[
\pi(\theta) \propto \sqrt{I_\theta(\boldsymbol{y})},
\]</span>
where <span class="math inline">\(I_\theta(\boldsymbol{y})\)</span> is the Fisher information for <span class="math inline">\(\theta\)</span> contained in <span class="math inline">\(\boldsymbol{y}\)</span>.</p>
</div>
<p>Jeffrey argues that if there are two ways of parameterising a model, e.g.Â via <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi\)</span>, then the priors on these parameters should be equivalent. In other words, the prior distribution should be invariant under sensible (one-to-one) transformations.</p>
<div class="proof">
<p><span id="unlabeled-div-49" class="proof"><em>Proof</em>. </span>Recall that the distribution of <span class="math inline">\(\psi = h(\theta)\)</span>, for some one-to-one function <span class="math inline">\(h\)</span>, is invariant to the distribution of <span class="math inline">\(\theta\)</span> if
<span class="math display">\[
\pi(\psi) = \pi(\theta) \left|\frac{d\theta}{d\psi}\right|.
\]</span>
Transforming the Fisher information for <span class="math inline">\(\psi\)</span> shows
<span class="math display">\[\begin{align*}
I_\psi(\boldsymbol{y}) &amp;= - \mathbb{E}\left(\frac{d^2\log \pi(\boldsymbol{y} \mid \psi)}{d\psi^2}\right) \\
&amp; = \mathbb{E}\left(\frac{d^2 \log \pi(\boldsymbol{y} \mid \theta = h^{-1}(\psi))}{d\theta^2}\right) \left(\frac{d\theta}{d\psi}\right)^2 \\
&amp; = I_\theta(\boldsymbol{y})\left(\frac{d\theta}{d\psi}\right)^2 .
\end{align*}\]</span>
Thus <span class="math inline">\(\sqrt{I_\psi(\boldsymbol{y})} = \sqrt{I_\theta(\boldsymbol{y})} \left|\frac{d\theta}{d\psi}\right|\)</span> and <span class="math inline">\(\sqrt{I_\psi(\boldsymbol{y})}\)</span> and <span class="math inline">\(\sqrt{I_\theta(\boldsymbol{y})}\)</span> are invariant prior distributions.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-50" class="example"><strong>Example 3.8  </strong></span>In Example <a href="bayesian-inference.html#exm:binom">3.1</a>, we modelled the number of bot accounts on a social media website by <span class="math inline">\(Y \sim \textrm{Bin}(n, \theta)\)</span>. To construct Jeffreyâ€™s prior distribution for <span class="math inline">\(\theta\)</span>, we must first derive the Fisherâ€™s information matrix.<br />
<span class="math display">\[\begin{align*}
&amp;\pi(y \mid \theta) = \begin{pmatrix} n \\ y \end{pmatrix} \theta^y (1-\theta)^{n-y}\\
\implies &amp;\log \pi(y \mid \theta) = \log \begin{pmatrix} n \\ y \end{pmatrix} + y \log\theta + (n-y)\log(1-\theta) \\
\implies &amp;\frac{\partial \log \pi(y \mid \theta)}{\partial \theta} = \frac{y}{\theta} - \frac{n-y}{1-\theta} \\
\implies &amp;\frac{\partial^2 \log \pi(y \mid \theta)}{\partial \theta^2} = -\frac{y}{\theta^2} + \frac{n-y}{(1-\theta)^2} \\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{\mathbb{E}(y)}{\theta^2} + \frac{n-\mathbb{E}(y)}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n\theta}{\theta^2} + \frac{n-n\theta}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta} + \frac{n}{1-\theta}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta(1-\theta)} \\
\implies &amp;I_\theta(y) \propto \frac{1}{\theta(1-\theta)}.
\end{align*}\]</span></p>
<p>Hence Jeffreyâ€™s prior is <span class="math inline">\(\pi(\theta) \propto \theta^{-\frac{1}{2}}(1-\theta)^{-\frac{1}{2}}\)</span>. This functional dependency on <span class="math inline">\(\theta\)</span> shows that <span class="math inline">\(\theta \sim \textrm{Beta}(\frac{1}{2}, \frac{1}{2})\)</span>.</p>
</div>
</div>
<div id="bernstein-von-mises-theorem" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Bernstein-von-Mises Theorem<a href="bayesian-inference.html#bernstein-von-mises-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have considered Bayesian methods in contrast to frequentist ones. The Bernstein-von-Mises theorem is a key theorem linking the two inference methods.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-51" class="theorem"><strong>Theorem 3.2  (Bernstein-von-Mises) </strong></span>For a well-specified model <span class="math inline">\(\pi(\boldsymbol{y} \mid \theta)\)</span> with a fixed number of parameters, and for a smooth prior distribution <span class="math inline">\(\pi(\theta)\)</span> that is non-zero around the MLE <span class="math inline">\(\hat{\theta}\)</span>, then
<span class="math display">\[
\left|\left| \pi(\theta \mid \boldsymbol{y}) - N\left(\hat{\theta}, \frac{I(\hat{\theta})^{-1}}{n}\right) \right|\right|_{TV} \rightarrow 0,
\]</span>
where <span class="math inline">\(||p - q||_{TV}\)</span> is the total variation distance between distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>:
<span class="math display">\[
||p - q||_{TV} = \frac{1}{2}\int|\pi(x) - q(x)|\,dx.
\]</span></p>
</div>
<p>The Berstein-von-Mises theorem says that as the number of data points approaches infinity, the posterior distribution tends to a Normal distribution centered around the MLE and variance dependent on the Fisher information. The proof of this theorem is out of the scope of this module, but can be found in Asymptotic Statistics (2000) by A. W. van der Vaart.</p>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Exercises<a href="bayesian-inference.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-52" class="exercise"><strong>Exercise 3.1  </strong></span>Suppose <span class="math inline">\(y_1, \ldots, y_N \sim \hbox{Geom}(p)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Derive the likelihood function and then the maximum likelihood estimator for <span class="math inline">\(p\)</span>.</li>
<li>By letting <span class="math inline">\(p \sim \hbox{Beta}(\alpha, \beta)\)</span>, derive the posterior distribution for <span class="math inline">\(p\)</span> given the data.</li>
<li>Compare the maximum likelihood estimate with with expectation of the posterior distribution. What values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> result in a posterior expectation that is equal to the maximum likelihood estimate? Why is this inadvisable?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-53" class="exercise"><strong>Exercise 3.2  </strong></span>When someone is infected with a disease, itâ€™s common to model the time that they are infectious for, denoted by T, with a Gamma distribution. Suppose you observe 100 measles infections and <span class="math inline">\(\sum_{i=1}^{100}t_i\)</span> = 870 days. Based on advice from clinicians, you suppose <span class="math inline">\(T \sim \Gamma(5, \theta)\)</span>. Using an <span class="math inline">\(\theta \sim \hbox{Exp}(0.01)\)</span> prior distribution, derive the posterior distribution. What is the 95% credible interval for <span class="math inline">\(\theta\)</span>?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-54" class="exercise"><strong>Exercise 3.3  </strong></span>The density function for the Pareto distribution with scale <span class="math inline">\(\alpha\)</span> and shape <span class="math inline">\(\beta\)</span> is given by
<span class="math display">\[
\pi(x \mid \alpha = 1,\, \beta) = \frac{\beta}{x^{\beta+1}}, \qquad x &gt; 1,
\]</span>
where <span class="math inline">\(\beta\)</span> is positive real valued parameter and <span class="math inline">\(\alpha = 1\)</span>. Suppose the data are denoted by <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_N\}\)</span>. Place a Gamma prior distribution on <span class="math inline">\(\beta\)</span> such that <span class="math inline">\(\beta \sim \Gamma(a, b)\)</span>. Derive the posterior distribution for <span class="math inline">\(\beta\)</span> given the data.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-55" class="exercise"><strong>Exercise 3.4  </strong></span>You are given the data are exponentially distributed with rate <span class="math inline">\(\lambda,\)</span> i.e.Â <span class="math inline">\(Y_1, \ldots, Y_N \sim \hbox{Exp}(\lambda)\)</span>. Your prior belief is that <span class="math inline">\(\lambda \in (0, 1)\)</span>. Show that the posterior distribution <span class="math inline">\(\pi(\lambda \mid \boldsymbol{y})\)</span> has no closed form when the prior distribution for <span class="math inline">\(\lambda \sim \hbox{Beta}(\alpha, \beta)\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-56" class="solution"><em>Solution</em>. </span>Since our observations are independent, the likelihood function is the product of <span class="math inline">\(N\)</span> exponential density functions,
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \lambda) &amp;= \prod_{i=1}^N \lambda \exp\left\{-\lambda y_i\right\}\\
&amp; = \lambda^N  \exp\left\{-\lambda \sum_{i=1}^N y_i\right\}.
\end{align*}\]</span></p>
<p>The prior distribution is <span class="math inline">\(\pi(\lambda) = \frac{1}{B(\alpha, \beta)}\lambda^{\alpha - 1}(1-\lambda)^{\beta - 1}\)</span> and the posterior distribution is therefore proportional to
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}) \propto \lambda^{\alpha +N - 1}(1-\lambda)^{\beta - 1}\exp\left\{-\lambda \sum_{i=1}^N y_i \right\}.
\]</span>
Hence, the posterior distribution has no closed form.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-57" class="exercise"><strong>Exercise 3.5  </strong></span>Suppose that <span class="math inline">\(Y_i \sim \hbox{Binom}(n, p)\)</span> are iid for <span class="math inline">\(i \in \{1, \ldots, N\}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>By placing a Beta prior distribution on <span class="math inline">\(p\)</span> such that <span class="math inline">\(p \sim \hbox{Beta}(\alpha, \beta)\)</span> derive the posterior distribution.</li>
<li>Suppose that <span class="math inline">\(y_{N+1}\)</span> is then observed and is also independently drawn from the same distribution. Derive the posterior distribution by updating the previous distribution (i.e.Â your posterior distribution for the previous part becomes your prior distribution).</li>
<li>Show that you obtain the same distribution if you observe all <span class="math inline">\(N+1\)</span> data points at the start of the process.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-58" class="exercise"><strong>Exercise 3.6  </strong></span>Let <span class="math inline">\(Y_1, \ldots, Y_N \sim \hbox{Pois}(\lambda)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Use Bayesâ€™ theorem to derive the posterior distribution of <span class="math inline">\(\lambda\)</span> given the observed data.</li>
<li>Place a Gamma prior distribution on <span class="math inline">\(\lambda \sim \Gamma(\alpha, \beta)\)</span> and derive the form the posterior distribution takes.</li>
<li>Discuss the effects of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> on the posterior distribution.</li>
<li>Derive the posterior predictive distribution for a new observation <span class="math inline">\(\tilde{y}\)</span>. <em>Hint: The mass function for the negative binomial distribution with failures <span class="math inline">\(r\)</span> and probability of success <span class="math inline">\(p\)</span> is </em>
<span class="math display">\[
\pi(x = k \mid r, p) = \begin{pmatrix} k + r - 1 \\ k \end{pmatrix} (1-p)^k p^r.
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-59" class="exercise"><strong>Exercise 3.7  </strong></span>Consider the data from @{exm:normal}. Suppose the police pull over a lorry driver after they failed to stop at a red light. During a test at the side of the road, the lorry driverâ€™s reaction time is tested and comes out as 77ms.</p>
<ol style="list-style-type: decimal">
<li>By constructing a credible interval from the posterior predictive distribution, determine if the driverâ€™s reaction time is suspicious.</li>
<li>Why is this not a good comparison to make?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-60" class="exercise"><strong>Exercise 3.8  </strong></span>A distribution is said to belong to the exponential family of distributions if its density function has the form
<span class="math display">\[
\pi(y_i \mid \theta) = f(y_i)g(\theta)\exp\{\nu(\theta)T(x)\}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Derive the likelihood function with the data <span class="math inline">\(\{y_1, \ldots, y_N\}\)</span>.</li>
<li>Show that the following prior distribution induces conjugacy:
<span class="math display">\[
\pi(\theta) \propto g(\theta)^\alpha\exp\{\beta\nu(\theta)\}.
\]</span>
<em>Note: In general, distributions that belong to the exponential family have conjugate prior distributions. This is because conjugacy involve manipulation of sufficient statistics.</em></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-61" class="exercise"><strong>Exercise 3.9  </strong></span>Let <span class="math inline">\(Y_1, \ldots, Y_N \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known. Derive an invariant prior distribution (Jeffreyâ€™s prior distribution) for <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-62" class="exercise"><strong>Exercise 3.10  </strong></span>Consider Example @(exm:exponential) featuring the exponential distribution.</p>
<ol style="list-style-type: decimal">
<li>Construct an invariant prior distribution for this distribution.</li>
<li>Derive the posterior distribution using the invariant prior distribution.</li>
<li>Using integration, discuss the validity of the prior distribution.</li>
</ol>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="programming-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
