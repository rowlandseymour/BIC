<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Bayesian inference | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Bayesian inference | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Bayesian inference | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Rowland Seymour" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="programming-in-r.html"/>
<link rel="next" href="sampling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#getting-help"><i class="fa fa-check"></i><b>0.4</b> Getting help</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.5</b> Recommended books and videos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-probability"><i class="fa fa-check"></i><b>1.3</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#conditional-probability-and-exchangability"><i class="fa fa-check"></i><b>1.4</b> Conditional Probability and Exchangability</a></li>
<li class="chapter" data-level="1.5" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.5</b> Bayesâ€™ Theorem</a></li>
<li class="chapter" data-level="1.6" data-path="fundamentals.html"><a href="fundamentals.html#problems"><i class="fa fa-check"></i><b>1.6</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-binomial-distirbution"><i class="fa fa-check"></i><b>3.1</b> The Binomial distirbution</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting conclsuions from Bayesian inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-exponential-distribution"><i class="fa fa-check"></i><b>3.3</b> The Exponential distribution</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-normal-distribtuion"><i class="fa fa-check"></i><b>3.4</b> The Normal Distribtuion</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#predicition"><i class="fa fa-check"></i><b>3.5</b> Predicition</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.6</b> Non-informative prior distibrutions</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bernstein-von-mises-theorem"><i class="fa fa-check"></i><b>3.7</b> Bernstein-von-Mises Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse transform sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.4</b> Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>4.5</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#gibbs-sampler"><i class="fa fa-check"></i><b>4.6</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="4.7" data-path="sampling.html"><a href="sampling.html#metropolis-hastings"><i class="fa fa-check"></i><b>4.7</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.8" data-path="sampling.html"><a href="sampling.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>4.8</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>5</b> Advanced computation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="advanced-computation.html"><a href="advanced-computation.html#heiracichal-models"><i class="fa fa-check"></i><b>5.1</b> Heiracichal Models</a></li>
<li class="chapter" data-level="5.2" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-ellicitation"><i class="fa fa-check"></i><b>5.2</b> Prior Ellicitation</a></li>
<li class="chapter" data-level="5.3" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>5.3</b> Data Augmentation</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-inference" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Bayesian inference<a href="bayesian-inference.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Whereas Chapter 1 dealt with the fundamentals of Bayesian inference and definitions, Chapter 2 is much more practical. Weâ€™re going to be deriving posterior distributions and proving when it does and doesnâ€™t work.</p>
<div id="the-binomial-distirbution" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> The Binomial distirbution<a href="bayesian-inference.html#the-binomial-distirbution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The first example weâ€™re going to go through is with the binomial distribution.</p>
<div class="example">
<p><span id="exm:binom" class="example"><strong>Example 3.1  </strong></span>A technician in a crisp factory wants to test the reliability of a production line. She collects a random sample of 200 crisp packets from line 1. She finds three packets weigh either too much or too little and must be rejected. She uses a Bayesian method to estimate the probability a crisp packet is the incorrect weight. She begins by denoting the weight of the 200 crisp packets <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_{200}\}\)</span> and the probability a crisp packet is the incorrect weight by <span class="math inline">\(\theta\)</span>. By Bayesâ€™ theorem
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y}\mid \theta) \pi(\theta)
\]</span></p>
<p><strong>Likelihood function</strong> <span class="math inline">\(\pi(\boldsymbol{y}\mid \theta)\)</span>. We observe a 200 trials each with a probability of success or failure. The binomial distribution seems the most suitable way of modelling this. Therefore
<span class="math display">\[
\pi(\boldsymbol{y}\mid \theta) = \begin{pmatrix} 200 \\ 3 \end{pmatrix} \theta^3(1-\theta)^{197}
\]</span>
We are going to ignore one questionable assumption here that the observations are independent. That is a crisp packet being the wrong weight, doesnâ€™t affect the weight of the next crisp packet to be produced.</p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\theta)\)</span>. We now need to describe our prior beliefs about <span class="math inline">\(\theta\)</span>. We have no reason to suggest <span class="math inline">\(\theta\)</span> takes any specific value, so we use a uniform prior distribution <span class="math inline">\(\theta \sim U[0, 1]\)</span>, where <span class="math inline">\(\pi(\theta) = 1\)</span> for <span class="math inline">\(\theta \in [0, 1]\)</span>.</p>
<p><strong>Posterior distribution</strong> <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. We can now derive the posterior distribution up to proportionality
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) \propto \theta^3(1-\theta)^{197}.
\]</span>
This functional dependence on p identifies the <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span> as a Beta distribution. The PDF for the beta distribution with shape parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
\pi(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1-x)^{\beta - 1}.
\]</span>
The posterior distribution is therefore <span class="math inline">\(\theta \mid \boldsymbol{y} \sim \textrm{Beta}(4, 198)\)</span>.</p>
</div>
</div>
<div id="reporting-conclsuions-from-bayesian-inference" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Reporting conclsuions from Bayesian inference<a href="bayesian-inference.html#reporting-conclsuions-from-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous example, we derived the posterior distribution <span class="math inline">\(\theta \mid \boldsymbol{y} \sim \textrm{Beta}(4, 198)\)</span>. But often, we want to share more descriptive information about our beliefs given the observed data. In this example, the posterior mean given the data is <span class="math inline">\(\frac{4}{198} = \frac{2}{99}\)</span>. That is to say given the data, we expect that for every 99 packets of crisps produced, two will be the incorrect weight. The posterior mode for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\frac{3}{200}\)</span> or 1.5%.</p>
<p>It is important to share the uncertainty about out beliefs. In a frequentist framework, this would be via a confidence interval. The Bayesian analogues is a credible interval.</p>
<div class="definition">
<p><span id="def:unlabeled-div-19" class="definition"><strong>Definition 3.1  </strong></span>A <strong>credible interval</strong> is a central interval of posterior probability which corresponds, in the case of a 100<span class="math inline">\((1-\alpha)\)</span>% interval, to the range of values that capture 100<span class="math inline">\((1-\alpha)\)</span>% of the posterior probability.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-20" class="example"><strong>Example 3.2  </strong></span>The 95% credible interval for the Binomial example is given by</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="bayesian-inference.html#cb8-1" aria-hidden="true" tabindex="-1"></a>cred.int<span class="fl">.95</span> <span class="ot">&lt;-</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">4</span>, <span class="dv">198</span>)</span>
<span id="cb8-2"><a href="bayesian-inference.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cred.int<span class="fl">.95</span>, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.005 0.043</code></pre>
<p>This says that we believe there is a 95% chance that the probability of a crisp packet being the incorrect weight lies between 0.005 and 0.043. This is a much more intuitive definition to the confidence interval, which says if we ran the experiment an infinite number of times and computed an infinite number of confidence intervals, 95% of them would contain the true value of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="the-exponential-distribution" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> The Exponential distribution<a href="bayesian-inference.html#the-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:unlabeled-div-21" class="example"><strong>Example 3.3  </strong></span>An insurance company want to estimate the time until a claim is made on a specific policy. They describe the rate at which claims come in by <span class="math inline">\(\lambda\)</span>. The company provides a sample of 10 months at which a claim was made <span class="math inline">\(\boldsymbol{y} = \{14, 10, 6, 7, 13, 9, 12, 7, 9, 8\}\)</span>. By Bayesâ€™ theorem, the posterior distribution for <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}) \propto \pi(\lambda \mid \boldsymbol{y}) \pi(\lambda)
\]</span></p>
<p><strong>Likelihood function</strong> <span class="math inline">\(\pi(\lambda \mid \boldsymbol{y})\)</span>. The exponential distribution is a good way of modelling lifetimes or the length of time until an event happens. Assuming all the claims are independent, the likelihood function is given by
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \lambda) &amp;= \prod_{i=1}^{10} \lambda e^{-\lambda y_i} \\
&amp; = \lambda^{10}e^{-\lambda \sum_{i=1}^{10} y_i} \\
&amp; = \lambda^{10} e^{-95\lambda}
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\lambda)\)</span>. As we are modelling a rate parameter, we know it must be positive and continuous. We decide to use an exponential prior distribution for <span class="math inline">\(\lambda\)</span>, but leave the choice of the rate parameter up to the insurance professionals at the insurance company. The prior distribution is given by <span class="math inline">\(\pi(\lambda) \sim \textrm{Exp}(\chi).\)</span></p>
<p><strong>Posterior distribution</strong> <span class="math inline">\(\pi(\lambda \mid \boldsymbol{y})\)</span>. We now have all the ingredients to derive the posterior distribution. It is given by
<span class="math display">\[\begin{align*}
\pi(\lambda \mid \boldsymbol{y}) &amp;\propto \lambda^{10} e^{-95\lambda} \times \lambda e^{-\chi\lambda} \\
&amp; \propto \lambda^{11}e^{-(95 + \chi)\lambda}
\end{align*}\]</span>
The functional form tells us that the posterior distribution is a Gamma distribution. The PDF of a gamma random variable with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
\pi(x \mid \alpha, \beta) = \frac{\alpha^\beta}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}.
\]</span>
The distribution of the rate of the claims given the observed data is <span class="math inline">\(\lambda \mid \boldsymbol{y} \sim \textrm{Gamma}(10, 95 + \chi)\)</span>.</p>
<p>The posterior mean months until a claim is <span class="math inline">\(\frac{10}{95 + \chi}\)</span>. We can see the effect of the choice of rate parameter in this mean. Small values of <span class="math inline">\(\chi\)</span> yield vague prior distribution, which plays a minimal role in the posterior distribution. Large values of <span class="math inline">\(\chi\)</span> result in prior distributions that contribute a lot to the posterior distribution. The plots below show the prior and posterior distributions for <span class="math inline">\(\chi = 0.01\)</span> and <span class="math inline">\(\chi = 50\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="bayesian-inference.html#cb10-1" aria-hidden="true" tabindex="-1"></a>plot.distributions <span class="ot">&lt;-</span> <span class="cf">function</span>(chi){</span>
<span id="cb10-2"><a href="bayesian-inference.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate at selected values of theta</span></span>
<span id="cb10-3"><a href="bayesian-inference.html#cb10-3" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.3</span>, <span class="fl">0.001</span>) </span>
<span id="cb10-4"><a href="bayesian-inference.html#cb10-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-5"><a href="bayesian-inference.html#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate prior desnity</span></span>
<span id="cb10-6"><a href="bayesian-inference.html#cb10-6" aria-hidden="true" tabindex="-1"></a>  prior <span class="ot">&lt;-</span> <span class="fu">dexp</span>(theta, <span class="at">rate =</span> chi)</span>
<span id="cb10-7"><a href="bayesian-inference.html#cb10-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-8"><a href="bayesian-inference.html#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#evaluate posterior density</span></span>
<span id="cb10-9"><a href="bayesian-inference.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  posterior <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(theta, <span class="at">shape =</span> <span class="dv">10</span>, <span class="at">rate =</span> <span class="dv">95</span> <span class="sc">+</span> chi)</span>
<span id="cb10-10"><a href="bayesian-inference.html#cb10-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-11"><a href="bayesian-inference.html#cb10-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-12"><a href="bayesian-inference.html#cb10-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#plot</span></span>
<span id="cb10-13"><a href="bayesian-inference.html#cb10-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(theta, posterior, <span class="at">type=</span> <span class="st">&#39;l&#39;</span>, </span>
<span id="cb10-14"><a href="bayesian-inference.html#cb10-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">xlab =</span> <span class="fu">expression</span>(theta), <span class="at">ylab =</span> <span class="st">&quot;density&quot;</span>)</span>
<span id="cb10-15"><a href="bayesian-inference.html#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(theta, prior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb10-16"><a href="bayesian-inference.html#cb10-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Posterior&quot;</span>, <span class="st">&quot;Prior&quot;</span>),  </span>
<span id="cb10-17"><a href="bayesian-inference.html#cb10-17" aria-hidden="true" tabindex="-1"></a>         <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb10-18"><a href="bayesian-inference.html#cb10-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-19"><a href="bayesian-inference.html#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="bayesian-inference.html#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="fl">0.01</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="bayesian-inference.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="dv">50</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>The insurance managers recommend that because this is a new premium, a vague prior distribution be used and <span class="math inline">\(\chi = 0.01\)</span>. The posterior mean is <span class="math inline">\(\frac{10}{95.01} \approx 0.105\)</span> and the 95% credible interval is</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="bayesian-inference.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">10</span>, <span class="fl">95.01</span>), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.05 0.18</code></pre>
</div>
</div>
<div id="the-normal-distribtuion" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> The Normal Distribtuion<a href="bayesian-inference.html#the-normal-distribtuion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Normal distribution is incredibly useful for modelling a wide range of natural phenomena and in its own right. Weâ€™re now going to derive posterior distributions for the normal distribution. As weâ€™re going to see, the concepts behind deriving posterior distributions are the same as in the previous two examples. However, the algebraic accounting is a lot more taxing.</p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 3.4  </strong></span><strong>Known variance</strong>. Reaction times can be modeled with a normal distribution. Suppose we have a data set of the reaction times of 30 lorry drivers when they see an obstacle. The reaction times were collected in a test environment on a rolling road. The time until each lorry driver reacts (in milliseconds) is</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="bayesian-inference.html#cb14-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.34</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.27</span>, <span class="fl">0.74</span>, <span class="fl">0.44</span>, <span class="fl">0.46</span>, <span class="fl">0.65</span>, <span class="fl">0.36</span>, <span class="fl">0.55</span>, <span class="fl">0.58</span>, <span class="fl">0.55</span>, <span class="fl">0.53</span>, <span class="fl">0.56</span>, <span class="fl">0.54</span>, <span class="fl">0.61</span>, <span class="fl">0.43</span>, <span class="fl">0.52</span>, <span class="fl">0.45</span>, <span class="fl">0.49</span>, <span class="fl">0.32</span>, <span class="fl">0.33</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.34</span>, <span class="fl">0.60</span>, <span class="fl">0.59</span>, <span class="fl">0.43</span>, <span class="fl">0.57</span>, <span class="fl">0.34</span>)</span>
<span id="cb14-2"><a href="bayesian-inference.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Reaction time (ms)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="bayesian-inference.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y)</span></code></pre></div>
<pre><code>## [1] 0.4896667</code></pre>
<p>Suppose that, somehow, we know the population standard deviation is 0.01<span class="math inline">\(ms\)</span> and we wish to estimate the population mean <span class="math inline">\(\mu\)</span>. By Bayesâ€™ theorem, the posterior distribution is
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \pi(\boldsymbol{y} \mid \mu, \sigma^2) \pi(\mu)
\]</span></p>
<p><strong>Likelihood function</strong>. We assume the each driverâ€™s reaction time is independently and identically distributed such that
<span class="math display">\[
y_i \sim N(\mu, \sigma^2)
\]</span>
The likelihood function is therefore given by the product of the 30 normal desnity functions as follows
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \mu, \theta^2) &amp;= \prod_{i=1}^{30} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(y_i - \mu)^2}{\sigma^2}\right\} \\
&amp;= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\}
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong> We suppose we have no prior beliefs about the values that <span class="math inline">\(\mu\)</span> can take. We assign a normal prior distribution to <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span> despite it being a time. We will set <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma_0^2 = 1000\)</span> to signify our vague prior beliefs, but, for ease, we will use the symbolic values during the derivation of the posterior distribution. We have
<span class="math display">\[
\pi(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\}.
\]</span></p>
<p><strong>Posterior distribution</strong>. To derive the posterior distribution, up to proportionality, we multiply the prior distribution by the likelihood function. As the fractions out the front of both terms do not depend on <span class="math inline">\(\mu\)</span>, we can ignore these.
<span class="math display">\[\begin{align*}
\pi(\mu \mid \boldsymbol{y}, \sigma^2) &amp;\propto\exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\}  \exp\left\{\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\frac{\sum_{i=1}^{30}y_i^2}{2\sigma^2} + \frac{\mu\sum_{i=1}^{30}y_i}{\sigma^2} - \frac{30\mu^2}{2\sigma^2} - \frac{\mu^2}{2\sigma_0^2} + \frac{\mu\mu_0}{\sigma_0^2} - \frac{\mu_0^2}{2\sigma_0^2}\right\}.
\end{align*}\]</span></p>
<p>We can drop the first and last term as they do not depend on <span class="math inline">\(\mu\)</span>. With some arranging, the equation becomes
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\mu^2\left(\frac{30}{2\sigma^2}  + \frac{1}{2\sigma_0^2}\right) + \mu\left(\frac{\sum_{i=1}^{30}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)  \right\}
\]</span>
Defining <span class="math inline">\(\mu_1 =\left(\frac{\sum_{i=1}^{30}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)\)</span> and <span class="math inline">\(\sigma^2_1 = \left(\frac{30}{\sigma^2} + \frac{1}{\sigma_0^2}\right)^{-1}\)</span> tidies this up and gives
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\frac{\mu^2}{2\sigma_1^2} + \mu\mu_1 \right\}.
\]</span>
Our last step to turning this into a distribution is completing the square. Consider the exponent term, completing the square becomes
<span class="math display">\[
-2\sigma_1^2\mu^2 + \mu\mu_1 = -\frac{1}{2\sigma^2_1}\left(\mu - \frac{\mu_1}{\sigma_1^2} \right)^2.
\]</span>
Therefore, the posterior distribution, up to proportionality, is given by
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-\frac{1}{2\sigma^2_1}\left(\mu - \frac{\mu_1}{\sigma_1^2} \right)^2\right\},
\]</span>
and so the posterior distribution of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\mu \mid \boldsymbol{y}, \sigma^2 \sim N(\mu_1, \sigma^2_1)\)</span>.</p>
<p>It may help to consider the meaning of <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\sigma^2_1\)</span>. The variance of the posterior distribution can be thought of as the weighted average of the population and sample precision, where the weight is the number of data points collected. The interpretation of the posterior mean can be seen more easily by writing is as
<span class="math display">\[
\mu  = \sigma_1^2\left(\frac{30\bar{y}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right).
\]</span>
The posterior mean is partially defined through the weighted average of the population and prior means, where the weighting depends on the number of data points collected and how precise the distributions are.</p>
<p>Now we have derived the posterior distribution, we can explore it using R.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="bayesian-inference.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#data</span></span>
<span id="cb17-2"><a href="bayesian-inference.html#cb17-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb17-3"><a href="bayesian-inference.html#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="bayesian-inference.html#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">#prior</span></span>
<span id="cb17-5"><a href="bayesian-inference.html#cb17-5" aria-hidden="true" tabindex="-1"></a>sigma0 <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb17-6"><a href="bayesian-inference.html#cb17-6" aria-hidden="true" tabindex="-1"></a>mu0     <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb17-7"><a href="bayesian-inference.html#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="bayesian-inference.html#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">#posterior</span></span>
<span id="cb17-9"><a href="bayesian-inference.html#cb17-9" aria-hidden="true" tabindex="-1"></a>sigma1.sq <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>)  <span class="sc">+</span> N<span class="sc">/</span>(<span class="fl">0.01</span><span class="sc">^</span><span class="dv">2</span>))<span class="sc">^-</span><span class="dv">1</span></span>
<span id="cb17-10"><a href="bayesian-inference.html#cb17-10" aria-hidden="true" tabindex="-1"></a>mu1       <span class="ot">&lt;-</span> sigma1.sq<span class="sc">*</span>(<span class="fu">sum</span>(y)<span class="sc">/</span>(<span class="fl">0.01</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> mu0<span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb17-11"><a href="bayesian-inference.html#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="bayesian-inference.html#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(mu1, sigma1.sq) <span class="co">#output mean and variance</span></span></code></pre></div>
<pre><code>## [1] 4.896667e-01 3.333333e-06</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="bayesian-inference.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create plot</span></span>
<span id="cb19-2"><a href="bayesian-inference.html#cb19-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.48</span>, <span class="fl">0.5</span>, <span class="fl">0.0001</span>) </span>
<span id="cb19-3"><a href="bayesian-inference.html#cb19-3" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu, <span class="at">mean =</span> mu1, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma1.sq))</span>
<span id="cb19-4"><a href="bayesian-inference.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu, posterior, <span class="at">type =</span><span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" />
The 95% credible interval for the populationâ€™s mean reaction time is</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="bayesian-inference.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), mu1, <span class="fu">sqrt</span>(sigma1.sq))</span></code></pre></div>
<pre><code>## [1] 0.4860883 0.4932451</code></pre>
</div>
<p>One issue in this example is the choice of the prior distribution for <span class="math inline">\(\mu\)</span>. Why are we putting a prior distribution that places weight on negative values, when we are modelling reaction times? We could argue that the resulting posterior distribution places negligible weight on invalid times. The real reason is analytical ease. The resulting posterior distribution has a nice closed form, the normal distribution. When the prior distribution induces the same function form in the posterior distribution, this is known as conjugacy.</p>
<div class="defintion">
<p>If the prior distribution <span class="math inline">\(\pi(\theta)\)</span> has the same distributional family as the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>, then the prior distribution is a <strong>conjugate prior distribution</strong>.</p>
</div>
</div>
<div id="predicition" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Predicition<a href="bayesian-inference.html#predicition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many cases, although we are interested in estimating the model parameters, what weâ€™re really interested in is predicting new values, whose distribution is determined by the model parameters.</p>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>Definition 3.2  </strong></span>Suppose we observe some data <span class="math inline">\(\boldsymbol{y}\)</span> and derive the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. The quantity we are interested in is some future observation <span class="math inline">\(z\)</span>, we would like to the distribution of <span class="math inline">\(z\)</span> given the observed data <span class="math inline">\(\boldsymbol{y}\)</span>, <span class="math inline">\(\pi(z \mid \boldsymbol{y})\)</span>. This distribution, known as the <strong>posterior predictive distribution</strong> of <span class="math inline">\(z\)</span>, must be exhibited as a mixture distribution over the possible values of <span class="math inline">\(\theta\)</span>. We must write
<span class="math display">\[
\pi(z \mid \boldsymbol{y}) = \int \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-24" class="example"><strong>Example 3.5  </strong></span>Students have to submit coursework for a particular statistical modules. However, each semester a number of students miss the deadline and hand in their coursework late. Last year, three out of 20 students handed their coursework in late. This year, the course has thirty students in. How many students can we expect to hand in their coursework late?</p>
<p>We can model the number of students handing in late <span class="math inline">\(X\)</span>, using a Binomial distribution, <span class="math inline">\(Y \sim \textrm{Bin}(n, \theta)\)</span>. As in Example <a href="bayesian-inference.html#exm:binom">3.1</a>, we assign a uniform prior distribution to <span class="math inline">\(\theta \sim U[0, 1]\)</span>. Given then observed data, we can derive <span class="math inline">\(\theta \mid \boldsymbol{y} \sim Beta(4, 28)\)</span> (See problem sheets for derivation).</p>
<p>Now we can derive the posterior predictive distribution of <span class="math inline">\(Z\)</span>, the number of students who hand in late. We model <span class="math inline">\(Z\)</span> using a Binomial distribution, <span class="math inline">\(Z \sim \textrm{Bin}(30, \theta)\)</span>. The distribution of <span class="math inline">\(Z\)</span> given the observed data is</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y}) &amp;= \int_0^1 \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta \\
&amp; = \int_0^1 \begin{pmatrix} 30 \\ z \end{pmatrix} \theta^z (1-\theta)^{30 - z} \frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\theta^{3}(1-\theta)^{27}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\int_0^1 \theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
\end{align*}\]</span>
This integral is difficult to evaluate immediately. But by multiplying (and dividing outside the integral) by a constant, we can turn it into the density function of a Beta<span class="math inline">\((5 + z, 58 - z)\)</span> random variable. This integrates to 1.</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y})  &amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\frac{\Gamma(z+4)\Gamma(58-z)}{\Gamma(62)}\int_0^1 \frac{\Gamma(62)}{\Gamma(z+4)\Gamma(58-z)}\theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)\Gamma(z+4)\Gamma(58-z)}{\Gamma(4)\Gamma(28)\Gamma(62)} \quad \textrm{for } 0 \leq z \leq 30.
\end{align*}\]</span></p>
<p>This code implements the distribution</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="bayesian-inference.html#cb22-1" aria-hidden="true" tabindex="-1"></a>beta.binom.posterior.predictive.distribution <span class="ot">&lt;-</span> <span class="cf">function</span>(z){</span>
<span id="cb22-2"><a href="bayesian-inference.html#cb22-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-3"><a href="bayesian-inference.html#cb22-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-4"><a href="bayesian-inference.html#cb22-4" aria-hidden="true" tabindex="-1"></a>  numerator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">32</span>)<span class="sc">*</span><span class="fu">gamma</span>(z <span class="sc">+</span> <span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">58</span><span class="sc">-</span>z)</span>
<span id="cb22-5"><a href="bayesian-inference.html#cb22-5" aria-hidden="true" tabindex="-1"></a>  denominator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">28</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">62</span>)</span>
<span id="cb22-6"><a href="bayesian-inference.html#cb22-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-7"><a href="bayesian-inference.html#cb22-7" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">choose</span>(<span class="dv">30</span>, z)<span class="sc">*</span>numerator<span class="sc">/</span>denominator</span>
<span id="cb22-8"><a href="bayesian-inference.html#cb22-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb22-9"><a href="bayesian-inference.html#cb22-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-10"><a href="bayesian-inference.html#cb22-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We can check itâ€™s correct, by seeing if it sums to one</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="bayesian-inference.html#cb23-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">30</span></span>
<span id="cb23-2"><a href="bayesian-inference.html#cb23-2" aria-hidden="true" tabindex="-1"></a>ppd <span class="ot">&lt;-</span> <span class="fu">beta.binom.posterior.predictive.distribution</span>(z)</span>
<span id="cb23-3"><a href="bayesian-inference.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(ppd)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="bayesian-inference.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(z, ppd, <span class="at">xlab =</span> <span class="st">&quot;z&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="fu">pi</span>(z <span class="sc">|</span> theta)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="672" />
The expected number of students who hand in late is 3.75 and thereâ€™s a 95% chance that up to 8 hand in late.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="bayesian-inference.html#cb26-1" aria-hidden="true" tabindex="-1"></a>z<span class="sc">%*%</span>ppd <span class="co">#expectation</span></span></code></pre></div>
<pre><code>##      [,1]
## [1,] 3.75</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="bayesian-inference.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(z, <span class="fu">cumsum</span>(ppd)) <span class="co">#CDF</span></span></code></pre></div>
<pre><code>##        z           
##  [1,]  0 0.06029453
##  [2,]  1 0.18723037
##  [3,]  2 0.35156696
##  [4,]  3 0.51889148
##  [5,]  4 0.66530044
##  [6,]  5 0.78021765
##  [7,]  6 0.86309065
##  [8,]  7 0.91880359
##  [9,]  8 0.95404202
## [10,]  9 0.97513714
## [11,] 10 0.98713498
## [12,] 11 0.99363285
## [13,] 12 0.99698773
## [14,] 13 0.99863936
## [15,] 14 0.99941423
## [16,] 15 0.99976022
## [17,] 16 0.99990696
## [18,] 17 0.99996591
## [19,] 18 0.99998826
## [20,] 19 0.99999622
## [21,] 20 0.99999887
## [22,] 21 0.99999969
## [23,] 22 0.99999992
## [24,] 23 0.99999998
## [25,] 24 1.00000000
## [26,] 25 1.00000000
## [27,] 26 1.00000000
## [28,] 27 1.00000000
## [29,] 28 1.00000000
## [30,] 29 1.00000000
## [31,] 30 1.00000000</code></pre>
</div>
</div>
<div id="non-informative-prior-distibrutions" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Non-informative prior distibrutions<a href="bayesian-inference.html#non-informative-prior-distibrutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen in a few examples how the choice of prior distribution can affect that posterior distributions and the resulting conclusions. As the choice of prior distribution is subjective, it is the main criticism of Bayesian inference. A possible way around this is to use a prior distribution that reflects a lack of information about <span class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 3.3  </strong></span>A <strong>non-informative prior distribution</strong> is a prior distribution that places equal weight on the every possible value of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 3.6  </strong></span>In Example <a href="bayesian-inference.html#exm:binom">3.1</a>, we assigned a uniform prior distribution to the parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-27" class="theorem"><strong>Theorem 3.1  (Jeffrey) </strong></span>Given some observed data <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_N\}\)</span>, an invariant prior distribution is
<span class="math display">\[
\pi(\theta) \propto \sqrt{I_\theta(\boldsymbol{y})},
\]</span>
where <span class="math inline">\(I_\theta(\boldsymbol{y})\)</span> is the Fisher information for <span class="math inline">\(\theta\)</span> contained in <span class="math inline">\(\boldsymbol{y}\)</span>.</p>
</div>
<p>Jeffrey argues that if there are two ways of parameterising a model, e.g.Â via <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi\)</span>, then the priors on these parameters should be equivalent. In other words, the prior distribution should be invariant under sensible (one-to-one) transformations.</p>
<div class="proof">
<p><span id="unlabeled-div-28" class="proof"><em>Proof</em>. </span>Recall that the distribution of <span class="math inline">\(\psi = h(\theta)\)</span>, for some one-to-one function <span class="math inline">\(h\)</span>, is invariant to the distribution of <span class="math inline">\(\theta\)</span> if
<span class="math display">\[
\pi(\psi) = \pi(\theta) \left|\frac{d\theta}{d\psi}\right|.
\]</span></p>
<p>Transforming the Fisher information for <span class="math inline">\(\psi\)</span> shows
<span class="math display">\[\begin{align*}
I_\psi(\boldsymbol{y}) &amp;= - \mathbb{E}\left(\frac{d^2\log \pi(\boldsymbol{y} \mid \psi)}{d\psi^2}\right) \\
&amp; = \mathbb{E}\left(\frac{d^2 \log \pi(\boldsymbol{y} \mid \theta = h^{-1}(\psi))}{d\theta^2}\right) \left(\frac{d\theta}{d\psi}\right)^2 \\
&amp; = I_\theta(\boldsymbol{y})\left(\frac{d\theta}{d\psi}\right)^2 .
\end{align*}\]</span>
Thus <span class="math inline">\(\sqrt{I_\psi(\boldsymbol{y})} = \sqrt{I_\theta(\boldsymbol{y})} \left|\frac{d\theta}{d\psi}\right|\)</span> and <span class="math inline">\(\sqrt{I_\psi(\boldsymbol{y})}\)</span> and <span class="math inline">\(\sqrt{I_\theta(\boldsymbol{y})}\)</span> are invariant prior distributions.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 3.7  </strong></span>In example <a href="bayesian-inference.html#exm:binom">3.1</a>, we modelled the number of crisps that were the incorrect weight by <span class="math inline">\(\theta \sim \textrm{Bin}(n, \theta)\)</span>. To construct Jeffreyâ€™s prior distribution for <span class="math inline">\(\theta\)</span>, we must first derive the Fisherâ€™s information matrix.<br />
<span class="math display">\[\begin{align*}
&amp;\pi(y \mid \theta) = \begin{pmatrix} n \\ y \end{pmatrix} \theta^y (1-\theta)^{n-y}\\
\implies &amp;\log \pi(y \mid \theta) = \log \begin{pmatrix} n \\ y \end{pmatrix} + y \log\theta + (n-y)\log(1-\theta) \\
\implies &amp;\frac{\partial \log \pi(y \mid \theta)}{\partial \theta} = \frac{y}{\theta} - \frac{n-y}{1-\theta} \\
\implies &amp;\frac{\partial^2 \log \pi(y \mid \theta)}{\partial \theta^2} = -\frac{y}{\theta^2} + \frac{n-y}{(1-\theta)^2} \\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{\mathbb{E}(y)}{\theta^2} + \frac{n-\mathbb{E}(y)}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n\theta}{\theta^2} + \frac{n-n\theta}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta} + \frac{n}{1-\theta}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta(1-\theta)} \\
\implies &amp;I_\theta(y) \propto \frac{1}{\theta(1-\theta)}.
\end{align*}\]</span>
$
Hence Jeffreyâ€™s prior is <span class="math inline">\(\pi(\theta) \propto \theta^{-\frac{1}{2}}(1-\theta)^{-\frac{1}{2}}\)</span>. This functional dependency on <span class="math inline">\(\theta\)</span> shows that <span class="math inline">\(\theta \sim \textrm{Beta}(\frac{1}{2}, \frac{1}{2})\)</span>.</p>
</div>
</div>
<div id="bernstein-von-mises-theorem" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Bernstein-von-Mises Theorem<a href="bayesian-inference.html#bernstein-von-mises-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have considered Bayesian methods in contrast to frequentist ones. The Bernstein-von-Mises theorem is a key theorem linking the two inference methods.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-30" class="theorem"><strong>Theorem 3.2  (Bernstein-von-Mises) </strong></span>For a well-specified model <span class="math inline">\(\pi(\boldsymbol{y} \mid \theta)\)</span> with a fixed number of parameters, and for a smooth prior distribution <span class="math inline">\(\pi(\theta)\)</span> that is non-zero around the MLE <span class="math inline">\(\hat{\theta}\)</span>, then
<span class="math display">\[
\left|\left| \pi(\theta \mid \boldsymbol{y}) - N\left(\hat{\theta}, \frac{I(\hat{\theta})^{-1}}{n}\right) \right|\right|_{TV} \rightarrow 0,
\]</span>
where <span class="math inline">\(||p - q||_{TV}\)</span> is the total variation distance between distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>:
<span class="math display">\[
||p - q||_{TV} = \frac{1}{2}\int|\pi(x) - q(x)|\,dx.
\]</span></p>
</div>
<p>The Berstein-von-Mises theorem says that as the number of data points approaches infinity, the posterior distribution tends to a Normal distribution centered around the MLE and variance dependent on the Fisher information.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="programming-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
